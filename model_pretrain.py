# -----------------------------------------------------------------------------
# å¯¼å…¥ä¾èµ–
# -----------------------------------------------------------------------------
import os
import math
import time
import json
import random
import argparse
from contextlib import nullcontext

import torch
import numpy as np
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

# åˆ†å¸ƒå¼è®­ç»ƒæ ¸å¿ƒ
# from accelerate import Accelerator, DistributedDataParallelKwargs, DeepSpeedPlugin
# from accelerate.logging import get_logger as get_accel_logger

# å®éªŒè¿½è¸ª
import swanlab

# ç»ˆç«¯ç¾åŒ–è¾“å‡º
from rich.console import Console
from rich.table import Table

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from datasets import PretrainDataset
from model.Model import ByteTransformer
from model.config import ByteModelConfig
from utils.checkpoint import CheckpointManager
from utils.progressbar import RichProgressBar
from utils.logger import get_logger

console = Console()

# -----------------------------------------------------------------------------
# éšæœºç§å­ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨
# -----------------------------------------------------------------------------
def set_seed(seed: int) -> None:
    """å›ºå®šéšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°æ€§ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def get_lr(it: int, all_iters: int, args) -> float:
    """
    ä½™å¼¦é€€ç« + çº¿æ€§é¢„çƒ­ + å‘¨æœŸé‡å¯ å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

    å‚æ•°è¯´æ˜
    --------
    it : int
        å½“å‰å…¨å±€ stepã€‚
    all_iters : int
        å•ä¸ª epoch å†…çš„ step æ•°ä¹˜ä»¥æ€» epoch æ•°ï¼Œè¡¨ç¤ºå•å‘¨æœŸè¿­ä»£æ•°ã€‚
    args : Namespace
        å‘½ä»¤è¡Œå‚æ•°é›†åˆã€‚
    """
    # è®¡ç®—å…³é”®èŠ‚ç‚¹
    warmup_iters = int(args.warmup_steps_ratio * all_iters)  # é¢„çƒ­æ­¥æ•°
    decay_steps = int(args.lr_decay_steps_ratio * all_iters) # æ¯æ¬¡è¡°å‡çš„æ­¥é•¿

    # ä¾¿æ·å˜é‡
    min_lr           = args.min_lr
    warmup_start_lr  = args.warmup_start_lr or args.learning_rate / 1_000
    num_restarts     = args.num_restarts
    lr_decay_rate    = args.lr_decay_rate

    cycle_length = all_iters                 # ä¸€ä¸ªå‘¨æœŸé•¿åº¦
    total_iters  = all_iters * (num_restarts + 1)  # æ‰€æœ‰å‘¨æœŸçš„æ€»æ­¥æ•°

    # å¦‚æœè¶…è¿‡æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œè¿”å›æœ€å°å­¦ä¹ ç‡
    if it >= total_iters:
        return min_lr

    # ------- 1. çº¿æ€§ + ä½™å¼¦é¢„çƒ­ -------
    if it < warmup_iters:
        ratio  = it / max(1, warmup_iters)
        cosine = 0.5 * (1 - math.cos(math.pi * ratio))
        return warmup_start_lr + cosine * (args.learning_rate - warmup_start_lr)

    # ------- 2. å¤šå‘¨æœŸä½™å¼¦é€€ç« -------
    cycle_step       = (it - warmup_iters) % cycle_length   # å½“å‰å‘¨æœŸå†…çš„æ­¥æ•°
    cycle_idx        = (it - warmup_iters) // cycle_length  # å‘¨æœŸç´¢å¼•
    decay_steps_cnt  = cycle_step // decay_steps            # å·²è§¦å‘çš„è¡°å‡æ¬¡æ•°

    decayed_lr       = args.learning_rate * (lr_decay_rate ** decay_steps_cnt)
    decay_ratio      = (cycle_step % decay_steps) / max(1, decay_steps)
    cosine_coeff     = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))

    cycle_base_lr    = decayed_lr * (lr_decay_rate ** cycle_idx)
    current_lr       = min_lr + cosine_coeff * (max(cycle_base_lr, min_lr) - min_lr)
    return max(current_lr, min_lr)


# -----------------------------------------------------------------------------
# è®­ç»ƒè¿‡ç¨‹è¾…åŠ©å‡½æ•°
# -----------------------------------------------------------------------------

def grad_global_norm(parameters) -> float:
    """è®¡ç®—æ‰€æœ‰å¯è®­ç»ƒå‚æ•°æ¢¯åº¦çš„ L2 èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ & æ¢¯åº¦è£å‰ªï¼‰ã€‚"""
    total_norm = 0.0
    for p in parameters:
        if p.grad is None:
            continue
        param_norm  = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
    return math.sqrt(total_norm)


def format_size(num_bytes: int) -> str:
    """å°†å­—èŠ‚æ•°æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²ï¼Œå¦‚ 256.0Â MiBã€‚"""
    for unit in ["B", "KiB", "MiB", "GiB", "TiB"]:
        if abs(num_bytes) < 1024.0:
            return f"{num_bytes:3.1f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.1f} PiB"


# -----------------------------------------------------------------------------
# éªŒè¯å¾ªç¯
# -----------------------------------------------------------------------------

def evaluate(model, dataloader, args, logger, global_step):
    """
    æ•´ä¸ªéªŒè¯é›†å‰å‘æ¨ç†ï¼Œä¸è®¡ç®—æ¢¯åº¦ã€‚

    è¿”å›
    ----
    avg_loss : float
        éªŒè¯é›†å¹³å‡æŸå¤±ï¼Œç”¨äºæ—©åœæˆ–å­¦ä¹ ç‡è°ƒåº¦ã€‚
    """
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    total_loss   = 0.0
    total_tokens = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(args.device)
            labels    = batch["labels"].to(args.device)
            outputs   = model(input_ids=input_ids, labels=labels)
            # ç´¯åŠ  *æ ·æœ¬æ•°* æ–¹ä¾¿æœ€åæ±‚å¹³å‡
            total_loss += outputs.loss.item() * input_ids.size(0)

    avg_loss = total_loss / len(dataloader.dataset)
    ppl      = math.exp(min(20, avg_loss))  # é˜²æ­¢æº¢å‡º

    logger.info(f"[Eval] Step {global_step} | éªŒè¯æŸå¤± {avg_loss:.4f} | PPL {ppl:.2f}")

    if args.use_swanlab:
        swanlab.log({
            "val/loss": avg_loss,
            "val/perplexity": ppl,
        }, step=global_step)

    model.train()  # è¯„ä¼°å®Œè®°å¾—åˆ‡å›è®­ç»ƒæ¨¡å¼
    return avg_loss


# -----------------------------------------------------------------------------
# æ¨¡å‹ä¸åˆ†è¯å™¨åˆå§‹åŒ–
# -----------------------------------------------------------------------------

def init_model(args, logger):
    """æ ¹æ® CLI å‚æ•°æ„é€  ByteTransformer ä¸åˆ†è¯å™¨ã€‚"""

    # 1. ç»„è£…é…ç½®
    config = ByteModelConfig(
        vocab_size           = args.vocab_size,
        dim                  = args.model_dim,
        n_layers             = args.num_layers,
        n_heads              = args.num_attention_heads,
        n_kv_heads           = args.num_kv_heads,
        hidden_dim           = args.hidden_dim,
        multiple_of          = args.dim_multiplier,
        max_seq_len          = args.max_seq_len,
        drop_path_prob       = args.drop_path_prob,
        hidden_dropout       = args.hidden_dropout_prob,
        attention_dropout    = args.attention_dropout_prob,
        residual_dropout     = args.residual_dropout_prob,
        layer_norm_eps       = args.layer_norm_eps,
        initializer_range    = args.initializer_range,
        layerscale_init_value= args.layerscale_init,
        tie_word_embeddings  = args.tie_word_embeddings,
        xpos_rope_theta      = args.xpos_rope_theta,
        xpos_scale_base      = args.xpos_scale_base,
        use_flash_attention  = args.use_flash_attention,
        causal               = args.use_causal,
        use_cache            = args.use_cache,
        key_dtype            = args.key_cache_dtype,
        value_dtype          = args.value_cache_dtype,
        model_parallel_size  = args.model_parallel_size,
        tensor_parallel_size = args.tensor_parallel_size,
        tensor_parallel_rank = args.tensor_parallel_rank,
    )

    # 2. åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)

    # 3. æ„å»ºæ¨¡å‹
    model = ByteTransformer(config)
    if torch.cuda.device_count() > 1 and not args.ddp:
        model = torch.nn.DataParallel(model)
    model = model.to(args.device)

    # 4. æ‰“å°å‚æ•°è§„æ¨¡
    param_cnt = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"æ¨¡å‹å‚æ•°æ€»é‡: {param_cnt/1e6:.2f}M")

    return model, tokenizer, config


# -----------------------------------------------------------------------------
# å•ä¸ª epoch è®­ç»ƒ
# -----------------------------------------------------------------------------
def train_epoch(model, dataloader, optimizer, scaler, ctx, args, epoch,
                total_iters, logger, global_step):
    """æ‰§è¡Œä¸€ä¸ª epoch çš„å‰å‘ã€åå‘ä¸æ¢¯åº¦æ›´æ–°ã€‚"""

    model.train()
    loss_sum = 0.0

    pb_total = len(dataloader)
    # ä½¿ç”¨ RichProgressBar å¯è§†åŒ–è®­ç»ƒè¿›åº¦
    with RichProgressBar(total_steps=pb_total, total_batches=pb_total,
                         total_epochs=args.epochs, desc=f"Epoch {epoch+1}") as pbar:

        start_wall = time.perf_counter()

        # --------------------------------------------------
        # éå†æ•°æ®é›†
        # --------------------------------------------------
        for step, batch in enumerate(dataloader, 1):
            input_ids          = batch["input_ids"].to(args.device)
            labels             = batch["labels"].to(args.device)
            tokens_this_batch  = input_ids.numel()

            # â€”â€” åŠ¨æ€å­¦ä¹ ç‡ â€”â€”
            lr = get_lr(global_step, total_iters, args)
            for pg in optimizer.param_groups:
                pg["lr"] = lr

            # â€”â€” å‰å‘ + åå‘ â€”â€”
            with ctx:  # æ”¯æŒ AMP autocast
                outputs = model(input_ids=input_ids, labels=labels)
                loss    = outputs.loss / args.accumulation_steps

            scaler.scale(loss).backward()

            # â€”â€” æ¢¯åº¦ç´¯ç§¯ â€”â€”
            if (step % args.accumulation_steps) == 0:
                # åç¼©æ”¾åè£å‰ª & è®°å½•æ¢¯åº¦èŒƒæ•°
                scaler.unscale_(optimizer)
                total_grad_norm = grad_global_norm(model.parameters())
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

                # æ›´æ–°å‚æ•°
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
            else:
                total_grad_norm = 0.0  # éæ›´æ–°æ­¥ï¼Œæ¢¯åº¦èŒƒæ•°ç½® 0 ä»…åšæ—¥å¿—å ä½

            # â€”â€” æŒ‡æ ‡ç»Ÿè®¡ â€”â€”
            loss_item     = loss.item() * args.accumulation_steps
            loss_sum     += loss_item
            tokens_per_s  = tokens_this_batch / max(1e-6, time.perf_counter() - start_wall)
            gpu_mem       = torch.cuda.memory_allocated(args.device) if torch.cuda.is_available() else 0

            # â€”â€” æ—¥å¿—æ‰“å° & SwanLab â€”â€”
            if global_step % args.log_interval == 0:
                ppl = math.exp(min(20, loss_item))
                logger.info(
                    f"E{epoch+1} S{global_step} | loss {loss_item:.4f} | ppl {ppl:.1f} "
                    f"| lr {lr:.6g} | gnorm {total_grad_norm:.2f} | tok/s {tokens_per_s:.0f} "
                    f"| mem {format_size(gpu_mem)}")

                if args.use_swanlab:
                    swanlab.log({
                        "train/loss": loss_item,
                        "train/perplexity": ppl,
                        "train/lr": lr,
                        "train/grad_norm": total_grad_norm,
                        "train/tokens_per_sec": tokens_per_s,
                        "train/gpu_mem": gpu_mem / (1024**2),  # è½¬ MB
                    }, step=global_step)

            # â€”â€” éªŒè¯ & ä¿å­˜ â€”â€”
            if args.val_data_path and (global_step % args.eval_interval == 0):
                evaluate(model, args.val_loader, args, logger, global_step)

            if global_step % args.save_interval == 0:
                args.ckpt_mgr.save_checkpoint(model, optimizer, None, epoch, step=global_step)

            # â€”â€” æ›´æ–°è¿›åº¦æ¡ â€”â€”
            pbar.update_loader(step)
            pbar.update_train(global_step, epoch+1, loss=loss_item, lr=lr)

            global_step += 1

    avg_loss = loss_sum / pb_total
    logger.info(f"[Epoch {epoch+1}] å¹³å‡æŸå¤± {avg_loss:.4f}")

    return global_step


# -----------------------------------------------------------------------------
# ä¸»è®­ç»ƒæµç¨‹
# -----------------------------------------------------------------------------
def train(args, logger):
    """ä¸»è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æ¨¡å‹åˆå§‹åŒ–ã€è®­ç»ƒä¸éªŒè¯ã€SwanLab æ¥å…¥ç­‰ã€‚"""
    set_seed(args.seed)

    # åˆå§‹åŒ– SwanLab å®éªŒ
    if args.use_swanlab:
        swanlab.login(api_key=args.swanlab_api_key)
        swanlab.init(
            project=args.swanlab_project,
            experiment_name=args.swanlab_experiment_name,
            config=vars(args)
        )

    # åˆå§‹åŒ–æ¨¡å‹ã€åˆ†è¯å™¨ä¸é…ç½®
    model, tokenizer, config = init_model(args, logger)

    # åŠ è½½è®­ç»ƒé›†
    train_dataset = PretrainDataset(args.train_data_path, tokenizer, max_length=config.max_seq_len)
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)

    # åŠ è½½éªŒè¯é›†ï¼ˆå¦‚æœæä¾›ï¼‰
    val_loader = None
    if args.val_data_path:
        val_dataset = PretrainDataset(args.val_data_path, tokenizer, max_length=config.max_seq_len)
        val_loader = DataLoader(val_dataset, batch_size=args.eval_batch_size, shuffle=False, num_workers=args.num_workers)
        args.val_loader = val_loader

    # æ„å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)

    # å­¦ä¹ ç‡è°ƒåº¦ä¸AMPé…ç½®
    total_iters = args.epochs * len(train_loader)
    use_amp = args.amp or args.dtype in ['float16', 'bfloat16']
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
    ctx = nullcontext() if args.device == 'cpu' else torch.cuda.amp.autocast(dtype=getattr(torch, args.dtype)) if use_amp else nullcontext()

    # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    ckpt_mgr = CheckpointManager(args.checkpoints_dir)
    args.ckpt_mgr = ckpt_mgr

    start_epoch = 0
    global_step = 0
    if ckpt_mgr.has_checkpoint():
        logger.info("ğŸ” æ£€æµ‹åˆ°å†å²æ£€æŸ¥ç‚¹ï¼Œæ­£åœ¨æ¢å¤ä¸­â€¦")
        checkpoint = ckpt_mgr.load_checkpoint(model, optimizer, None)
        start_epoch = checkpoint.get("epoch", 0) + 1
        global_step = checkpoint.get("step", 0)

    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒâ€¦")
    for epoch in range(start_epoch, args.epochs):
        global_step = train_epoch(model, train_loader, optimizer, scaler, ctx,
                                  args, epoch, total_iters, logger, global_step)
        if val_loader:
            evaluate(model, val_loader, args, logger, global_step)
        ckpt_mgr.save_checkpoint(model, optimizer, None, epoch)

    logger.info("âœ… è®­ç»ƒå®Œæˆã€‚")



# ç¨‹åºå…¥å£
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # =============================
    #       å®éªŒåŸºç¡€é…ç½®
    # =============================
    parser.add_argument("--seed", type=int, default=42, help="å…¨å±€éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°æ€§")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",help="è®­ç»ƒè®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--use_swanlab", action="store_true", help="æ˜¯å¦å¯ç”¨SwanLabå®éªŒè¿½è¸ª")
    parser.add_argument("--swanlab_project", type=str, default="Happy-LLM", help="SwanLabé¡¹ç›®åç§°")
    parser.add_argument("--swanlab_experiment_name", type=str, default="Pretrain-215M", help="SwanLabå®éªŒåç§°")
    parser.add_argument("--swanlab_api_key", type=str, default="",  help="SwanLab APIè®¤è¯å¯†é’¥")
    parser.add_argument("--logs_dir", type=str, default="./logs", help="æ—¥å¿—è¾“å‡ºç›®å½•")
    parser.add_argument("--checkpoints_dir", type=str, default="./checkpoints", help="æ¨¡å‹æ£€æŸ¥ç‚¹è¾“å‡ºç›®å½•")

    # =============================
    #       æ•°æ®é›†é…ç½®
    # =============================
    parser.add_argument("--tokenizer_path", type=str, default="./tokenizer", help="åˆ†è¯å™¨é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--train_data_path", type=str, default="./datasets/test/train.jsonl", help="è®­ç»ƒæ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--val_data_path", type=str, default="./datasets/test/val.jsonl", help="éªŒè¯æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")

    # =============================
    #       æ¨¡å‹æ¶æ„é…ç½®
    # =============================
    parser.add_argument("--vocab_size", type=int, default=32000, help="è¯æ±‡è¡¨å¤§å°")
    parser.add_argument("--model_dim", type=int, default=768, help="æ¨¡å‹éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_layers", type=int, default=12, help="Transformerå±‚æ•°")
    parser.add_argument("--num_attention_heads", type=int, default=16, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--num_kv_heads", type=int, default=8, help="Key/Valueæ³¨æ„åŠ›å¤´æ•°ï¼ˆå¤´åˆ†ç¦»ï¼‰")
    parser.add_argument("--hidden_dim", type=int, default=None, help="FFNéšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤4*model_dimï¼‰")
    parser.add_argument("--dim_multiplier", type=int, default=4, help="éšè—å±‚ç»´åº¦å¯¹é½åŸºæ•°")
    parser.add_argument("--max_seq_len", type=int, default=2048, help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # Dropout å‚æ•°
    parser.add_argument("--drop_path_prob", type=float, default=0.0, help="æ®‹å·®è·¯å¾„Dropoutæ¦‚ç‡")
    parser.add_argument("--hidden_dropout_prob", type=float, default=0.1, help="éšè—å±‚Dropoutæ¦‚ç‡")
    parser.add_argument("--attention_dropout_prob", type=float, default=0.1, help="æ³¨æ„åŠ›Dropoutæ¦‚ç‡")
    parser.add_argument("--residual_dropout_prob", type=float, default=0.1, help="æ®‹å·®è¿æ¥Dropoutæ¦‚ç‡")
    
    # å½’ä¸€åŒ–å‚æ•°
    parser.add_argument("--layer_norm_eps", type=float, default=1e-5, help="å±‚å½’ä¸€åŒ–epsilon")
    parser.add_argument("--initializer_range", type=float, default=0.02, help="å‚æ•°åˆå§‹èŒƒå›´")
    parser.add_argument("--layerscale_init", type=float, default=1e-5, help="å±‚ç¼©æ”¾åˆå§‹åŒ–å€¼")
    
    # åµŒå…¥å‚æ•°
    parser.add_argument("--tie_word_embeddings", action="store_true", help="ç»‘å®šè¾“å…¥è¾“å‡ºè¯åµŒå…¥")
    
    # ä½ç½®ç¼–ç å‚æ•°
    parser.add_argument("--xpos_rope_theta", type=float, default=10000.0, help="XPosä½ç½®ç¼–ç theta")
    parser.add_argument("--xpos_scale_base", type=float, default=512.0, help="XPosç¼©æ”¾å› å­")
    
    # æ³¨æ„åŠ›æœºåˆ¶
    parser.add_argument("--use_flash_attention", action="store_true", help="å¯ç”¨FlashAttention")
    parser.add_argument("--use_causal", action="store_true", help="ä½¿ç”¨å› æœæ³¨æ„åŠ›æ©ç ")
    
    # æ¨ç†ä¼˜åŒ–
    parser.add_argument("--use_cache", action="store_true", help="å¯ç”¨KVç¼“å­˜åŠ é€Ÿæ¨ç†")
    parser.add_argument("--key_cache_dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"], help="Keyç¼“å­˜æ•°æ®ç±»å‹")
    parser.add_argument("--value_cache_dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"], help="Valueç¼“å­˜æ•°æ®ç±»å‹")
    
    # å¹¶è¡Œè®­ç»ƒ
    parser.add_argument("--model_parallel_size", type=int, default=1, help="æ¨¡å‹å¹¶è¡Œå¤§å°")
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="å¼ é‡å¹¶è¡Œå¤§å°")
    parser.add_argument("--tensor_parallel_rank", type=int, default=0, help="å¼ é‡å¹¶è¡Œrank")

    # =============================
    #       è®­ç»ƒè¶…å‚æ•°é…ç½®
    # =============================
    parser.add_argument("--batch_size", type=int, default=32, help="æ¯æ‰¹æ¬¡æ ·æœ¬æ•°é‡")
    parser.add_argument("--epochs", type=int, default=5, help="è®­ç»ƒæ€»è½®æ¬¡")
    parser.add_argument("--accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§batch sizeï¼‰")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰")

    # =============================
    #       ä¼˜åŒ–å™¨å‚æ•°é…ç½®
    # =============================
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="åŸºç¡€å­¦ä¹ ç‡")
    parser.add_argument("--min_lr", type=float, default=5e-6, help="å­¦ä¹ ç‡æœ€å°å€¼ï¼ˆä½™å¼¦é€€ç«ä¸‹é™ï¼‰")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="AdamWä¼˜åŒ–å™¨æƒé‡è¡°å‡ç³»æ•°")

    # =============================
    #       å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    # =============================
    parser.add_argument("--warmup_steps_ratio", type=float, default=0.05, help="é¢„çƒ­é˜¶æ®µå æ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ä¾‹")
    parser.add_argument("--warmup_start_lr", type=float, default=5e-7, help="é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡ï¼ˆé»˜è®¤: learning_rate/1000ï¼‰")
    parser.add_argument("--lr_decay_rate", type=float, default=0.8,  help="å­¦ä¹ ç‡è¡°å‡ç‡ï¼ˆå¤šå‘¨æœŸä½™å¼¦é€€ç«ï¼‰")
    parser.add_argument("--lr_decay_steps_ratio", type=int, default=0.3, help="å­¦ä¹ ç‡è¡°å‡é—´éš”æ¯”ä¾‹ï¼ˆæ­¥è¿›å¼è¡°å‡ï¼‰")
    parser.add_argument("--num_restarts", type=int, default=0,  help="ä½™å¼¦é€€ç«é‡å¯æ¬¡æ•°ï¼ˆ0è¡¨ç¤ºå•å‘¨æœŸï¼‰")

    # =============================
    #       æ··åˆç²¾åº¦è®­ç»ƒé…ç½®
    # =============================
    parser.add_argument("--dtype", type=str, default="bfloat16", 
                       choices=["float32", "float16", "bfloat16"],
                       help="è®­ç»ƒæ•°æ®ç±»å‹ï¼ˆfloat32/full precision, float16/half, bfloat16/brain floatï¼‰")
    parser.add_argument("--amp", action="store_true", help="å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä¸dtypeäº’æ–¥ï¼‰")

    # =============================
    #       æ—¥å¿—ä¸ä¿å­˜é…ç½®
    # =============================
    parser.add_argument("--log_interval", type=int, default=100, help="è®­ç»ƒæ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--save_interval", type=int, default=1000, help="æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--eval_interval", type=int, default=2000, help="éªŒè¯é›†è¯„ä¼°é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--max_checkpoints", type=int, default=5, help="æœ€å¤§ä¿ç•™çš„æ£€æŸ¥ç‚¹æ•°é‡")

    # =============================
    #       éªŒè¯é…ç½®
    # =============================
    parser.add_argument("--eval_batch_size", type=int, default=64, help="éªŒè¯æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--eval_max_steps", type=int, default=100, help="æœ€å¤§éªŒè¯æ­¥æ•°ï¼ˆå…¨éªŒè¯é›†è¿‡å¤§æ—¶ä½¿ç”¨ï¼‰")
    args = parser.parse_args()
    
    # -------- æ—¥å¿—ç³»ç»Ÿ --------
    logger = get_logger(args.logs_dir, args.swanlab_experiment_name)
    logger.info("é…ç½®å‚æ•°:\n" + str(vars(args)))

    train(args, logger)  # å¯åŠ¨è®­ç»ƒ
