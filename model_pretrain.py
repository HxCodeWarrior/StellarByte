import os
import sys
import math
import time
import random
from contextlib import nullcontext

import torch
import numpy as np
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

# åˆ†å¸ƒå¼è®­ç»ƒæ ¸å¿ƒ
from torch.utils.data.distributed import DistributedSampler

# å®éªŒè¿½è¸ª
import swanlab

# ç»ˆç«¯ç¾åŒ–è¾“å‡º
from rich.console import Console
from rich.table import Table

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from datasets import PretrainDataset
from model.Model import ByteTransformer
from model.config import ByteModelConfig
from utils.checkpoint import CheckpointManager, GracefulKiller
from utils.progressbar import RichProgressBar
from utils.logger import get_logger
from utils.config_params import load_config

console = Console()

# ========= å…¨å±€æ€§èƒ½ / æ˜¾å­˜ä¼˜åŒ–ï¼ˆæ–°å¢ï¼‰ =========
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")  # å‡ç¢ç‰‡ :contentReference[oaicite:0]{index=0}
torch.backends.cuda.matmul.allow_tf32 = True        # Ampere+ TF32 â¾ƒåŠ¨é™ç²¾åº¦
torch.backends.cudnn.benchmark = True               # cuDNN ç®—æ³•è‡ªåŠ¨æœç´¢

# -----------------------------------------------------------------------------
# éšæœºç§å­ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨
# -----------------------------------------------------------------------------
def set_seed(seed: int, args=None) -> int:
    """
    å›ºå®šéšæœºç§å­ï¼›DDP æ—¶å°† rank0 çš„ seed å¹¿æ’­ç»™æ‰€æœ‰è¿›ç¨‹ï¼Œ
    è¿”å›æœ€ç»ˆ seed ä»¥ä¾¿è°ƒç”¨è€…å¤ç”¨ã€‚
    """
    if args and args.enable_ddp:
        # rank0 å†³å®šéšæœºç§å­å¹¶å¹¿æ’­
        seed_tensor = torch.tensor([seed], dtype=torch.long, device=f"cuda:{args.local_rank}")
        torch.distributed.broadcast(seed_tensor, src=0)
        seed = int(seed_tensor.item())

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    return seed


# -----------------------------------------------------------------------------
# è®­ç»ƒè¿‡ç¨‹è¾…åŠ©å‡½æ•°
# -----------------------------------------------------------------------------
def grad_global_norm(parameters) -> float:
    """è®¡ç®—æ‰€æœ‰å¯è®­ç»ƒå‚æ•°æ¢¯åº¦çš„ L2 èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ & æ¢¯åº¦è£å‰ªï¼‰ã€‚"""
    total_norm = 0.0
    for p in parameters:
        if p.grad is None:
            continue
        param_norm  = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
    return math.sqrt(total_norm)


def format_size(num_bytes: int) -> str:
    """å°†å­—èŠ‚æ•°æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²ï¼Œå¦‚ 256.0Â MiBã€‚"""
    for unit in ["B", "KiB", "MiB", "GiB", "TiB"]:
        if abs(num_bytes) < 1024.0:
            return f"{num_bytes:3.1f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.1f} PiB"


# -----------------------------------------------------------------------------
# å­¦ä¹ ç‡è°ƒåº¦å™¨
# -----------------------------------------------------------------------------
def get_lr(it: int, all_iters: int, args) -> float:
    """
    ä½™å¼¦é€€ç« + çº¿æ€§é¢„çƒ­ + å‘¨æœŸé‡å¯ å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

    å‚æ•°è¯´æ˜
    --------
    it : int,å½“å‰å…¨å±€ stepã€‚
    all_iters : int,å•ä¸ª epoch å†…çš„ step æ•°ä¹˜ä»¥æ€» epoch æ•°ï¼Œè¡¨ç¤ºå•å‘¨æœŸè¿­ä»£æ•°ã€‚
    args : Namespace,å‘½ä»¤è¡Œå‚æ•°é›†åˆã€‚
    """
    # è®¡ç®—å…³é”®èŠ‚ç‚¹
    warmup_iters = int(args.warmup_steps_ratio * all_iters)  # é¢„çƒ­æ­¥æ•°
    decay_steps = int(args.lr_decay_steps_ratio * all_iters) # æ¯æ¬¡è¡°å‡çš„æ­¥é•¿

    # ä¾¿æ·å˜é‡
    min_lr           = args.min_lr
    warmup_start_lr  = args.warmup_start_lr or args.learning_rate / 1_000
    num_restarts     = args.num_restarts
    lr_decay_rate    = args.lr_decay_rate

    cycle_length = all_iters                 # ä¸€ä¸ªå‘¨æœŸé•¿åº¦
    total_iters  = all_iters * (num_restarts + 1)  # æ‰€æœ‰å‘¨æœŸçš„æ€»æ­¥æ•°

    # å¦‚æœè¶…è¿‡æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œè¿”å›æœ€å°å­¦ä¹ ç‡
    if it >= total_iters:
        return min_lr

    # ------- 1. çº¿æ€§ + ä½™å¼¦é¢„çƒ­ -------
    if it < warmup_iters:
        ratio  = it / max(1, warmup_iters)
        cosine = 0.5 * (1 - math.cos(math.pi * ratio))
        return warmup_start_lr + cosine * (args.learning_rate - warmup_start_lr)

    # ------- 2. å¤šå‘¨æœŸä½™å¼¦é€€ç« -------
    cycle_step       = (it - warmup_iters) % cycle_length   # å½“å‰å‘¨æœŸå†…çš„æ­¥æ•°
    cycle_idx        = (it - warmup_iters) // cycle_length  # å‘¨æœŸç´¢å¼•
    decay_steps_cnt  = cycle_step // decay_steps            # å·²è§¦å‘çš„è¡°å‡æ¬¡æ•°

    decayed_lr       = args.learning_rate * (lr_decay_rate ** decay_steps_cnt)
    decay_ratio      = (cycle_step % decay_steps) / max(1, decay_steps)
    cosine_coeff     = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))

    cycle_base_lr    = decayed_lr * (lr_decay_rate ** cycle_idx)
    current_lr       = min_lr + cosine_coeff * (max(cycle_base_lr, min_lr) - min_lr)
    return max(current_lr, min_lr)


# -----------------------------------------------------------------------------
# éªŒè¯å¾ªç¯
# -----------------------------------------------------------------------------
def evaluate(model, dataloader, args, global_step, logger):
    """
    æ•´ä¸ªéªŒè¯é›†å‰å‘æ¨ç†ï¼Œä¸è®¡ç®—æ¢¯åº¦ã€‚

    è¿”å›
    ----
    avg_loss : float
        éªŒè¯é›†å¹³å‡æŸå¤±ï¼Œç”¨äºæ—©åœæˆ–å­¦ä¹ ç‡è°ƒåº¦ã€‚
    """
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    total_loss   = torch.tensor(0.0, device=args.device)
    total_tokens = torch.tensor(0,   device=args.device)

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(args.device)
            labels    = batch["labels"].to(args.device)
            outputs   = model(input_ids=input_ids, labels=labels)
            # ç´¯åŠ  *æ ·æœ¬æ•°* æ–¹ä¾¿æœ€åæ±‚å¹³å‡
            total_loss   += outputs.loss.detach() * input_ids.size(0)
            total_tokens += input_ids.size(0)

     # -------- åˆ†å¸ƒå¼æ±‡æ€» --------
    if args.ddp:
        torch.distributed.all_reduce(total_loss,  op=torch.distributed.ReduceOp.SUM)
        torch.distributed.all_reduce(total_tokens, op=torch.distributed.ReduceOp.SUM)

    avg_loss = (total_loss / total_tokens).item()
    ppl      = math.exp(min(20, avg_loss))

    if is_main_process(args):
        logger.info(f"[Eval] Step {global_step} | loss {avg_loss:.4f} | ppl {ppl:.2f}")
        if args.use_swanlab:
            swanlab.log({"val/loss": avg_loss, "val/ppl": ppl}, step=global_step)

    model.train()  # è¯„ä¼°å®Œè®°å¾—åˆ‡å›è®­ç»ƒæ¨¡å¼
    return avg_loss


# -----------------------------------------------------------------------------
# æ¨¡å‹ä¸åˆ†è¯å™¨åˆå§‹åŒ–
# -----------------------------------------------------------------------------
def init_model(args, logger):
    """æ ¹æ® CLI å‚æ•°æ„é€  ByteTransformer ä¸åˆ†è¯å™¨ã€‚"""

    # 1. ç»„è£…é…ç½®
    config = ByteModelConfig(
        vocab_size           = args.vocab_size,
        dim                  = args.dim,
        n_layers             = args.n_layers,
        n_heads              = args.n_heads,
        n_kv_heads           = args.n_kv_heads,
        hidden_dim           = args.hidden_dim,
        multiple_of          = args.dim_multiplier,
        max_seq_len          = args.max_seq_len,
        drop_path_prob       = args.drop_path_prob,
        hidden_dropout       = args.hidden_dropout_prob,
        attention_dropout    = args.attention_dropout_prob,
        residual_dropout     = args.residual_dropout_prob,
        layer_norm_eps       = args.layer_norm_eps,
        initializer_range    = args.initializer_range,
        layerscale_init_value= args.layerscale_init,
        tie_word_embeddings  = args.tie_word_embeddings,
        xpos_rope_theta      = args.rope_theta,
        xpos_scale_base      = args.scale_base,
        use_flash_attention  = args.use_flash_attention,
        causal               = args.use_causal,
        use_cache            = args.use_cache,
        key_dtype            = args.key_dtype,
        value_dtype          = args.value_dtype,
        model_parallel_size  = args.model_parallel_size,
        tensor_parallel_size = args.tensor_parallel_size,
        tensor_parallel_rank = args.tensor_parallel_rank,
    )

    # 2. åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)

    # 3. æ„å»ºæ¨¡å‹
    model = ByteTransformer(config).to(args.device)

    # 4. æ¢¯åº¦æ£€æŸ¥ç‚¹
    if getattr(args, "grad_checkpoint", False):
        model.gradient_checkpointing_enable()
        logger.info("âœ… å·²å¯ç”¨ Gradient Checkpointing")
 
    # 5. torch.compileï¼ˆåå+æ˜¾å­˜åŒèµ¢ï¼‰
    if getattr(args, "use_torch_compile", False) and hasattr(torch, "compile"):
        mode = getattr(args, "compile_mode", "max-autotune")
        model = torch.compile(model, mode=mode, fullgraph=False)
        logger.info(f"ğŸš€ torch.compile(mode='{mode}') å·²å¯ç”¨")  # :contentReference[oaicite:1]{index=1}
 
    # 6. å¹¶è¡ŒåŒ…è£…
    if args.enable_ddp:
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[args.local_rank],
            output_device=args.local_rank,
            find_unused_parameters=False
        )
    elif torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)

    # 7. æ‰“å°å‚æ•°è§„æ¨¡
    param_cnt = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"æ¨¡å‹å‚æ•°æ€»é‡: {param_cnt/1e6:.2f}M")

    return model, tokenizer, config


# -----------------------------------------------------------------------------
# åˆ†å¸ƒå¼è®­ç»ƒ
# -----------------------------------------------------------------------------
def init_distributed(args):
    """DDP åˆå§‹åŒ–ï¼ˆtorchrun ç¯å¢ƒä¸‹è‡ªåŠ¨è¯»å– env å˜é‡ï¼‰"""
    if not args.enable_ddp:     # å•è¿›ç¨‹é€»è¾‘
        args.rank = 0
        args.world_size = 1
        args.local_rank = 0
        return

    # ---- æ£€æŸ¥ CUDA è®¾å¤‡ ----
    if args.device.startswith("cuda") and not torch.cuda.is_available():
        raise RuntimeError("DDP å·²å¯ç”¨ï¼Œä½†å½“å‰æœªæ£€æµ‹åˆ°å¯ç”¨ CUDAï¼Œè¯·å®‰è£… GPU ç‰ˆ PyTorch æˆ–è®¾ç½® enable_ddp=False")

    # ---- torchrun æ³¨å…¥çš„ç¯å¢ƒå˜é‡ ----
    args.rank = int(os.environ["RANK"])            # å…¨å±€ rank
    args.world_size = int(os.environ["WORLD_SIZE"])# å…¨å±€è¿›ç¨‹æ•°
    args.local_rank = int(os.environ["LOCAL_RANK"])# æœ¬èŠ‚ç‚¹å±€éƒ¨ rank

    # ---- è®¾å¤‡ã€è¿›ç¨‹ç»„ ----
    backend="nccl" if torch.cuda.is_available() and sys.platform != "win32" else "gloo"
    torch.cuda.set_device(args.local_rank)
    torch.distributed.init_process_group(
        backend=backend,
        init_method="env://"
    )
    torch.distributed.barrier(device_ids=[args.local_rank])

def cleanup_distributed():
    """è®­ç»ƒç»“æŸåé”€æ¯è¿›ç¨‹ç»„ï¼Œé‡Šæ”¾èµ„æºã€‚"""
    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()

def is_main_process(args) -> bool:
    """åˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦ä¸»è¿›ç¨‹ï¼ˆrank0ï¼‰"""
    return (not args.enable_ddp) or args.rank == 0


# -----------------------------------------------------------------------------
# å•ä¸ª epoch è®­ç»ƒ
# -----------------------------------------------------------------------------
def train_epoch(model, dataloader, optimizer, scaler, ctx, args, epoch,
                total_iters, logger, global_step, ckpt_mgr, killer, best_val_loss):
    """æ‰§è¡Œä¸€ä¸ª epoch çš„å‰å‘ã€åå‘ä¸æ¢¯åº¦æ›´æ–°ã€‚"""

    model.train()
    loss_sum = 0.0
    pb_total = len(dataloader)
    
    # DDP sampler æ´—ç‰Œ
    if args.enable_ddp and isinstance(dataloader.sampler, DistributedSampler):
        dataloader.sampler.set_epoch(epoch)

    # ä½¿ç”¨ RichProgressBar å¯è§†åŒ–è®­ç»ƒè¿›åº¦
    with RichProgressBar(total_steps=pb_total, 
                         total_batches=pb_total,
                         total_epochs=args.epochs, 
                         desc=f"Epoch {epoch+1}") as pbar:

        start_wall = time.perf_counter()

        # --------------------------------------------------
        # éå†æ•°æ®é›†
        # --------------------------------------------------
        for step, batch in enumerate(dataloader, 1):
            # æ¯ N æ­¥å†æ¸…ä¸€æ¬¡ï¼Œé¿å…å¼ºåŒæ­¥å¯¼è‡´ååä¸‹é™
            if (step % args.empty_cache_interval) == 0:
                torch.cuda.empty_cache()
            input_ids          = batch["input_ids"].to(args.device)
            labels             = batch["labels"].to(args.device)
            tokens_this_batch  = input_ids.numel()

            # â€”â€” åŠ¨æ€å­¦ä¹ ç‡ â€”â€”
            lr = get_lr(global_step, total_iters, args)
            for pg in optimizer.param_groups:
                pg["lr"] = lr

            # â€”â€” å‰å‘ + åå‘ â€”â€”
            with ctx:  # æ”¯æŒ AMP autocast
                outputs = model(input_ids=input_ids, labels=labels)
                loss    = outputs.loss / args.accumulation_steps

            scaler.scale(loss).backward()

            # â€”â€” æ¢¯åº¦ç´¯ç§¯ â€”â€”
            if (step % args.accumulation_steps) == 0:
                # åç¼©æ”¾åè£å‰ª & è®°å½•æ¢¯åº¦èŒƒæ•°
                scaler.unscale_(optimizer)
                total_grad_norm = grad_global_norm(model.parameters())
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

                # æ›´æ–°å‚æ•°
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
            else:
                total_grad_norm = 0.0  # éæ›´æ–°æ­¥ï¼Œæ¢¯åº¦èŒƒæ•°ç½® 0 ä»…åšæ—¥å¿—å ä½

            # â€”â€” æŒ‡æ ‡ç»Ÿè®¡ â€”â€”
            loss_item     = loss.item() * args.accumulation_steps
            loss_sum     += loss_item
            tokens_per_s  = tokens_this_batch / max(1e-6, time.perf_counter() - start_wall)
            gpu_mem       = torch.cuda.memory_allocated(args.device) if torch.cuda.is_available() else 0

            if is_main_process(args):
                # â€”â€” è½»é‡ checkpoint(æŒ‰æ­¥) â€”â€”
                if ckpt_mgr.should_save(global_step):
                    ckpt_mgr.save(model, optimizer, scaler,
                                  epoch=epoch, step=global_step,
                                  full=False)

                # â€”â€” æ›´æ–° Killer â€”â€”
                killer.update(epoch, global_step, best_val_loss)

                # â€”â€” æ—¥å¿—æ‰“å° & SwanLab â€”â€”
                if global_step % args.log_interval == 0:
                    ppl = math.exp(min(20, loss_item))
                    logger.info(
                        f"E{epoch+1} S{global_step} | loss {loss_item:.4f} | ppl {ppl:.1f} "
                        f"| lr {lr:.6g} | gnorm {total_grad_norm:.2f} | tok/s {tokens_per_s:.0f} "
                        f"| mem {format_size(gpu_mem)}")

                    if args.use_swanlab:
                        swanlab.log({
                            "train/loss": loss_item,
                            "train/perplexity": ppl,
                            "train/lr": lr,
                            "train/grad_norm": total_grad_norm,
                            "train/tokens_per_sec": tokens_per_s,
                            "train/gpu_mem": gpu_mem / (1024**2),  # è½¬ MB
                        }, step=global_step)

            # â€”â€” æ›´æ–°è¿›åº¦æ¡ â€”â€”
            pbar.update_loader(step)
            pbar.update_train(global_step, epoch+1, loss=loss_item, lr=lr)

            global_step += 1

    avg_loss = loss_sum / pb_total
    if is_main_process(args):
        logger.info(f"[Epoch {epoch+1}] å¹³å‡æŸå¤± {avg_loss:.4f}")

    return global_step


# -----------------------------------------------------------------------------
# ä¸»è®­ç»ƒæµç¨‹
# -----------------------------------------------------------------------------
def train(args, logger):
    """ä¸»è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æ¨¡å‹åˆå§‹åŒ–ã€è®­ç»ƒä¸éªŒè¯ã€SwanLab æ¥å…¥ç­‰ã€‚"""
    set_seed(args.seed, args)

    # --------- åˆå§‹åŒ–ç»„ä»¶ ---------
    # åˆå§‹åŒ– SwanLab å®éªŒ
    if is_main_process(args) and args.use_swanlab:
        swanlab.login(api_key=args.swanlab_api_key)
        swanlab.init(
            project=args.swanlab_project,
            experiment_name=args.swanlab_experiment_name,
            config=vars(args)
        )

     # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    ckpt_mgr = CheckpointManager(
        args.checkpoints_dir,
        keep_latest=args.keep_latest,
        keep_epoch=args.keep_epoch,
        keep_best=args.keep_best,
        save_every_n_steps=args.save_interval
    )

    # åˆå§‹åŒ–æ¨¡å‹ã€åˆ†è¯å™¨ä¸é…ç½®
    model, tokenizer, config = init_model(args, logger)

    # åŠ è½½è®­ç»ƒé›†
    # 1. åŠ è½½è®­ç»ƒé›†
    train_dataset = PretrainDataset(
        args.train_data_path, 
        tokenizer, 
        max_length=config.max_seq_len,
        fields=args.dataset_loader.fields,
        template=args.dataset_loader.template if args.dataset_loader.template else None,
        add_bos=args.dataset_loader.add_bos
    )
    # 2. æ ¹æ®åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®é‡‡æ ·å™¨
    if args.enable_ddp:
        train_sampler = DistributedSampler(train_dataset, shuffle=True)
        shuffle_flag  = False
    else:
        train_sampler = None
        shuffle_flag  = True
    # 3. æ„å»º Train_DataLoader
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=shuffle_flag, 
        sampler=train_sampler,
        num_workers=args.num_workers,
        pin_memory=True
    )

    # åŠ è½½éªŒè¯é›†ï¼ˆå¦‚æœæä¾›ï¼‰
    val_loader = None
    if args.val_data_path:
        # 1. åŠ è½½éªŒè¯é›†
        val_dataset = PretrainDataset(
            args.val_data_path, 
            tokenizer, 
            max_length=config.max_seq_len,
            fields=args.dataset_loader.fields,
            template=args.dataset_loader.template if args.dataset_loader.template else None,
            add_bos=args.dataset_loader.add_bos
        )
        # 2. æ ¹æ®åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®é‡‡æ ·å™¨
        if args.enable_ddp:
            val_sampler = DistributedSampler(val_dataset, shuffle=False)
        else:
            val_sampler = None
        # 3. æ„å»º Val_DataLoader
        val_loader  = DataLoader(
            val_dataset, 
            batch_size=args.eval_batch_size, 
            shuffle=False,
            sampler=val_sampler,
            num_workers=args.num_workers,
            pin_memory=True
        )
        args.val_loader = val_loader

    # æ„å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate, 
        weight_decay=args.weight_decay
    )

    # å­¦ä¹ ç‡è°ƒåº¦ä¸AMPé…ç½®
    total_iters = args.epochs * len(train_loader)
    use_amp = args.amp or args.dtype in ['float16', 'bfloat16']
    scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)
    ctx     = (nullcontext() if args.device == 'cpu'
               else torch.cuda.amp.autocast(dtype=getattr(torch, args.dtype))
               if use_amp else nullcontext())

    # åˆå§‹åŒ– GracefulKiller
    killer = GracefulKiller(model, optimizer, scaler, ckpt_mgr, logger, sync=True)

    start_epoch   = 0
    global_step   = 0
    val_loss      = None
    best_val_loss = float("inf")

    # ---------- æ¢å¤ ----------
    try:
        logger.info("ğŸ” æ£€æµ‹åˆ°å†å²æ£€æŸ¥ç‚¹ï¼Œæ­£åœ¨æ¢å¤ä¸­â€¦")
        ckpt = ckpt_mgr.load_latest(model, optimizer, scaler)
        start_epoch  = ckpt['epoch'] + 1
        global_step  = ckpt['step'] + 1
        best_val_loss = ckpt.get('val_loss', float('inf'))
        logger.info(f"ğŸª„ å·²æ¢å¤åˆ° epoch {start_epoch}, step {global_step}")
    except FileNotFoundError:
        logger.info("ğŸ†• æœªæ£€æµ‹åˆ° checkpointï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ")
        start_epoch, global_step, best_val_loss = 0, 0, float('inf')

    # ---------- è®­ç»ƒ ----------
    try:
        if is_main_process(args):
            logger.info("ğŸš€ å¼€å§‹è®­ç»ƒâ€¦")
        for epoch in range(start_epoch, args.epochs):
            global_step = train_epoch(
                model, train_loader, optimizer, scaler, ctx,
                args, epoch, total_iters, logger, global_step, 
                ckpt_mgr, killer, best_val_loss
            )
            # -------------- éªŒè¯ (åªåœ¨ä¸»è¿›ç¨‹) --------------------
            if val_loader and is_main_process(args):
                val_loss = evaluate(model, val_loader, args, global_step, logger)

                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    logger.info(f"ğŸ‰ éªŒè¯é›†æŸå¤±ä¸‹é™è‡³ {best_val_loss:.4f}ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹æƒé‡ã€‚")
                    ckpt_mgr.save(model, optimizer, scaler,
                                  epoch=epoch, step=global_step,
                                  val_loss=best_val_loss, full=True)

            # --------- æ¯ Epoch æœ«ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ ------------------
            if is_main_process(args):
                ckpt_mgr.save(model, optimizer, scaler,
                              epoch=epoch, step=global_step,
                              val_loss=val_loss, full=True)
                killer.update(epoch, global_step, best_val_loss)  # ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹
            killer.update(epoch, global_step, best_val_loss)   # åŒæ­¥æœ€æ–° best

        if is_main_process(args):
            logger.info("âœ… è®­ç»ƒå®Œæˆ")
    except Exception as e:
        if is_main_process(args):
            logger.error(f"è®­ç»ƒå¼‚å¸¸: {e}")
            logger.info("ğŸ’€ å¼‚å¸¸é€€å‡ºï¼Œæ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹â€¦")
        raise e
    finally:
        cleanup_distributed()


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    args = load_config("./configs/pretrain_config.yaml")

    # ==== æ—¥å¿— ====
    logger = get_logger(args.logging.logs_dir, "pretrain-distributed")

    # ==== è‡ªåŠ¨é™çº§ä¸º CPUï¼ˆè‹¥æ—  CUDAï¼‰====                         
    if args.device.startswith("cuda") and not torch.cuda.is_available():  
        logger.warning("âš ï¸  å½“å‰ç¯å¢ƒæœªæ£€æµ‹åˆ° CUDAï¼Œå·²è‡ªåŠ¨å›é€€åˆ° CPU è®­ç»ƒæ¨¡å¼ã€‚")    
        args.device = "cpu"
        args.enable_ddp = False

    # ==== åˆå§‹åŒ–åˆ†å¸ƒå¼ ====
    init_distributed(args)

    # ==== è®¾ç½®è®¾å¤‡å­—ç¬¦ä¸² ====
    if args.device.startswith("cuda"):
        rank_device = f"cuda:{args.local_rank}"
        args.distributed.device = rank_device if args.enable_ddp else "cuda"
    else:
        logger.warning("âš ï¸  æ£€æµ‹ä¸åˆ° CUDAï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ° CPUã€‚è‹¥æƒ³ç”¨ GPUï¼Œè¯·å®‰è£…å¸¦ CUDA çš„ PyTorchã€‚")
        args.distributed.device = "cpu"

    # ==== è¿›ç¨‹ä¿¡æ¯ ====
    if is_main_process(args):
        logger.info(f"ä¸»è¿›ç¨‹ rank={args.rank}, local_rank={args.local_rank}")
    else:
        logger.info(f"å­è¿›ç¨‹ rank={args.rank}, local_rank={args.local_rank}")

    # ==== è®­ç»ƒ ====
    train(args, logger)
