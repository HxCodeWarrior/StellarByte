import os
import sys
import math
import time
import random
import logging
from datetime import datetime
from contextlib import nullcontext
from types import SimpleNamespace

import torch
import numpy as np
import torch.distributed as dist
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆ†å¸ƒå¼è®­ç»ƒæ ¸å¿ƒ
from torch.utils.data.distributed import DistributedSampler

# å®éªŒè¿½è¸ª
import swanlab

# ç»ˆç«¯ç¾åŒ–è¾“å‡º
from rich.console import Console

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from datasets import PretrainDataset
from model.Model import ByteTransformer
from model.config import ByteModelConfig
from utils.checkpoint import CheckpointManager, GracefulKiller
from utils.progressbar import ProgressBarManager
from utils.logger import register_global_exception_handler, _build_logger
from utils.config_params import load_config

console = Console()

# ========= å…¨å±€æ€§èƒ½ / æ˜¾å­˜ä¼˜åŒ–ï¼‰ =========
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")  # å‡ç¢ç‰‡ :contentReference[oaicite:0]{index=0}
torch.backends.cuda.matmul.allow_tf32 = True        # Ampere+ TF32 â¾ƒåŠ¨é™ç²¾åº¦
torch.backends.cudnn.allow_tf32 = True              # Turing+ TF32 â¾ƒåŠ¨é™ç²¾åº¦
torch.backends.cudnn.benchmark = True               # cuDNN ç®—æ³•è‡ªåŠ¨æœç´¢

# -----------------------------------------------------------------------------
# éšæœºç§å­ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨
# -----------------------------------------------------------------------------
def set_seed(seed: int, args=None) -> int:
    """
    å›ºå®šéšæœºç§å­ï¼›DDP æ—¶å°† rank0 çš„ seed å¹¿æ’­ç»™æ‰€æœ‰è¿›ç¨‹ï¼Œ
    è¿”å›æœ€ç»ˆ seed ä»¥ä¾¿è°ƒç”¨è€…å¤ç”¨ã€‚
    """
    if args and args.enable_ddp:
        # rank0 å†³å®šéšæœºç§å­å¹¶å¹¿æ’­
        seed_tensor = torch.tensor([seed], dtype=torch.long, device=f"cuda:{args.local_rank}")
        torch.distributed.broadcast(seed_tensor, src=0)
        seed = int(seed_tensor.item())

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    return seed


# -----------------------------------------------------------------------------
# è®­ç»ƒè¿‡ç¨‹è¾…åŠ©å‡½æ•°
# -----------------------------------------------------------------------------
def grad_global_norm(parameters) -> float:
    """è®¡ç®—æ‰€æœ‰å¯è®­ç»ƒå‚æ•°æ¢¯åº¦çš„ L2 èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ & æ¢¯åº¦è£å‰ªï¼‰ã€‚"""
    total_norm = 0.0
    for p in parameters:
        if p.grad is None:
            continue
        param_norm  = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
    return math.sqrt(total_norm)

def compute_label_smoothing(logits, labels, loss_mask, label_smoothing=0.1):
    """
    ä½¿ç”¨ Label Smoothing è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ŒåŒæ—¶è€ƒè™‘ loss_maskã€‚
    logits: (B, T, V)
    labels: (B, T)
    loss_mask: (B, T)
    """
    vocab_size = logits.size(-1)
    log_probs = F.log_softmax(logits, dim=-1)  # (B, T, V)

    # æ„é€ å¹³æ»‘æ ‡ç­¾
    with torch.no_grad():
        true_dist = torch.zeros_like(log_probs)  # (B, T, V)
        true_dist.fill_(label_smoothing / (vocab_size - 1))
        ignore_mask = (labels == -100)
        labels = labels.clone()
        labels[ignore_mask] = 0
        true_dist.scatter_(2, labels.unsqueeze(2), 1.0 - label_smoothing)
        true_dist[ignore_mask] = 0  # å¿½ç•¥ pad éƒ¨åˆ†

    # äº¤å‰ç†µæŸå¤±
    loss = -(true_dist * log_probs).sum(dim=-1)  # (B, T)
    loss = loss * loss_mask  # ä»…å¯¹æœ‰æ•ˆ token æ±‚æŸå¤±
    return loss.sum() / (loss_mask.sum() + 1e-8)

def format_size(num_bytes: int) -> str:
    """å°†å­—èŠ‚æ•°æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²ï¼Œå¦‚ 256.0Â MiBã€‚"""
    for unit in ["B", "KiB", "MiB", "GiB", "TiB"]:
        if abs(num_bytes) < 1024.0:
            return f"{num_bytes:3.1f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.1f} PiB"


# -----------------------------------------------------------------------------
# å­¦ä¹ ç‡è°ƒåº¦å™¨
# -----------------------------------------------------------------------------
def get_lr(global_step: int, total_iters: int, args) -> float:
    """
    ä½™å¼¦é€€ç« + çº¿æ€§é¢„çƒ­ + å‘¨æœŸé‡å¯ å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

    å‚æ•°è¯´æ˜
    --------
    global_step : int,å½“å‰å…¨å±€ stepã€‚
    total_iters : int,æ€»è®­ç»ƒæ­¥æ•°ï¼ˆæ‰€æœ‰ epoch * (é‡å¯å‘¨æœŸ+1)ï¼‰ï¼Œè¡¨ç¤ºå•å‘¨æœŸè¿­ä»£æ•°ã€‚
    args : Namespace,å‘½ä»¤è¡Œå‚æ•°é›†åˆã€‚
    """
    # è®¡ç®—å…³é”®èŠ‚ç‚¹
    warmup_iters = int(args.warmup_steps_ratio * total_iters)  # é¢„çƒ­æ­¥æ•°
    decay_steps = int(args.lr_decay_steps_ratio * total_iters) # æ¯æ¬¡è¡°å‡çš„æ­¥é•¿

    # ä¾¿æ·å˜é‡
    min_lr           = args.min_lr
    warmup_start_lr  = args.warmup_start_lr or args.learning_rate / 1_000
    num_restarts     = args.num_restarts
    lr_decay_rate    = args.lr_decay_rate

    cycle_length = total_iters // (num_restarts + 1) # ä¸€ä¸ªå‘¨æœŸé•¿åº¦

    # å¦‚æœè¶…è¿‡æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œè¿”å›æœ€å°å­¦ä¹ ç‡
    if global_step >= total_iters:
        return min_lr

    # ------- 1. çº¿æ€§ + ä½™å¼¦é¢„çƒ­ -------
    if global_step < warmup_iters:
        ratio  = global_step / max(1, warmup_iters)
        cosine = 0.5 * (1 - math.cos(math.pi * ratio))
        return warmup_start_lr + cosine * (args.learning_rate - warmup_start_lr)

    # ------- 2. å¤šå‘¨æœŸä½™å¼¦é€€ç« -------
    cycle_step       = (global_step - warmup_iters) % cycle_length   # å½“å‰å‘¨æœŸå†…çš„æ­¥æ•°
    cycle_idx        = (global_step - warmup_iters) // cycle_length  # å‘¨æœŸç´¢å¼•
    decay_steps_cnt  = cycle_step // decay_steps            # å·²è§¦å‘çš„è¡°å‡æ¬¡æ•°

    decayed_lr       = args.learning_rate * (lr_decay_rate ** decay_steps_cnt)
    decay_ratio      = (cycle_step % decay_steps) / max(1, decay_steps)
    cosine_coeff     = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))

    cycle_base_lr    = decayed_lr * (lr_decay_rate ** cycle_idx)
    current_lr       = min_lr + cosine_coeff * (max(cycle_base_lr, min_lr) - min_lr)
    return max(current_lr, min_lr)


# -----------------------------------------------------------------------------
# éªŒè¯å¾ªç¯
# -----------------------------------------------------------------------------
def evaluate(model, dataloader, args, logger, epoch=None, progressor=None):
    """
    æ•´ä¸ªéªŒè¯é›†å‰å‘æ¨ç†ï¼Œä¸è®¡ç®—æ¢¯åº¦ã€‚

    è¿”å›
    ----
    avg_loss : float
        éªŒè¯é›†å¹³å‡æŸå¤±ï¼Œç”¨äºæ—©åœæˆ–å­¦ä¹ ç‡è°ƒåº¦ã€‚
    """
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    total_loss   = torch.tensor(0.0, device=args.device)
    total_tokens = torch.tensor(0,   device=args.device)
    num_batches = len(dataloader)

    # -------- åˆå§‹åŒ–è¿›åº¦æ¡ --------
    if progressor and is_main_process(args) and epoch is not None:
        progressor.set_epoch(epoch)
        progressor.start_phase(total_steps=num_batches, phase='val')

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(args.device)
            labels    = batch["labels"].to(args.device)
            outputs   = model(input_ids=input_ids, labels=labels)
            # ç´¯åŠ  *æ ·æœ¬æ•°* æ–¹ä¾¿æœ€åæ±‚å¹³å‡
            total_loss   += outputs.loss.detach() * input_ids.size(0)
            total_tokens += input_ids.size(0)

            # æ›´æ–°è¿›åº¦æ¡
            if progressor and is_main_process(args):
                avg_loss_sofar = (total_loss / total_tokens).item()
                progressor.update_phase(loss=avg_loss_sofar)

     # -------- åˆ†å¸ƒå¼æ±‡æ€» --------
    if args.ddp:
        torch.distributed.all_reduce(total_loss,  op=torch.distributed.ReduceOp.SUM)
        torch.distributed.all_reduce(total_tokens, op=torch.distributed.ReduceOp.SUM)

    avg_loss = (total_loss / total_tokens).item()
    ppl      = math.exp(min(20, avg_loss))

    if is_main_process(args):
        logger.info(f"[Eval] Step {epoch} | loss {avg_loss:.4f} | ppl {ppl:.2f}")
        if args.use_swanlab:
            swanlab.log({"val/loss": avg_loss, "val/ppl": ppl}, step=epoch)
    
    if progressor and is_main_process(args):
        progressor.end_phase()

    model.train()  # è¯„ä¼°å®Œè®°å¾—åˆ‡å›è®­ç»ƒæ¨¡å¼
    return avg_loss


# -----------------------------------------------------------------------------
# æ¨¡å‹ä¸åˆ†è¯å™¨åˆå§‹åŒ–
# -----------------------------------------------------------------------------
def init_model(args, logger):
    """æ ¹æ® CLI å‚æ•°æ„é€  ByteTransformer ä¸åˆ†è¯å™¨ã€‚"""

    # 1. åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)

    # 2. æ£€æŸ¥è¯è¡¨å¤§å°æ˜¯å¦å˜åŒ–ï¼ˆæ¯”å¦‚æ·»åŠ äº†pad tokenï¼‰
    if len(tokenizer) > args.vocab_size:
        logger.info(f"æ£€æµ‹åˆ° tokenizer è¯è¡¨å¤§å°å˜ä¸º {len(tokenizer)}ï¼Œæ›´æ–°æ¨¡å‹é…ç½® vocab_size")
        args.vocab_size = len(tokenizer)

    # 3. ç»„è£…é…ç½®
    config = ByteModelConfig(
        vocab_size           = args.vocab_size,
        dim                  = args.dim,
        n_layers             = args.n_layers,
        n_heads              = args.n_heads,
        n_kv_heads           = args.n_kv_heads,
        hidden_dim           = args.hidden_dim,
        multiple_of          = args.dim_multiplier,
        max_seq_len          = args.max_seq_len,
        drop_path_prob       = args.drop_path_prob,
        hidden_dropout       = args.hidden_dropout_prob,
        attention_dropout    = args.attention_dropout_prob,
        residual_dropout     = args.residual_dropout_prob,
        layer_norm_eps       = args.layer_norm_eps,
        initializer_range    = args.initializer_range,
        layerscale_init_value= args.layerscale_init,
        tie_word_embeddings  = args.tie_word_embeddings,
        xpos_rope_theta      = args.rope_theta,
        xpos_scale_base      = args.scale_base,
        use_flash_attention  = args.use_flash_attention,
        causal               = args.use_causal,
        use_cache            = args.use_cache,
        key_dtype            = args.key_dtype,
        value_dtype          = args.value_dtype,
        model_parallel_size  = args.model_parallel_size,
        tensor_parallel_size = args.tensor_parallel_size,
        tensor_parallel_rank = args.tensor_parallel_rank,
    )

    # 4. æ„å»ºæ¨¡å‹
    model = ByteTransformer(config).to(args.device)

    # 5. æ¢¯åº¦æ£€æŸ¥ç‚¹
    if getattr(args, "grad_checkpoint", False):
        model.gradient_checkpointing_enable()
        logger.info("âœ… å·²å¯ç”¨ Gradient Checkpointing")
 
    # 6. torch.compileï¼ˆåå+æ˜¾å­˜åŒèµ¢ï¼‰
    if getattr(args, "use_torch_compile", False) and hasattr(torch, "compile"):
        mode = getattr(args, "compile_mode", "max-autotune")
        model = torch.compile(model, mode=mode, fullgraph=False)
        logger.info(f"ğŸš€ torch.compile(mode='{mode}') å·²å¯ç”¨")  # :contentReference[oaicite:1]{index=1}
 
    # 7. å¹¶è¡ŒåŒ…è£…
    if args.enable_ddp:
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[args.local_rank],
            output_device=args.local_rank,
            find_unused_parameters=False
        )
    elif torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)

    # 8. æ‰“å°å‚æ•°è§„æ¨¡
    param_cnt = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"æ¨¡å‹å‚æ•°æ€»é‡: {param_cnt/1e6:.2f}M")

    return model, tokenizer, config


# -----------------------------------------------------------------------------
# åˆ†å¸ƒå¼è®­ç»ƒ
# -----------------------------------------------------------------------------
def init_distributed(args):
    """DDP åˆå§‹åŒ–ï¼ˆtorchrun ç¯å¢ƒä¸‹è‡ªåŠ¨è¯»å– env å˜é‡ï¼‰"""
    # ---- æ£€æŸ¥ CUDA è®¾å¤‡ ----
    if args.device.startswith("cuda") and not torch.cuda.is_available():
        raise RuntimeError("DDP å·²å¯ç”¨ï¼Œä½†å½“å‰æœªæ£€æµ‹åˆ°å¯ç”¨ CUDAï¼Œè¯·å®‰è£… GPU ç‰ˆ PyTorch æˆ–è®¾ç½® enable_ddp=False")

    # ---- torchrun æ³¨å…¥çš„ç¯å¢ƒå˜é‡ ----
    args.rank = int(os.environ["RANK"])            # å…¨å±€ rank
    args.world_size = int(os.environ["WORLD_SIZE"])# å…¨å±€è¿›ç¨‹æ•°
    args.local_rank = int(os.environ["LOCAL_RANK"])# æœ¬èŠ‚ç‚¹å±€éƒ¨ rank

    # ---- è®¾å¤‡ã€è¿›ç¨‹ç»„ ----
    backend="nccl" if torch.cuda.is_available() and sys.platform != "win32" else "gloo"
    torch.cuda.set_device(args.local_rank)
    torch.distributed.init_process_group(
        backend=backend,
        init_method="env://"
    )
    torch.distributed.barrier(device_ids=[args.local_rank])

def cleanup_distributed():
    """è®­ç»ƒç»“æŸåé”€æ¯è¿›ç¨‹ç»„ï¼Œé‡Šæ”¾èµ„æºã€‚"""
    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()

def is_main_process(args) -> bool:
    """åˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦ä¸»è¿›ç¨‹ï¼ˆrank0ï¼‰"""
    return (not args.enable_ddp) or args.rank == 0

def sync_metrics(total_loss, total_correct, total_tokens, args):
    if args.enable_ddp and dist.is_initialized():
        t_loss = torch.tensor(total_loss, device=args.device)
        t_correct = torch.tensor(total_correct, device=args.device)
        t_tokens = torch.tensor(total_tokens, device=args.device)

        dist.all_reduce(t_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(t_correct, op=dist.ReduceOp.SUM)
        dist.all_reduce(t_tokens, op=dist.ReduceOp.SUM)

        total_loss = t_loss.item()
        total_correct = t_correct.item()
        total_tokens = t_tokens.item()

    return total_loss, total_correct, total_tokens

# -----------------------------------------------------------------------------
# å•ä¸ª epoch è®­ç»ƒ
# -----------------------------------------------------------------------------
def train_epoch(model, dataloader, tokenizer, optimizer, scaler, ctx, args, epoch, 
                total_iters, global_state, best_val_loss, logger, ckpt_mgr, killer):
    """æ‰§è¡Œä¸€ä¸ª epoch çš„å‰å‘ã€åå‘ä¸æ¢¯åº¦æ›´æ–°ã€‚"""

    model.train()

    global_step = global_state.global_step
    total_loss    = 0.0 # ç´¯è®¡æ€»æŸå¤±
    total_correct = 0   # æ­£ç¡®é¢„æµ‹çš„ token æ•°
    total_tokens  = 0   # æ€»é¢„æµ‹ token æ•°
    num_batches   = len(dataloader)
    
    # DDP sampler æ´—ç‰Œ
    if args.enable_ddp and isinstance(dataloader.sampler, DistributedSampler):
        dataloader.sampler.set_epoch(epoch)

    # æ¸…ç†æ¢¯åº¦
    optimizer.zero_grad(set_to_none=True)

    # åˆå§‹åŒ–ååç‡æ—¶é—´åŸºå‡†ï¼ˆä¸ºæ¯æ­¥è®¡ç®—ååï¼‰
    start_wall = time.perf_counter()

    # --------------------------------------------------
    # éå†æ•°æ®é›†
    # --------------------------------------------------
    for step, batch in enumerate(dataloader):
        # æ¯ N æ­¥å†æ¸…ä¸€æ¬¡ï¼Œé¿å…å¼ºåŒæ­¥å¯¼è‡´ååä¸‹é™
        if (step % args.empty_cache_interval) == 0:
            torch.cuda.empty_cache()
        input_ids          = batch["input_ids"].to(args.device)
        labels             = batch["labels"].to(args.device)
        loss_mask          = batch["loss_mask"].to(args.device)
        attention_mask     = (input_ids != tokenizer.pad_token_id).long()
        tokens_this_batch  = input_ids.numel()

        # è®¡ç®—å…¨å±€æ­¥æ•°  
        global_state.global_step += 1

        # â€”â€” åŠ¨æ€å­¦ä¹ ç‡ â€”â€”
        lr = get_lr(global_state.global_step, total_iters, args)
        for pg in optimizer.param_groups:
            pg["lr"] = lr

        # â€”â€” å‰å‘ + åå‘ â€”â€”
        with ctx:  # æ”¯æŒ AMP autocast
            if torch.cuda.is_available():
                torch.compiler.cudagraph_mark_step_begin()  # æ–°å›¾å¼€å§‹æ ‡è®°
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss    = outputs.loss / args.accumulation_steps
            logits  = outputs.logits.detach()  # [B, T, Vocab]
            predictions = torch.argmax(logits, dim=-1)  # [B, T],é¢„æµ‹ token id
        # è‡ªåŠ¨æ··åˆç²¾åº¦
        scaler.scale(loss).backward()

        # â€”â€” æ¢¯åº¦ç´¯ç§¯ â€”â€”
        # æ¢¯åº¦ç´¯ç§¯åˆ°æŒ‡å®šæ­¥æ•°ï¼Œè¿›è¡Œæ¢¯åº¦è£å‰ªä¸å‚æ•°æ›´æ–°
        if (global_state.global_step % args.accumulation_steps) == 0:
            # åç¼©æ”¾åè£å‰ª & è®°å½•æ¢¯åº¦èŒƒæ•°
            scaler.unscale_(optimizer)
            total_grad_norm = grad_global_norm(model.parameters())
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

            # æ›´æ–°å‚æ•°
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
        else:
            total_grad_norm = 0.0  # éæ›´æ–°æ­¥ï¼Œæ¢¯åº¦èŒƒæ•°ç½® 0 ä»…åšæ—¥å¿—å ä½

        # â€”â€” æŒ‡æ ‡ç»Ÿè®¡ â€”â€”
        loss_item     = loss.item() * args.accumulation_steps
        total_loss    += loss_item * loss_mask.sum().item()

        # å‡†ç¡®ç‡
        valid_mask    = (labels != -100)
        correct       = ((predictions == labels) & valid_mask).sum().item()
        total_correct += correct
        total_tokens  += valid_mask.sum().item()

        acc           = total_correct / (total_tokens + 1e-8)
        ppl           = math.exp(min(20, loss_item))
        
        # è®¡ç®—æ¯ç§’å¤„ç† token æ•°ï¼ˆé€šè¿‡æ—¶é—´å·®ä¼°ç®—ï¼‰
        tokens_per_s  = tokens_this_batch / max(1e-6, time.perf_counter() - start_wall)
        gpu_mem       = torch.cuda.memory_allocated(args.device) if torch.cuda.is_available() else 0

        if is_main_process(args):
            # â€”â€” è½»é‡ checkpoint(æŒ‰æ­¥) â€”â€”
            # æ¯ N æ­¥ä¿å­˜è½»é‡ checkpoint
            if ckpt_mgr.should_save(global_state.global_step) and (step) % args.save_interval == 0:
                ckpt_mgr.save(model, 
                              optimizer, 
                              scaler,
                              epoch=epoch, 
                              step=global_state.global_step,
                              full=False)
            # â€”â€” æ›´æ–° Killer â€”â€”
            killer.update(epoch, global_step, best_val_loss)

            # â€”â€” æ—¥å¿—æ‰“å° & SwanLab â€”â€”
            if global_state.global_step % args.log_interval == 0:
                logger.info(
                    f"E{epoch+1} S{global_state.global_step} | Loss {loss_item:.4f} | PPL {ppl:.1f} | ACC {acc:.4f} "
                    f"| LR {lr:.6g} | GradNorm {total_grad_norm:.2f} | Tokens/s {tokens_per_s:.0f} "
                    f"| Mem {format_size(gpu_mem)}")
                if args.use_swanlab:
                    swanlab.log({
                        "loss": loss_item,
                        "perplexity": ppl,
                        "token_accuracy": acc,
                        "lr": lr,
                        "grad_norm": total_grad_norm,
                        "tokens_per_sec": tokens_per_s,
                        "gpu_mem": gpu_mem / (1024**2),  # è½¬ MB
                    })


    # â€”â€” åŒæ­¥æŒ‡æ ‡ â€”â€”
    if args.enable_ddp:
        total_loss, total_correct, total_tokens = sync_metrics(total_loss, total_correct, total_tokens, args)

    avg_loss = total_loss / (total_tokens + 1e-8)
    accuracy = total_correct / (total_tokens + 1e-8)
    if is_main_process(args):
        logger.info(f"[Epoch {epoch+1}] å¹³å‡æŸå¤± {avg_loss:.4f}, å‡†ç¡®ç‡ {accuracy:.4f}")


# -----------------------------------------------------------------------------
# ä¸»è®­ç»ƒæµç¨‹
# -----------------------------------------------------------------------------
def train(args, logger):
    """ä¸»è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æ¨¡å‹åˆå§‹åŒ–ã€è®­ç»ƒä¸éªŒè¯ã€SwanLab æ¥å…¥ç­‰ã€‚"""
    set_seed(args.seed, args)

    # --------- åˆå§‹åŒ–ç»„ä»¶ ---------
    # åˆå§‹åŒ– SwanLab å®éªŒ
    if is_main_process(args) and args.use_swanlab:
        swanlab.login(api_key=args.swanlab_api_key)
        swanlab.init(
            project=args.swanlab_project,
            experiment_name=args.swanlab_experiment_name,
            config=vars(args)
        )

     # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    ckpt_mgr = CheckpointManager(
        args.checkpoints_dir,
        keep_latest=args.keep_latest,
        keep_epoch=args.keep_epoch,
        keep_best=args.keep_best,
        save_every_n_steps=args.save_interval
    )

    # åˆå§‹åŒ–æ¨¡å‹ã€åˆ†è¯å™¨ä¸é…ç½®
    model, tokenizer, config = init_model(args, logger)

    # åŠ è½½è®­ç»ƒé›†
    # 1. åŠ è½½è®­ç»ƒé›†
    train_dataset = PretrainDataset(
        args.train_data_path, 
        tokenizer, 
        max_length=config.max_seq_len,
        fields=args.dataset_loader.fields,
        template=args.dataset_loader.template if args.dataset_loader.template else None,
        add_bos=args.dataset_loader.add_bos
    )
    # 2. æ ¹æ®åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®é‡‡æ ·å™¨
    if args.enable_ddp:
        train_sampler = DistributedSampler(train_dataset, shuffle=True)
        shuffle_flag  = False
    else:
        train_sampler = None
        shuffle_flag  = True
    # 3. æ„å»º Train_DataLoader
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=shuffle_flag, 
        sampler=train_sampler,
        num_workers=args.num_workers,
        pin_memory=True
    )

    # åŠ è½½éªŒè¯é›†ï¼ˆå¦‚æœæä¾›ï¼‰
    val_loader = None
    if args.val_data_path:
        # 1. åŠ è½½éªŒè¯é›†
        val_dataset = PretrainDataset(
            args.val_data_path, 
            tokenizer, 
            max_length=config.max_seq_len,
            fields=args.dataset_loader.fields,
            template=args.dataset_loader.template if args.dataset_loader.template else None,
            add_bos=args.dataset_loader.add_bos
        )
        # 2. æ ¹æ®åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®é‡‡æ ·å™¨
        if args.enable_ddp:
            val_sampler = DistributedSampler(val_dataset, shuffle=False)
        else:
            val_sampler = None
        # 3. æ„å»º Val_DataLoader
        val_loader  = DataLoader(
            val_dataset, 
            batch_size=args.eval_batch_size, 
            shuffle=False,
            sampler=val_sampler,
            num_workers=args.num_workers,
            pin_memory=True
        )
        args.val_loader = val_loader

    # æ„å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate, 
        weight_decay=args.weight_decay
    )

    # å­¦ä¹ ç‡è°ƒåº¦ä¸AMPé…ç½®
    steps_per_epoch = len(train_loader)  
    total_iters = steps_per_epoch * args.epochs * (args.num_restarts + 1)
    use_amp     = args.amp or args.dtype in ['float16', 'bfloat16']
    scaler      = torch.amp.GradScaler(enabled=use_amp)
    device_type = 'cuda' if 'cuda' in args.device else 'cpu'
    ctx         = (nullcontext() if device_type == 'cpu'
                   else torch.amp.autocast(device_type=device_type, dtype=getattr(torch, args.dtype))
                   if use_amp else nullcontext())

    # åˆå§‹åŒ– GracefulKillerï¼Œç»‘å®š checkpoint ç®¡ç†å™¨
    killer = GracefulKiller(model, optimizer, scaler, ckpt_mgr, logger, sync=True)

    start_epoch   = 0
    global_state   = SimpleNamespace(global_step=0)
    val_loss      = None
    best_val_loss = float("inf")

    # ---------- æ¢å¤ ----------
    try:
        logger.info("ğŸ” æ£€æµ‹åˆ°å†å²æ£€æŸ¥ç‚¹ï¼Œæ­£åœ¨æ¢å¤ä¸­â€¦")
        ckpt = ckpt_mgr.load_latest(model, optimizer, scaler)
        start_epoch  = ckpt['epoch'] + 1
        global_state.global_step  = ckpt['global_step']
        best_val_loss = ckpt.get('val_loss', float('inf'))
        logger.info(f"ğŸª„ å·²æ¢å¤åˆ° epoch {start_epoch}, step {global_state.global_step}")
    except FileNotFoundError:
        logger.info("ğŸ†• æœªæ£€æµ‹åˆ° checkpointï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ")
        start_epoch, best_val_loss = 0, float('inf')

    # ---------- è®­ç»ƒ ----------
    try:
        for epoch in range(start_epoch, args.epochs):
            total_iters = len(train_loader)
            train_epoch(
                model, train_loader, tokenizer, optimizer, scaler, ctx, args, epoch,
                total_iters, global_state, best_val_loss, logger,  ckpt_mgr, killer
            )

            # -------------- éªŒè¯ (åªåœ¨ä¸»è¿›ç¨‹) --------------------
            if val_loader and is_main_process(args) and (epoch + 1) % args.eval_interval == 0:
                val_loss = evaluate(model, val_loader, args, logger, epoch, progress=None)

                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    ckpt_mgr.save(model, 
                                  optimizer, 
                                  scaler,
                                  epoch=epoch, 
                                  step=global_state.global_step,
                                  val_loss=best_val_loss, 
                                  full=True)
                    logger.info(f"ğŸ‰ éªŒè¯é›†æŸå¤±ä¸‹é™è‡³ {best_val_loss:.4f}ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹æƒé‡ã€‚")

            # --------- æ¯ Epoch æœ«ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ ------------------
            if is_main_process(args) and (epoch + 1) % args.save_interval == 0:
                ckpt_mgr.save(model, 
                              optimizer, 
                              scaler,
                              epoch=epoch,
                              step=global_state.global_step,
                              val_loss=val_loss, 
                              full=True)
                killer.update(epoch, global_state.global_step, best_val_loss)  # ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹
            killer.update(epoch, global_state.global_step, best_val_loss)   # åŒæ­¥æœ€æ–° best

        if is_main_process(args):
            logger.info("âœ… è®­ç»ƒå®Œæˆ")
    except Exception as e:
        if is_main_process(args):
            logger.error(f"è®­ç»ƒå¼‚å¸¸: {e}")
            logger.info("ğŸ’€ å¼‚å¸¸é€€å‡ºï¼Œæ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹â€¦")
        raise e
    finally:
        cleanup_distributed()


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    # ==== åŠ è½½é…ç½® ====
    args = load_config("./configs/pretrain_config.yaml")

    # ==== æ—¥å¿— ====
    log_file_path = os.path.join(args.logs_dir, f"pretrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    logger = _build_logger(
        logger_name="ByteLogger",
        log_file=log_file_path,
        log_level=logging.DEBUG,
        console_level=logging.INFO,
        enable_color=True,
    )
    register_global_exception_handler(logger)

    # ==== è‡ªåŠ¨é™çº§ä¸º CPUï¼ˆè‹¥æ—  CUDAï¼‰====                         
    if args.device.startswith("cuda") and not torch.cuda.is_available():  
        logger.warning("âš ï¸  å½“å‰ç¯å¢ƒæœªæ£€æµ‹åˆ° CUDAï¼Œå·²è‡ªåŠ¨å›é€€åˆ° CPU è®­ç»ƒæ¨¡å¼ã€‚")    
        args.device = "cpu"
        args.enable_ddp = False

    # ==== åˆå§‹åŒ–åˆ†å¸ƒå¼ ====
    if args.enable_ddp:
        init_distributed(args)
    else:
        args.rank = 0
        args.world_size = 1
        args.local_rank = 0

    # ==== è®¾ç½®è®¾å¤‡å­—ç¬¦ä¸² ====
    if args.device.startswith("cuda"):
        rank_device = f"cuda:{args.local_rank}"
        args.device = rank_device if args.enable_ddp else "cuda"
    else:
        logger.warning("âš ï¸  æ£€æµ‹ä¸åˆ° CUDAï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ° CPUã€‚è‹¥æƒ³ç”¨ GPUï¼Œè¯·å®‰è£…å¸¦ CUDA çš„ PyTorchã€‚")
        args.device = "cpu"

    # ==== è¿›ç¨‹ä¿¡æ¯ ====
    if is_main_process(args):
        logger.info(f"ä¸»è¿›ç¨‹ rank={args.rank}, local_rank={args.local_rank}")
    else:
        logger.info(f"å­è¿›ç¨‹ rank={args.rank}, local_rank={args.local_rank}")

    # ==== è®­ç»ƒ ====
    train(args, logger)
