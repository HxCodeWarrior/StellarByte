import os
import math
import torch
import random
import argparse
import numpy as np
from contextlib import nullcontext
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

from model.Model import ByteTransformer
from datasets import PretrainDataset
from utils.logger import get_logger
from utils.checkpoint import CheckpointManager
from utils.progressbar import RichProgressBar
from model.config import ByteModelConfig


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def get_lr(it, all_iters, args):
    warmup_iters = int(args.warmup_steps_ratio * all_iters)
    decay_steps = int(args.lr_decay_steps_ratio * all_iters)

    min_lr = args.min_lr
    warmup_start_lr = args.warmup_start_lr
    num_restarts = args.num_restarts
    lr_decay_rate = args.lr_decay_rate

    cycle_length = all_iters
    total_iters = all_iters * (num_restarts + 1)

    if it >= total_iters:
        return min_lr

    if it < warmup_iters:
        ratio = it / max(1, warmup_iters)
        cosine = 0.5 * (1 - math.cos(math.pi * ratio))
        return warmup_start_lr + cosine * (args.learning_rate - warmup_start_lr)

    cycle_step = (it - warmup_iters) % cycle_length
    cycle_idx = (it - warmup_iters) // cycle_length
    decay_steps_count = cycle_step // decay_steps

    decayed_lr = args.learning_rate * (lr_decay_rate ** decay_steps_count)
    decay_ratio = (cycle_step % decay_steps) / max(1, decay_steps)
    cosine_coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))

    cycle_base_lr = decayed_lr * (lr_decay_rate ** cycle_idx)
    current_lr = min_lr + cosine_coeff * (max(cycle_base_lr, min_lr) - min_lr)
    return max(current_lr, min_lr)


def evaluate(model, dataloader, args, logger):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(args.device)
            labels = batch['labels'].to(args.device)
            outputs = model(input_ids=input_ids, labels=labels)
            total_loss += outputs.loss.item()
    avg_loss = total_loss / len(dataloader)
    logger.info(f"[Eval] Validation Loss: {avg_loss:.4f}")
    return avg_loss


def init_model(args, logger):
    lm_config = ByteModelConfig(
        vocab_size=args.vocab_size,
        dim=args.model_dim,
        n_layers=args.num_layers,
        n_heads=args.num_attention_heads,
        n_kv_heads=args.num_kv_heads,
        hidden_dim=args.hidden_dim,
        multiple_of=args.dim_multiplier,
        max_seq_len=args.max_seq_len,
        drop_path_prob=args.drop_path_prob,
        hidden_dropout=args.hidden_dropout_prob,
        attention_dropout=args.attention_dropout_prob,
        residual_dropout=args.residual_dropout_prob,
        layer_norm_eps=args.layer_norm_eps,
        initializer_range=args.initializer_range,
        layerscale_init_value=args.layerscale_init,
        tie_word_embeddings=args.tie_word_embeddings,
        xpos_rope_theta=args.xpos_rope_theta,
        xpos_scale_base=args.xpos_scale_base,
        use_flash_attention=args.use_flash_attention,
        causal=args.use_causal,
        use_cache=args.use_cache,
        key_dtype=args.key_cache_dtype,
        value_dtype=args.value_cache_dtype,
        model_parallel_size=args.model_parallel_size,
        tensor_parallel_size=args.tensor_parallel_size,
        tensor_parallel_rank=args.tensor_parallel_rank,
    )

    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    model = ByteTransformer(lm_config)
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    model = model.to(args.device)

    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"æ¨¡å‹å‚æ•°æ€»é‡ï¼š{param_count / 1e6:.2f}M")
    return model, tokenizer, lm_config


def train_epoch(model, dataloader, optimizer, scaler, ctx, args, epoch, total_iters, logger, global_step):
    model.train()
    total_loss = 0.0
    total_steps = len(dataloader)
    total = args.epochs * total_steps

    with RichProgressBar(total_steps=total, total_batches=total_steps, total_epochs=args.epochs,desc="Training") as pbar:
        for step, batch in enumerate(dataloader, 1):
            input_ids = batch['input_ids'].to(args.device)
            labels = batch['labels'].to(args.device)

            lr = get_lr(global_step, total_iters, args)
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

            with ctx:
                outputs = model(input_ids=input_ids, labels=labels)
                loss = outputs.loss / args.accumulation_steps

            scaler.scale(loss).backward()

            if (step + 1) % args.accumulation_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)

            total_loss += loss.item()
            global_step += 1

            if global_step % args.log_interval == 0:
                logger.info(f"[Epoch {epoch} | Step {global_step}] Loss: {loss.item() * args.accumulation_steps:.4f}, LR: {lr:.8f}")

            if args.val_data_path and global_step % args.eval_interval == 0:
                evaluate(model, dataloader, args, logger)

            if global_step % args.save_interval == 0:
                args.ckpt_mgr.save_checkpoint(model, optimizer, None, epoch, step=global_step)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.update_loader(step)
            pbar.update_train(global_step, epoch+1, loss=loss.item() * args.accumulation_steps, lr=lr)

    avg_loss = total_loss / len(dataloader)
    logger.info(f"[Epoch {epoch}] å¹³å‡æŸå¤±: {avg_loss:.4f}")
    return global_step


def train(args, logger):
    set_seed(args.seed)

    model, tokenizer, config = init_model(args, logger)
    dataset = PretrainDataset(args.train_data_path, tokenizer, max_length=config.max_seq_len)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)

    val_loader = None
    if args.val_data_path:
        val_dataset = PretrainDataset(args.val_data_path, tokenizer, max_length=config.max_seq_len)
        val_loader = DataLoader(val_dataset, batch_size=args.eval_batch_size, shuffle=False, num_workers=args.num_workers)

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    total_iters = args.epochs * len(dataloader)

    use_amp = args.amp or args.dtype in ['float16', 'bfloat16']
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
    ctx = nullcontext() if args.device == 'cpu' else torch.cuda.amp.autocast(dtype=getattr(torch, args.dtype)) if use_amp else nullcontext()

    ckpt_mgr = CheckpointManager(args.checkpoints_dir)
    start_epoch = 0
    global_step = 0
    if ckpt_mgr.has_checkpoint():
        logger.info("æ¢å¤æ¨¡å‹æƒé‡ä¸­...")
        checkpoint = ckpt_mgr.load_checkpoint(model, optimizer, None)
        start_epoch = checkpoint.get("epoch", 0) + 1
        global_step = checkpoint.get("step", 0)

    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒâ€¦")
    for epoch in range(start_epoch, args.epochs):
        global_step = train_epoch(model, dataloader, optimizer, scaler, ctx, args, epoch, total_iters, logger, global_step)
        if val_loader:
            evaluate(model, val_loader, args, logger)
        ckpt_mgr.save_checkpoint(model, optimizer, None, epoch)
    logger.info("âœ… è®­ç»ƒç»“æŸã€‚")


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # =============================
    #       å®éªŒåŸºç¡€é…ç½®
    # =============================
    parser.add_argument("--seed", type=int, default=42, help="å…¨å±€éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°æ€§")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",help="è®­ç»ƒè®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--use_swanlab", action="store_true", help="æ˜¯å¦å¯ç”¨SwanLabå®éªŒè¿½è¸ª")
    parser.add_argument("--swanlab_project", type=str, default="Happy-LLM", help="SwanLabé¡¹ç›®åç§°")
    parser.add_argument("--swanlab_experiment_name", type=str, default="Pretrain-215M", help="SwanLabå®éªŒåç§°")
    parser.add_argument("--swanlab_api_key", type=str, default="",  help="SwanLab APIè®¤è¯å¯†é’¥")
    parser.add_argument("--logs_dir", type=str, default="./logs", help="æ—¥å¿—è¾“å‡ºç›®å½•")
    parser.add_argument("--checkpoints_dir", type=str, default="./checkpoints", help="æ¨¡å‹æ£€æŸ¥ç‚¹è¾“å‡ºç›®å½•")

    # =============================
    #       æ•°æ®é›†é…ç½®
    # =============================
    parser.add_argument("--tokenizer_path", type=str, default="./tokenizer", help="åˆ†è¯å™¨é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--train_data_path", type=str, default="./datasets/test/train.jsonl", help="è®­ç»ƒæ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--val_data_path", type=str, default="./datasets/test/val.jsonl", help="éªŒè¯æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")

    # =============================
    #       æ¨¡å‹æ¶æ„é…ç½®
    # =============================
    parser.add_argument("--vocab_size", type=int, default=32000, help="è¯æ±‡è¡¨å¤§å°")
    parser.add_argument("--model_dim", type=int, default=768, help="æ¨¡å‹éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_layers", type=int, default=12, help="Transformerå±‚æ•°")
    parser.add_argument("--num_attention_heads", type=int, default=16, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--num_kv_heads", type=int, default=8, help="Key/Valueæ³¨æ„åŠ›å¤´æ•°ï¼ˆå¤´åˆ†ç¦»ï¼‰")
    parser.add_argument("--hidden_dim", type=int, default=None, help="FFNéšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤4*model_dimï¼‰")
    parser.add_argument("--dim_multiplier", type=int, default=4, help="éšè—å±‚ç»´åº¦å¯¹é½åŸºæ•°")
    parser.add_argument("--max_seq_len", type=int, default=2048, help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # Dropout å‚æ•°
    parser.add_argument("--drop_path_prob", type=float, default=0.0, help="æ®‹å·®è·¯å¾„Dropoutæ¦‚ç‡")
    parser.add_argument("--hidden_dropout_prob", type=float, default=0.1, help="éšè—å±‚Dropoutæ¦‚ç‡")
    parser.add_argument("--attention_dropout_prob", type=float, default=0.1, help="æ³¨æ„åŠ›Dropoutæ¦‚ç‡")
    parser.add_argument("--residual_dropout_prob", type=float, default=0.1, help="æ®‹å·®è¿æ¥Dropoutæ¦‚ç‡")
    
    # å½’ä¸€åŒ–å‚æ•°
    parser.add_argument("--layer_norm_eps", type=float, default=1e-5, help="å±‚å½’ä¸€åŒ–epsilon")
    parser.add_argument("--initializer_range", type=float, default=0.02, help="å‚æ•°åˆå§‹èŒƒå›´")
    parser.add_argument("--layerscale_init", type=float, default=1e-5, help="å±‚ç¼©æ”¾åˆå§‹åŒ–å€¼")
    
    # åµŒå…¥å‚æ•°
    parser.add_argument("--tie_word_embeddings", action="store_true", help="ç»‘å®šè¾“å…¥è¾“å‡ºè¯åµŒå…¥")
    
    # ä½ç½®ç¼–ç å‚æ•°
    parser.add_argument("--xpos_rope_theta", type=float, default=10000.0, help="XPosä½ç½®ç¼–ç theta")
    parser.add_argument("--xpos_scale_base", type=float, default=512.0, help="XPosç¼©æ”¾å› å­")
    
    # æ³¨æ„åŠ›æœºåˆ¶
    parser.add_argument("--use_flash_attention", action="store_true", help="å¯ç”¨FlashAttention")
    parser.add_argument("--use_causal", action="store_true", help="ä½¿ç”¨å› æœæ³¨æ„åŠ›æ©ç ")
    
    # æ¨ç†ä¼˜åŒ–
    parser.add_argument("--use_cache", action="store_true", help="å¯ç”¨KVç¼“å­˜åŠ é€Ÿæ¨ç†")
    parser.add_argument("--key_cache_dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"], help="Keyç¼“å­˜æ•°æ®ç±»å‹")
    parser.add_argument("--value_cache_dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"], help="Valueç¼“å­˜æ•°æ®ç±»å‹")
    
    # å¹¶è¡Œè®­ç»ƒ
    parser.add_argument("--model_parallel_size", type=int, default=1, help="æ¨¡å‹å¹¶è¡Œå¤§å°")
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="å¼ é‡å¹¶è¡Œå¤§å°")
    parser.add_argument("--tensor_parallel_rank", type=int, default=0, help="å¼ é‡å¹¶è¡Œrank")

    # =============================
    #       è®­ç»ƒè¶…å‚æ•°é…ç½®
    # =============================
    parser.add_argument("--batch_size", type=int, default=32, help="æ¯æ‰¹æ¬¡æ ·æœ¬æ•°é‡")
    parser.add_argument("--epochs", type=int, default=5, help="è®­ç»ƒæ€»è½®æ¬¡")
    parser.add_argument("--accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§batch sizeï¼‰")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰")

    # =============================
    #       ä¼˜åŒ–å™¨å‚æ•°é…ç½®
    # =============================
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="åŸºç¡€å­¦ä¹ ç‡")
    parser.add_argument("--min_lr", type=float, default=5e-6, help="å­¦ä¹ ç‡æœ€å°å€¼ï¼ˆä½™å¼¦é€€ç«ä¸‹é™ï¼‰")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="AdamWä¼˜åŒ–å™¨æƒé‡è¡°å‡ç³»æ•°")

    # =============================
    #       å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    # =============================
    parser.add_argument("--warmup_steps_ratio", type=float, default=0.05, help="é¢„çƒ­é˜¶æ®µå æ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ä¾‹")
    parser.add_argument("--warmup_start_lr", type=float, default=5e-7, help="é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡ï¼ˆé»˜è®¤: learning_rate/1000ï¼‰")
    parser.add_argument("--lr_decay_rate", type=float, default=0.8,  help="å­¦ä¹ ç‡è¡°å‡ç‡ï¼ˆå¤šå‘¨æœŸä½™å¼¦é€€ç«ï¼‰")
    parser.add_argument("--lr_decay_steps_ratio", type=int, default=0.3, help="å­¦ä¹ ç‡è¡°å‡é—´éš”æ¯”ä¾‹ï¼ˆæ­¥è¿›å¼è¡°å‡ï¼‰")
    parser.add_argument("--num_restarts", type=int, default=0,  help="ä½™å¼¦é€€ç«é‡å¯æ¬¡æ•°ï¼ˆ0è¡¨ç¤ºå•å‘¨æœŸï¼‰")

    # =============================
    #       æ··åˆç²¾åº¦è®­ç»ƒé…ç½®
    # =============================
    parser.add_argument("--dtype", type=str, default="bfloat16", 
                       choices=["float32", "float16", "bfloat16"],
                       help="è®­ç»ƒæ•°æ®ç±»å‹ï¼ˆfloat32/full precision, float16/half, bfloat16/brain floatï¼‰")
    parser.add_argument("--amp", action="store_true", help="å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä¸dtypeäº’æ–¥ï¼‰")

    # =============================
    #       æ—¥å¿—ä¸ä¿å­˜é…ç½®
    # =============================
    parser.add_argument("--log_interval", type=int, default=100, help="è®­ç»ƒæ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--save_interval", type=int, default=1000, help="æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--eval_interval", type=int, default=2000, help="éªŒè¯é›†è¯„ä¼°é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--max_checkpoints", type=int, default=5, help="æœ€å¤§ä¿ç•™çš„æ£€æŸ¥ç‚¹æ•°é‡")

    # =============================
    #       éªŒè¯é…ç½®
    # =============================
    parser.add_argument("--eval_batch_size", type=int, default=64, help="éªŒè¯æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--eval_max_steps", type=int, default=100, help="æœ€å¤§éªŒè¯æ­¥æ•°ï¼ˆå…¨éªŒè¯é›†è¿‡å¤§æ—¶ä½¿ç”¨ï¼‰")
    args = parser.parse_args()
    
    # -------- æ—¥å¿—ç³»ç»Ÿ --------
    logger = get_logger(args.logs_dir, args.swanlab_experiment_name)
    logger.info("é…ç½®å‚æ•°:\n" + str(vars(args)))

    train(args, logger)  # å¯åŠ¨è®­ç»ƒ
