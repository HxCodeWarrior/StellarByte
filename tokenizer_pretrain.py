import random
import json
import os
import json
import re
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from transformers import AutoTokenizer, PreTrainedTokenizerFast
from tokenizers import (
    decoders,
    models,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
    Regex,
    normalizers
)
from tokenizers.normalizers import (
    NFKC, 
    NFD, 
    StripAccents,
    Replace,
    Lowercase
)
from tokenizers.pre_tokenizers import (
    ByteLevel,
    Digits,
    Punctuation,
    Metaspace,
    Split
)
from typing import Generator, List, Dict, Union, Optional
from emoji import demojize
import unicodedata

random.seed(42)

def read_data_from_jsonl(
    file_path: str, 
    text_fields: Optional[List[str]] = None,
    fallback_field: str = "text"
) -> Generator[str, None, None]:
    """
    è¯»å–JSONLæ–‡ä»¶ï¼Œæ”¯æŒå¤šç§å­—æ®µç»“æ„å’Œå›é€€æœºåˆ¶
    
    å‚æ•°:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
        text_fields: å°è¯•æå–çš„æ–‡æœ¬å­—æ®µä¼˜å…ˆçº§åˆ—è¡¨
        fallback_field: å½“æŒ‡å®šå­—æ®µä¸å­˜åœ¨æ—¶çš„å›é€€å­—æ®µ
    """
    if text_fields is None:
        text_fields = ["input", "content", "reasoning_content", "text"]
    
    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line)
                
                # å°è¯•æŒ‰ä¼˜å…ˆçº§è·å–æ–‡æœ¬å­—æ®µ
                text_parts = []
                for field in text_fields:
                    if field in data:
                        content = data[field]
                        if isinstance(content, list):
                            text_parts.extend(content)
                        elif isinstance(content, dict):
                            text_parts.append(json.dumps(content))
                        else:
                            text_parts.append(str(content))
                
                # å›é€€æœºåˆ¶
                if not text_parts:
                    if fallback_field in data:
                        text_parts.append(str(data[fallback_field]))
                    else:
                        # å°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²å€¼
                        text_parts = [str(v) for v in data.values() if isinstance(v, str)]
                
                # åˆå¹¶æ–‡æœ¬
                text = "\n".join(text_parts)
                if not text.strip():
                    raise ValueError(f"Empty text in line {line_num}")
                
                yield text
                
            except json.JSONDecodeError:
                print(f"Error decoding JSON in line {line_num}")
                continue
            except Exception as e:
                print(f"Error in line {line_num}: {e}")
                continue

def create_tokenizer_config(save_dir: str, model_max_length: int = 8192) -> None:
    """
    åˆ›å»ºå®Œæ•´çš„tokenizeré…ç½®æ–‡ä»¶

    å‚æ•°:
        save_dir: ä¿å­˜ç›®å½•
        model_max_length: æœ€å¤§æ¨¡å‹é•¿åº¦
    """

    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)

    config = {
        # tokenizerç±»ï¼Œé¢„è®­ç»ƒfast tokenizer
        "tokenizer_class": "PreTrainedTokenizerFast",
        # æ˜¯å¦è‡ªåŠ¨åœ¨åºåˆ—å¼€å¤´æ·»åŠ bos_tokenï¼Œå»ºè®®å¼€å¯å¯¹è¯æ¨¡å‹æ—¶ä½¿ç”¨
        "add_bos_token": True,  
        # æ˜¯å¦è‡ªåŠ¨åœ¨åºåˆ—ç»“å°¾æ·»åŠ eos_tokenï¼Œå»ºè®®å¼€å¯å¯¹è¯æ¨¡å‹æ—¶ä½¿ç”¨
        "add_eos_token": True,
        # æ˜¯å¦åœ¨å•è¯å‰åŠ ç©ºæ ¼ï¼Œé€šå¸¸GPTç±»æ¨¡å‹éœ€è¦
        "add_prefix_space": True,
        # æ˜¯å¦æ¸…ç†tokenizationäº§ç”Ÿçš„å¤šä½™ç©ºæ ¼ï¼Œå»ºè®®å¼€å¯
        "clean_up_tokenization_spaces": True,
        # æœ€å¤§è¾“å…¥é•¿åº¦
        "model_max_length": model_max_length,
        # paddingç­–ç•¥ï¼Œå¸¸ç”¨right paddingï¼Œä¹Ÿå¯è®¾ç½®ä¸º'left'
        "padding_side": "right",
        # æˆªæ–­ç­–ç•¥ï¼Œå¸¸ç”¨rightæˆªæ–­
        "truncation_side": "right",

        # é¢„å®šä¹‰çš„ç‰¹æ®Štoken
        "bos_token": "<|sbos|>",
        "eos_token": "<|seos|>",
        "pad_token": "<|spad|>",  # ä¸“ç”¨pad token
        "unk_token": "<unk>",
        "mask_token": "<mask>",  # Mask tokenï¼Œæ–¹ä¾¿maskå¡«ç©ºä»»åŠ¡

        # chatæ¨¡æ¿ï¼ŒåŸºäºJinja2æ¨¡æ¿è¯­æ³•ç”Ÿæˆå¯¹è¯è¾“å…¥
        "chat_template": (
            "{% for message in messages %}"
            "{% set content = message['content'] | trim %}"
            "{% if message['role'] == 'system' %}"
            "<|sbos|>system\n{{ content }}<|seos|>\n"
            "{% elif message['role'] == 'user' %}"
            "<|sbos|>user\n{{ content }}<|seos|>\n"
            "{% elif message['role'] == 'assistant' %}"
            "<|sbos|>assistant\n{{ content }}<|seos|>\n"
            "{% elif message['role'] == 'tool' %}"
            "<|sbos|>tool\n{{ content }}<|seos|>\n"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "{{ '<|sbos|>assistant\n' }}"
            "{% endif %}"
        )
    }

    # ä¿å­˜ä¸»é…ç½®æ–‡ä»¶
    with open(os.path.join(save_dir, "tokenizer_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=4)

    # special_tokens_mapé…ç½®ï¼Œæ˜ å°„ç‰¹æ®Štoken
    special_tokens_map = {
        "bos_token": "<|sbos|>",
        "eos_token": "<|seos|>",
        "unk_token": "<unk>",
        "pad_token": "<|spad|>",
        "mask_token": "<mask>",
        # é¢å¤–ç‰¹æ®Štokenï¼Œç”¨äºæ ‡è®°å¯¹è¯è§’è‰²æˆ–ç‰¹æ®Šåˆ†éš”ç¬¦ï¼Œå»ºè®®ä¸æ¨¡æ¿ä¿æŒä¸€è‡´
        "additional_special_tokens": [
            "<s>",
            "</s>",
            "<|system|>",
            "<|user|>",
            "<|assistant|>",
            "<|tool|>"
        ]
    }

    # ä¿å­˜ç‰¹æ®Štokenæ˜ å°„
    with open(os.path.join(save_dir, "special_tokens_map.json"), "w", encoding="utf-8") as f:
        json.dump(special_tokens_map, f, ensure_ascii=False, indent=4)

def create_advanced_normalizer() -> normalizers.Sequence:
    """åˆ›å»ºæ–‡æœ¬è§„èŒƒåŒ–åºåˆ—"""
    return normalizers.Sequence([
        # Unicodeè§„èŒƒåŒ–
        NFD(),  # æ ‡å‡†åŒ–åˆ†è§£ï¼Œå…¨è§’åŠè§’ç»Ÿä¸€ï¼Œå…¼å®¹ä¸­è‹±æ–‡
        StripAccents(),  # å»é™¤é‡éŸ³ç¬¦å·
        
        # å¤„ç†ç‰¹æ®Šå­—ç¬¦
        Replace(Regex(r"\p{Cc}"), ""),  # ç§»é™¤æ§åˆ¶å­—ç¬¦
        Replace(Regex(r"\p{Cf}"), ""),  # ç§»é™¤æ ¼å¼å­—ç¬¦
        Replace(Regex(r"[ \t\n\r\f\v]+"), " "),  # åˆå¹¶ç©ºç™½å­—ç¬¦
        
        # è¡¨æƒ…ç¬¦å·å¤„ç†
        Replace(Regex(r"(\p{Emoji})"), r" \1 "),  # åˆ†éš”è¡¨æƒ…ç¬¦å·
        
        # URLå’Œç‰¹æ®Šæ¨¡å¼
        Replace(Regex(r"https?://\S+"), " [URL] "),      # URLå ä½
        Replace(Regex(r"\b\d[\d,.]*\b"), " [NUMBER] "),  # æ•°å­—å ä½
        
        # æœ€ç»ˆæ¸…ç†
        Replace(Regex(r"\s+"), " "),  # åˆå¹¶ç©ºæ ¼
        Replace(Regex(r"^ | $"), ""),  # å»é™¤é¦–å°¾ç©ºæ ¼
    ])

def create_advanced_pre_tokenizer() -> pre_tokenizers.Sequence:
    """åˆ›å»ºé«˜çº§é¢„åˆ†è¯å™¨åºåˆ—"""
    return pre_tokenizers.Sequence([
        # æŒ‰ç©ºç™½åˆ†å‰²ï¼ˆä¿ç•™ç©ºç™½ä¿¡æ¯ï¼‰
        Metaspace(replacement="â–", add_prefix_space=True),
        
        # å•ç‹¬éš”ç¦»ä»£ç å¸¸ç”¨ç¬¦å·
        Split(Regex(r'([\+\-\*/=%&|^~!@#])'), behavior="isolated"),
        Split(Regex(r"(['\"]|//|#|/\*|\*/)"), behavior="isolated"),

        # åˆ†éš”å­—æ¯ä¸å¤§å†™å­—æ¯ï¼ˆCamelCaseï¼‰
        Split(Regex(r'([a-z])([A-Z])'), behavior="contiguous"),
        Split(Regex(r"(\d+)([a-zA-Z])"), behavior="contiguous"),
        
        # æ•°å­—å¤„ç†
        Digits(individual_digits=True),
        
        # æ ‡ç‚¹ç¬¦å·å¤„ç†
        Punctuation(behavior="isolated"),
    ])

def train_tokenizer(
    data_paths: Union[str, List[str]],
    save_dir: str,
    text_fields: List[str] = None,
    vocab_size: int = 32768,
    min_frequency: int = 2,
    model_max_length: int = 8192,
    sampling_rate: float = 0.5,
    num_threads: int = 8
) -> None:
    """
    è®­ç»ƒå¹¶ä¿å­˜è‡ªå®šä¹‰tokenizer
    
    å‚æ•°:
        data_paths: å•ä¸ªè·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
        vocab_size: è¯æ±‡è¡¨å¤§å°
        min_frequency: æœ€å°è¯é¢‘
        model_max_length: æœ€å¤§æ¨¡å‹é•¿åº¦
        sampling_rate: æ•°æ®é‡‡æ ·ç‡
        num_threads: å¹¶è¡Œçº¿ç¨‹æ•°
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # åˆå§‹åŒ–tokenizer
    tokenizer = Tokenizer(models.BPE(
        unk_token="<unk>",
        fuse_unk=True,
        byte_fallback=True  # å¯ç”¨byte fallback
    ))
    
    # é«˜çº§æ–‡æœ¬è§„èŒƒåŒ–
    tokenizer.normalizer = create_advanced_normalizer()
    
    # é«˜çº§é¢„åˆ†è¯å™¨
    tokenizer.pre_tokenizer = create_advanced_pre_tokenizer()
    
    # è§£ç å™¨é…ç½®
    tokenizer.decoder = decoders.Metaspace(
        replacement="â–", 
        add_prefix_space=True
    )
    
    # åå¤„ç†å™¨é…ç½®
    tokenizer.post_processor = processors.TemplateProcessing(
        single="$A",
        pair="$A $B",
        special_tokens=[
            ("<s>", 1),
            ("</s>", 2),
            ("<|sbos|>", 3),
            ("<|seos|>", 4),
            ("<|spad|>", 5),
            ("<mask>", 6),
            ("<|system|>", 7),
            ("<|user|>", 8),
            ("<|assistant|>", 9),
            ("<|tool|>", 10),
        ]
    )

    # ç‰¹æ®Štoken
    special_tokens = [
        "<unk>", 
        "<s>", 
        "</s>", 
        "<|sbos|>", 
        "<|seos|>",
        "<|spad|>",  # ä¸“ç”¨pad token
        "<mask>",    # MLMä»»åŠ¡æ”¯æŒ
        "<|system|>",
        "<|user|>",
        "<|assistant|>",
        "<|tool|>"
    ]

    # é…ç½®è®­ç»ƒå™¨
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        min_frequency=min_frequency,
        show_progress=True,
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
        continuing_subword_prefix="##",  # å­è¯å‰ç¼€
        end_of_word_suffix="</w>",       # è¯å°¾æ ‡è®°
        limit_alphabet=1000,              # é™åˆ¶åŸºç¡€å­—ç¬¦é›†å¤§å°
    )

    # å¤„ç†å¤šæ–‡ä»¶è·¯å¾„
    if isinstance(data_paths, str):
        data_paths = [data_paths]
    
    # å¹¶è¡Œæ•°æ®è¯»å–
    def process_path(path):
        return list(read_data_from_jsonl(path, text_fields))
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        all_texts = []
        for texts in executor.map(process_path, data_paths):
            # åº”ç”¨é‡‡æ ·
            if sampling_rate < 1.0:
                sample_size = max(1, int(len(texts) * sampling_rate))
                texts = random.sample(texts, sample_size)
            all_texts.extend(texts)
    
    # è®­ç»ƒtokenizer
    print(f"Training tokenizer with {len(all_texts)} samples from {len(data_paths)} files")
    tokenizer.train_from_iterator(
        all_texts, 
        trainer=trainer, 
        length=len(all_texts)
    )

    # éªŒè¯ç‰¹æ®Štokenæ˜ å°„
    special_token_map = {
        "<unk>": 0,
        "<s>": 1,
        "</s>": 2,
        "<|sbos|>": 3,
        "<|seos|>": 4,
        "<|spad|>": 5,
        "<mask>": 6,
        "<|system|>": 7,
        "<|user|>": 8,
        "<|assistant|>": 9,
        "<|tool|>": 10
    }
    
    for token, expected_id in special_token_map.items():
        actual_id = tokenizer.token_to_id(token)
        if actual_id != expected_id:
            print(f"Warning: Special token '{token}' has ID {actual_id} (expected {expected_id})")

    # ä¿å­˜tokenizeræ–‡ä»¶
    tokenizer.save(os.path.join(save_dir, "tokenizer.json"))
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_tokenizer_config(save_dir, model_max_length)
    print(f"Tokenizer saved to {save_dir}")
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    vocab = tokenizer.get_vocab()
    token_lengths = [len(token) for token in vocab.keys() if isinstance(token, str)]
    
    print("\n=== Tokenizer Statistics ===")
    print(f"Vocab size: {len(vocab)}")
    print(f"Average token length: {sum(token_lengths)/len(token_lengths):.2f} chars")
    print(f"Longest token: {max(token_lengths)} chars")
    print(f"Special tokens: {special_tokens}")

def eval_tokenizer(tokenizer_path: str) -> None:
    """è¯„ä¼°tokenizeråŠŸèƒ½"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            use_fast=True,
            spadding_side="right",
            truncation_side="right"
        )
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return

    # æµ‹è¯•åŸºæœ¬å±æ€§
    print("\n=== TokenizeråŸºæœ¬ä¿¡æ¯ ===")
    print(f"Vocab size: {len(tokenizer)}")
    print(f"Special tokens: {tokenizer.all_special_tokens}")
    print(f"Special token IDs: {tokenizer.all_special_ids}")
    print(f"Model max length: {tokenizer.model_max_length}")

    # æµ‹è¯•èŠå¤©æ¨¡æ¿
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½å—?"},
        {"role": "assistant", "content": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼ä½ å‘¢?"},
        {"role": "user", "content": "æˆ‘ä¹Ÿå¾ˆå¥½!"},
        {"role": "tool", "content": "æ‰§è¡Œç»“æœ: 42"},
    ]
    
    print("\n=== èŠå¤©æ¨¡æ¿æµ‹è¯• ===")
    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    print("Generated prompt:\n", prompt, sep="")

    # æµ‹è¯•ç¼–ç è§£ç 
    print("\n=== ç¼–ç è§£ç æµ‹è¯• ===")
    encoded = tokenizer(
        prompt, 
        truncation=True, 
        max_length=256,
        return_offsets_mapping=True
    )
    decoded = tokenizer.decode(encoded["input_ids"], skip_special_tokens=False)
    print("Decoded text matches original:", decoded == prompt)
    
    # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
    print("\n=== è¾¹ç•Œæƒ…å†µæµ‹è¯• ===")
    test_cases = [
        "",  # ç©ºå­—ç¬¦ä¸²
        " ",  # ç©ºæ ¼
        "   ",  # å¤šä¸ªç©ºæ ¼
        "\n\n",  # æ¢è¡Œç¬¦
        "Hello\tworld",  # åˆ¶è¡¨ç¬¦
        "1234567890",  # æ•°å­—
        "https://example.com/path?query=param",  # URL
        "emoji: ğŸ˜ŠğŸ‘ğŸ”¥",  # è¡¨æƒ…ç¬¦å·
        "CamelCase and snake_case",  # å‘½åè§„èŒƒ
        "æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆ",  # æ—¥è¯­
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù†Øµ",  # é˜¿æ‹‰ä¼¯è¯­
        "Mixing è¯­è¨€ and languages",  # æ··åˆè¯­è¨€
        "<|sbos|>special<|seos|>",  # ç‰¹æ®Štoken
        "Newline\ntest",  # æ¢è¡Œ
        "Tab\ttest",  # åˆ¶è¡¨ç¬¦
        "   Leading and trailing spaces   ",  # é¦–å°¾ç©ºæ ¼
    ]
    
    for text in test_cases:
        encoded = tokenizer(text)
        decoded = tokenizer.decode(encoded["input_ids"])
        print(f"Original: {repr(text)}")
        print(f"Decoded:  {repr(decoded)}")
        print(f"Match:    {text == decoded}\n")

    # æµ‹è¯•å‹ç¼©ç‡
    print("\n=== å‹ç¼©ç‡æµ‹è¯• ===")
    sample_text = """
    Transformeræ¨¡å‹æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç”±Vaswaniç­‰äººåœ¨2017å¹´æå‡ºã€‚
    å®ƒå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæˆä¸ºBERTã€GPTç­‰ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚
    """
    char_count = len(sample_text)
    encoded = tokenizer(sample_text)
    token_count = len(encoded["input_ids"])
    compression_ratio = char_count / token_count
    print(f"Characters: {char_count}, Tokens: {token_count}, Ratio: {compression_ratio:.2f}")

    # æµ‹è¯•å¤šè¯­è¨€æ”¯æŒ
    print("\n=== å¤šè¯­è¨€æ”¯æŒæµ‹è¯• ===")
    languages = [
        ("English", "The quick brown fox jumps over the lazy dog"),
        ("Chinese", "æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’æƒ°çš„ç‹—"),
        ("Japanese", "é€Ÿã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒã®ã‚ã¾ãªçŠ¬ã‚’é£›ã³è¶Šãˆã¾ã™"),
        ("Korean", "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤"),
        ("Arabic", "Ø§Ù„Ø«Ø¹Ù„Ø¨ Ø§Ù„Ø¨Ù†ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ ÙŠÙ‚ÙØ² ÙÙˆÙ‚ Ø§Ù„ÙƒÙ„Ø¨ Ø§Ù„ÙƒØ³ÙˆÙ„"),
        ("Russian", "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ ĞºĞ¾Ñ€Ğ¸Ñ‡Ğ½ĞµĞ²Ğ°Ñ Ğ»Ğ¸ÑĞ° Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ñ‹Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ½Ğ¸Ğ²ÑƒÑ ÑĞ¾Ğ±Ğ°ĞºÑƒ")
    ]
    
    for lang, text in languages:
        tokens = tokenizer.tokenize(text)
        print(f"{lang}: {tokens}")

    # æµ‹è¯•ç‰¹æ®Štokenå¤„ç†
    print("\n=== ç‰¹æ®Štokenå¤„ç† ===")
    test_text = "<|sbos|>user\nHello<|seos|>"
    encoded = tokenizer(test_text).input_ids
    decoded = tokenizer.decode(encoded)
    print(f"Original: {test_text}")
    print(f"Decoded:  {decoded}")
    print(f"Special tokens preserved: {decoded == test_text}")
    
    # æµ‹è¯•æœªçŸ¥å­—ç¬¦å¤„ç†
    print("\n=== æœªçŸ¥å­—ç¬¦å¤„ç† ===")
    test_text = "Character: â¨€â¨â¨‚ (math symbols)"
    tokens = tokenizer.tokenize(test_text)
    print(f"Tokens: {tokens}")

def main():
    # é…ç½®è·¯å¾„ - æ”¯æŒå¤šæ–‡ä»¶è·¯å¾„
    data_paths = [
        "path/to/dataset1.jsonl",
        "path/to/dataset2.jsonl",
        "path/to/plain_text.jsonl"
    ]
    save_dir = "stellarbytetokenizer"

    # è®­ç»ƒtokenizer
    train_tokenizer(
        data_paths=data_paths,
        save_dir=save_dir,
        text_fields=None,
        vocab_size=65536,        # è¯æ±‡è¡¨å¤§å°[2^n] [2^15ï¼ˆ32768ï¼‰æˆ– 2^16ï¼ˆ65536ï¼‰]
        min_frequency=2,         # æœ€ä½è¯é¢‘
        model_max_length=8192,   # æœ€å¤§æ¨¡å‹é•¿åº¦
        sampling_rate=1.0,       # æ•°æ®é‡‡æ ·å æ¯”
        num_threads=12           # å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°
    )

    # è¯„ä¼°tokenizer
    eval_tokenizer(save_dir)

if __name__ == '__main__':
    main()