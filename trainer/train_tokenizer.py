import time
import random
import gc
import psutil
import json
import os
import json
import re
import unicodedata
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from transformers import AutoTokenizer, PreTrainedTokenizerFast
from tokenizers import Regex as TokenizersRegex
from tokenizers import (
    decoders,
    models,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
    Regex,
    normalizers
)
from tokenizers.normalizers import (
    NFC,
    NFKC, 
    NFD, 
    StripAccents,
    Replace,
    Lowercase,
    Strip
)
from tokenizers.pre_tokenizers import (
    Sequence,
    Split,
    ByteLevel,
    Digits,
    Punctuation,
    UnicodeScripts
)
from typing import (
    Generator, 
    List, 
    Dict, 
    Union, 
    Optional, 
    Iterator,
    Any
)
from trainer.sqlmanager import SQLiteDatabaseManager

random.seed(42)

class MemoryAwareIterator:
    """å†…å­˜æ„ŸçŸ¥è¿­ä»£å™¨ï¼ŒåŠ¨æ€æ§åˆ¶æ•°æ®å¤„ç†é€Ÿåº¦"""
    def __init__(self, iterator, max_memory_usage=0.8, check_interval=1000):
        self.iterator = iterator
        self.max_memory_usage = max_memory_usage
        self.check_interval = check_interval
        self.count = 0
        
    def __iter__(self):
        return self
    
    def __next__(self):
        # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        if self.count % self.check_interval == 0:
            memory_percent = psutil.virtual_memory().percent / 100
            if memory_percent > self.max_memory_usage:
                # å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæš‚åœä¸€ä¸‹è®©GCå·¥ä½œ
                gc.collect()
                # åŠ¨æ€è°ƒæ•´å¤„ç†é€Ÿåº¦
                time.sleep(0.1 * (memory_percent - self.max_memory_usage) * 10)
        
        self.count += 1
        return next(self.iterator)

def read_data_from_json(
    file_path: str, 
    text_fields: Optional[List[str]] = None,
    sampling_rate: float = 1.0,
    max_samples: Optional[int] = None
) -> Generator[str, None, None]:
    """
    è¯»å–JSONLæ–‡ä»¶ï¼Œä»…æ”¯æŒå¤šå­—æ®µä¼˜å…ˆçº§æå–ï¼ˆæ— å›é€€æœºåˆ¶ï¼‰
    
    å‚æ•°:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
        text_fields: å°è¯•æå–çš„æ–‡æœ¬å­—æ®µä¼˜å…ˆçº§åˆ—è¡¨
        sampling_rate: é‡‡æ ·ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
    """
    if text_fields is None:
        text_fields = ["input", "content", "reasoning_content", "text"]
    
    sample_count = 0
    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
        for line_num, line in enumerate(f, 1):
            if max_samples and sample_count >= max_samples:
                break
                
            if random.random() > sampling_rate:
                continue
                
            try:
                data = json.loads(line)
                
                # ä»…æå–æŒ‡å®šå­—æ®µ
                text_parts = []
                for field in text_fields:
                    if field in data:
                        content = data[field]
                        if isinstance(content, list):
                            text_parts.extend(content)
                        elif isinstance(content, dict):
                            text_parts.append(json.dumps(content))
                        else:
                            text_parts.append(str(content))
                
                # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•æ–‡æœ¬ï¼Œç›´æ¥è·³è¿‡
                if not text_parts:
                    continue
                
                text = "\n".join(text_parts)
                if not text.strip():
                    continue
                
                sample_count += 1
                yield text
                
            except json.JSONDecodeError:
                print(f"Error decoding JSON in line {line_num}")
                continue
            except Exception as e:
                print(f"Error in line {line_num}: {e}")
                continue

def read_data_from_sqlite(
    db_manager: SQLiteDatabaseManager,
    table_name: str = "tokenizer_data",
    text_field: str = "text",
    batch_size: int = 1000,
    sampling_rate: float = 1.0,
    max_samples: Optional[int] = None,
) -> Generator[str, None, None]:
    """
    ä»SQLiteæ•°æ®åº“æµå¼è¯»å–æ•°æ®ï¼Œæ”¯æŒé‡‡æ ·å’Œé™åˆ¶
    âš ï¸ æ³¨æ„ï¼šå¿…é¡»ä¼ å…¥å·²è¿æ¥çš„ db_managerï¼Œé¿å…å¤šæ¬¡é‡å¤è¿æ¥

    å‚æ•°:
        db_manager: å·²è¿æ¥çš„æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
        table_name: è¡¨å
        text_field: æ–‡æœ¬å­—æ®µå
        batch_size: æ¯æ¬¡è¯»å–çš„æ‰¹é‡å¤§å°
        sampling_rate: é‡‡æ ·ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶

    è¿”å›:
        æ–‡æœ¬æ•°æ®ç”Ÿæˆå™¨
    """
    sample_count = 0
    for text in db_manager.stream_text_data(table_name, text_field, batch_size, show_progress=True):
        # æ£€æŸ¥æœ€å¤§æ ·æœ¬é™åˆ¶
        if max_samples and sample_count >= max_samples:
            break

        # åº”ç”¨é‡‡æ ·
        if random.random() > sampling_rate:
            continue

        yield text
        sample_count += 1

def batch_iterator_json(
    file_paths: List[str], 
    text_fields: Optional[List[str]], 
    batch_size: int = 1000,
    sampling_rate: float = 1.0,
    max_samples: Optional[int] = None
) -> Iterator[List[str]]:
    """
    æ‰¹é‡è¿­ä»£å™¨ï¼Œé€æ‰¹è¯»å–æ•°æ®ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    
    å‚æ•°:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        text_fields: æ–‡æœ¬å­—æ®µ
        batch_size: æ‰¹é‡å¤§å°
        sampling_rate: é‡‡æ ·ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
    """
    batch = []
    total_samples = 0
    
    for file_path in file_paths:
        for text in read_data_from_json(
            file_path, text_fields, 
            sampling_rate=sampling_rate, 
            max_samples=max_samples
        ):
            batch.append(text)
            total_samples += 1
            
            if len(batch) >= batch_size:
                yield batch
                batch = []
            
            # æ£€æŸ¥æœ€å¤§æ ·æœ¬é™åˆ¶
            if max_samples and total_samples >= max_samples:
                if batch:
                    yield batch
                return
    
    if batch:
        yield batch

def batch_iterator_sqlite(
    db_manager: SQLiteDatabaseManager,
    table_name: str = "tokenizer_data",
    text_field: str = "text",
    batch_size: int = 1000,
    sampling_rate: float = 1.0,
    max_samples: Optional[int] = None,
) -> Iterator[List[str]]:
    """
    æ‰¹é‡è¿­ä»£å™¨
    âš ï¸ æ³¨æ„ï¼šå¿…é¡»ä¼ å…¥å·²è¿æ¥çš„ db_manager

    å‚æ•°:
        db_manager: å·²è¿æ¥çš„æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
        table_name: è¡¨å
        text_field: æ–‡æœ¬å­—æ®µå
        batch_size: æ‰¹é‡å¤§å°
        sampling_rate: é‡‡æ ·ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶

    è¿”å›:
        æ‰¹é‡æ–‡æœ¬æ•°æ®è¿­ä»£å™¨
    """
    batch = []
    total_samples = 0

    for text in read_data_from_sqlite(
        db_manager,
        table_name,
        text_field,
        batch_size,
        sampling_rate,
        max_samples
    ):
        batch.append(text)
        total_samples += 1

        if len(batch) >= batch_size:
            yield batch
            batch = []

        # æ£€æŸ¥æœ€å¤§æ ·æœ¬é™åˆ¶
        if max_samples and total_samples >= max_samples:
            if batch:
                yield batch
            return

    if batch:
        yield batch

def count_samples_json(
    file_paths: List[str], 
    text_fields: Optional[List[str]], 
    sampling_rate: float = 1.0,
    max_samples: Optional[int] = None
) -> int:
    """
    è®¡ç®—æ»¡è¶³æ¡ä»¶çš„æ ·æœ¬æ•°é‡
    
    å‚æ•°:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        text_fields: æ–‡æœ¬å­—æ®µ
        sampling_rate: é‡‡æ ·ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
    
    è¿”å›:
        æ ·æœ¬æ•°é‡
    """
    count = 0
    for file_path in file_paths:
        for text in read_data_from_json(
            file_path, text_fields, 
            sampling_rate=sampling_rate, 
            max_samples=max_samples
        ):
            count += 1
            if max_samples and count >= max_samples:
                return count
    return count

def count_samples_sqlite(
    db_manager: SQLiteDatabaseManager,
    table_name: str = "tokenizer_data",
    text_field: str = "text",
    sampling_rate: float = 1.0,
    max_samples: Optional[int] = None,
) -> int:
    """
    è®¡ç®—æ»¡è¶³æ¡ä»¶çš„æ ·æœ¬æ•°é‡
    âš ï¸ æ³¨æ„ï¼šå¿…é¡»ä¼ å…¥å·²è¿æ¥çš„ db_manager

    å‚æ•°:
        db_manager: å·²è¿æ¥çš„æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
        table_name: è¡¨å
        text_field: æ–‡æœ¬å­—æ®µå
        sampling_rate: é‡‡æ ·ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°é™åˆ¶

    è¿”å›:
        æ ·æœ¬æ•°é‡
    """
    count = 0

    for text in read_data_from_sqlite(
        db_manager,
        table_name,
        text_field,
        1000,
        sampling_rate,
        max_samples
    ):
        count += 1
        if max_samples and count >= max_samples:
            return count

    return count

def create_tokenizer_config(save_dir: str, model_max_length: int = 8192) -> None:
    """
    åˆ›å»ºå®Œæ•´çš„tokenizeré…ç½®æ–‡ä»¶

    å‚æ•°:
        save_dir: ä¿å­˜ç›®å½•
        model_max_length: æœ€å¤§æ¨¡å‹é•¿åº¦
    """

    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)

    config = {
        # tokenizerç±»ï¼Œé¢„è®­ç»ƒfast tokenizer
        "tokenizer_class": "PreTrainedTokenizerFast",
        # æ˜¯å¦è‡ªåŠ¨åœ¨åºåˆ—å¼€å¤´æ·»åŠ bos_tokenï¼Œå»ºè®®å¼€å¯å¯¹è¯æ¨¡å‹æ—¶ä½¿ç”¨
        "add_bos_token": False,  
        # æ˜¯å¦è‡ªåŠ¨åœ¨åºåˆ—ç»“å°¾æ·»åŠ eos_tokenï¼Œå»ºè®®å¼€å¯å¯¹è¯æ¨¡å‹æ—¶ä½¿ç”¨
        "add_eos_token": False,
        # æ˜¯å¦åœ¨å•è¯å‰åŠ ç©ºæ ¼ï¼Œé€šå¸¸GPTç±»æ¨¡å‹éœ€è¦
        "add_prefix_space": True,
        # æ˜¯å¦æ¸…ç†tokenizationäº§ç”Ÿçš„å¤šä½™ç©ºæ ¼ï¼Œå»ºè®®å¼€å¯
        "clean_up_tokenization_spaces": True,
        # æœ€å¤§è¾“å…¥é•¿åº¦
        "model_max_length": model_max_length,
        # paddingç­–ç•¥ï¼Œå¸¸ç”¨right paddingï¼Œä¹Ÿå¯è®¾ç½®ä¸º'left'
        "padding_side": "right",
        # æˆªæ–­ç­–ç•¥ï¼Œå¸¸ç”¨rightæˆªæ–­
        "truncation_side": "right",

        # é¢„å®šä¹‰çš„ç‰¹æ®Štoken
        "bos_token": "<|im_start|>",
        "eos_token": "<|im_end|>",
        "pad_token": "<|pad|>",  # ä¸“ç”¨pad token
        "unk_token": "<unk>",
        "mask_token": "<mask>",  # Mask tokenï¼Œæ–¹ä¾¿maskå¡«ç©ºä»»åŠ¡

        # é€šç”¨chatæ¨¡æ¿ï¼ŒåŸºäºJinja2æ¨¡æ¿è¯­æ³•ç”Ÿæˆå¯¹è¯è¾“å…¥
        "chat_template": (
            "{% for message in messages %}"
            "{% set role = message['role'] %}"
            "{% set content = message['content'] | trim %}"
            "{% if role == 'system' %}"
            "<|im_start|>system\n{{ content }}<|im_end|>"
            "{% elif role == 'user' %}"
            "<|im_start|>user\n{{ content }}<|im_end|>"
            "{% elif role == 'assistant' %}"
            "<|im_start|>assistant\n{{ content }}<|im_end|>"
            "{% elif role == 'tool' %}"
            "<|im_start|>tool\n{{ content }}<|im_end|>"
            "{% elif role == 'function' %}"
            "<|im_start|>function\n{{ content }}<|im_end|>"
            "{% else %}"
            "<|im_start|>unknown\n{{ content }}<|im_end|>"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "<|im_start|>assistant\n"
            "{% endif %}"
        )
    }

    # ä¿å­˜ä¸»é…ç½®æ–‡ä»¶
    with open(os.path.join(save_dir, "tokenizer_config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=4)

    # special_tokens_mapé…ç½®ï¼Œæ˜ å°„ç‰¹æ®Štoken
    special_tokens_map = {
        "bos_token": "<|im_start|>",
        "eos_token": "<|im_end|>",
        "pad_token": "<|pad|>",
        "unk_token": "<unk>",
        "mask_token": "<mask>",
        # é¢å¤–ç‰¹æ®Štokenï¼Œç”¨äºæ ‡è®°å¯¹è¯è§’è‰²æˆ–ç‰¹æ®Šåˆ†éš”ç¬¦ï¼Œå»ºè®®ä¸æ¨¡æ¿ä¿æŒä¸€è‡´
        "additional_special_tokens": [
            "<s>",             # æ¶ˆæ¯å¼€å§‹
            "</s>",            # æ¶ˆæ¯ç»“æŸ
            "<|system|>",      # ç³»ç»Ÿæç¤º
            "<|user|>",        # ç”¨æˆ·
            "<|assistant|>",   # æ¨¡å‹
            "<|tool|>",        # å·¥å…·
            "<|function|>",    # å‡½æ•°è°ƒç”¨
            "<|observation|>", # å·¥å…·/å‡½æ•°è¾“å‡º
            "<|unknown|>",     # æœªçŸ¥token
            "<|safe|>",        # å®‰å…¨token
            "<|unsafe|>",      # éå®‰å…¨token
        ]
    }

    # ä¿å­˜ç‰¹æ®Štokenæ˜ å°„
    with open(os.path.join(save_dir, "special_tokens_map.json"), "w", encoding="utf-8") as f:
        json.dump(special_tokens_map, f, ensure_ascii=False, indent=4)

def create_tokenizer_normalizer() -> normalizers.Sequence:
    """åˆ›å»ºæ–‡æœ¬è§„èŒƒåŒ–åºåˆ—"""
    normalizer_list = []

    # 1. Unicodeæ ‡å‡†åŒ–ï¼šNFKC é€‚åˆè‡ªç„¶è¯­è¨€ï¼Œä½†ä¼šç ´åéƒ¨åˆ†æ•°å­¦/ä»£ç ç¬¦å·
    # ğŸ‘‰ æ¨èç­–ç•¥ï¼šä»£ç ä»»åŠ¡ç”¨ NFCï¼Œè‡ªç„¶è¯­è¨€ä»»åŠ¡ç”¨ NFKC
    #    å¯ä»¥åœ¨å¤šè¯­æ–™åœºæ™¯ç”¨ NFCï¼Œä¿æŒæ›´ä¿å®ˆ
    normalizer_list.append(NFC())

    # 2. è§„èŒƒæ¢è¡Œï¼šCRLF/CR -> LF
    normalizer_list.append(Replace(Regex(r"\r\n?"), "\n"))

    # 4. ä¿ç•™å…³é”® Cfï¼›æ¸…ç†å…¶ä½™â€œé«˜é£é™©â€æ§åˆ¶ç¬¦
    #   - ä¿ç•™: ZWJ (U+200D), ZWNJ (U+200C), VS-16/VS-15 (U+FE0F/U+FE0E)
    #   - ç§»é™¤: C0 æ§åˆ¶å­—ç¬¦(é™¤ \t \n), å¤§éƒ¨åˆ† Bidi/éš”ç¦»æ§åˆ¶ç­‰
    normalizer_list.append(Replace(Regex(r"[\u0000-\u0008\u000B\u000C\u000E-\u001F\u007F]"), ""))
    normalizer_list.append(Replace(Regex(r"[\u2066-\u2069]"), ""))     # LRI/RLI/FSI/PDIï¼šå»æ‰ä»¥å‡å°‘å™ªå£°
    # ä¸ç§»é™¤ \u200C \u200D \uFE0E \uFE0F

    # 5. ç©ºæ ¼æ ‡å‡†åŒ–ï¼šå…¨è§’ç©ºæ ¼ã€çª„ä¸æ¢è¡Œç©ºæ ¼ç­‰ -> æ™®é€šç©ºæ ¼
    normalizer_list.append(Replace(Regex(r"[\u3000\u00A0\u2007\u202F]"), " "))

    # 6. åˆå¹¶è¡Œå†…å¤šä½™ç©ºæ ¼ï¼ˆè‡ªç„¶è¯­è¨€ä¸“ç”¨ï¼‰
    #    ä½†ä¸è¦å½±å“ç¼©è¿›ï¼Œæ‰€ä»¥ä¸èƒ½æ›¿æ¢åˆ¶è¡¨ç¬¦
    normalizer_list.append(Replace(Regex(r"[ ]{2,}"), " "))

    # 7. è¿‡å¤šç©ºè¡ŒæŠ˜å ä¸ºæœ€å¤šä¸¤ä¸ªï¼ˆä¿ç•™æ®µè½è¯­ä¹‰ï¼‰
    normalizer_list.append(Replace(Regex(r"\n{3,}"), "\n\n"))

    # 8. å»é™¤é¦–å°¾ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦
    normalizer_list.append(Replace(Regex(r"^[ \t]+"), ""))  # å»é™¤å¼€å¤´ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦
    normalizer_list.append(Replace(Regex(r"[ \t]+$"), ""))  # å»é™¤ç»“å°¾ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦

    return normalizers.Sequence(normalizer_list)

def create_tokenizer_pre_tokenizer() -> pre_tokenizers.Sequence:
    """åˆ›å»ºé¢„åˆ†è¯å™¨åºåˆ—"""
    return Sequence([
        # 1. ä¿ç•™æ•´æ®µå­—ç¬¦ä¸²ï¼ˆæ”¯æŒå•åŒå¼•å·ã€ä¸‰å¼•å·ï¼‰
        Split(
            Regex(r"([rubf]*(['\"]{3}.*?['\"]{3}|['\"].*?['\"]))"),
            behavior="isolated"
        ),

        # 2. ä¿ç•™æ³¨é‡Šæ•´ä½“ï¼ˆ//, #, /* */ï¼‰
        Split(
            Regex(r"(#.*|//.*|/\*[\s\S]*?\*/)"),
            behavior="isolated"
        ),

        # 3. URL æ£€æµ‹ï¼šä¿è¯ URL æ•´ä½“ä¸è¢«æ‹†å¼€
        Split(
            Regex(r"https?://[^\s]+"),
            behavior="isolated"
        ),

        # 4. æ•°å­—ï¼šä¿è¯è¿ç»­æ•°å­—ï¼ˆå«å°æ•°ç‚¹ã€åƒåˆ†ä½ï¼‰ä¸è¢«æ‹†æ•£
        Split(
            Regex(r"\d+([._]\d+)*([eE][+-]?\d+)?"),
            behavior="isolated"
        ),

        # 5. å¤šå­—ç¬¦æ“ä½œç¬¦ä¼˜å…ˆåˆ‡åˆ†
        Split(
            Regex(r"(===|!==|==|!=|<=|>=|\+\+|--|&&|\|\||->|=>|::|\?\?|\.?)"),
            behavior="isolated"
        ),

        # 6. å•å­—ç¬¦ç¬¦å·/æ“ä½œç¬¦åˆ†å¼€
        Punctuation(behavior="isolated"),

        # 7. Emoji / Extended Pictographic ä¿æŒå®Œæ•´
        #   - Extended_Pictographic = å¤§å¤šæ•° emoji
        #   - å…è®¸å’Œ ZWJ (U+200D) / VS-16 (U+FE0F) è¿ç”¨å½¢æˆå¤åˆè¡¨æƒ…
        Split(
            Regex(r"(\p{Extended_Pictographic}(?:\u200D\p{Extended_Pictographic})*)"),
            behavior="isolated"
        ),

        # 8. æ ‡è¯†ç¬¦ï¼ˆå˜é‡åã€å‡½æ•°åï¼‰æ•´ä½“(ä½¿ç”¨Unicodeå±æ€§åŒ¹é…å­—æ¯å’Œæ•°å­—)
        Split(
            Regex(r"[\p{L}_][\p{L}\p{N}_]*"), 
            behavior="isolated"
        ),

        # 9. UnicodeScripts: æŒ‰è„šæœ¬åˆ‡åˆ†ï¼ˆä¸­/è‹±æ–‡/é˜¿æ‹‰ä¼¯æ–‡ç­‰ï¼‰
        #   - ä¸­æ–‡å’Œæ—¥æ–‡ä¸åŠ ç©ºæ ¼ä¹Ÿèƒ½å•ç‹¬åˆ‡å‡º
        UnicodeScripts(),

        # 10. ByteLevelï¼šå¯¹æ‰€æœ‰å‰©ä½™å­—ç¬¦åš fallback
        #   - ä¿è¯ tokenizer å¯¹ä»»ä½•è¾“å…¥éƒ½èƒ½ç¼–ç /è§£ç 
        ByteLevel(add_prefix_space=False),
    ])

def train_tokenizer(
    db_path: Union[str, List[str]],
    save_dir: str,
    table_name: str = "tokenizer_data",
    text_fields: str = 'text',
    vocab_size: int = 32768,
    min_frequency: int = 2,
    model_max_length: int = 8192,
    batch_size: int = 10000,
    phase_sampling_rates: List[float] = [0.3, 0.7, 1.0]
) -> None:
    """
    è®­ç»ƒå¹¶ä¿å­˜è‡ªå®šä¹‰tokenizer
    
    å‚æ•°:
        db_path: è¯­æ–™æ•°æ®åº“è·¯å¾„
        save_dir: ä¿å­˜ç›®å½•
        db_path: æ•°æ®åº“è·¯å¾„
        vocab_size: è¯æ±‡è¡¨å¤§å°
        min_frequency: æœ€å°è¯é¢‘
        model_max_length: æœ€å¤§æ¨¡å‹é•¿åº¦
        sampling_rate: æ•°æ®é‡‡æ ·ç‡
        batch_size: æ‰¹é‡å¤„ç†å¤§å°
        phase_sampling_rates: ä¸‰è®­ç»ƒé˜¶æ®µçš„é‡‡æ ·ç‡
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # åˆå§‹åŒ–tokenizer
    tokenizer = Tokenizer(models.BPE(
        unk_token="<unk>",
        fuse_unk=True,
        byte_fallback=True  # å¯ç”¨byte fallback
    ))
    
    # é«˜çº§æ–‡æœ¬è§„èŒƒåŒ–
    tokenizer.normalizer = create_tokenizer_normalizer()
    
    # é«˜çº§é¢„åˆ†è¯å™¨
    tokenizer.pre_tokenizer = create_tokenizer_pre_tokenizer()
    
    # è§£ç å™¨é…ç½®
    tokenizer.decoder = decoders.ByteLevel()
    
    # åå¤„ç†å™¨é…ç½®
    tokenizer.post_processor = processors.TemplateProcessing(
         # å¦‚æœè®­ç»ƒçš„æ•°æ®å·²ç»æŒ‰ç…§éœ€è¦çš„æ ¼å¼è¿›è¡Œå¤„ç†ï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ  bos_token å’Œ eos_token, åä¹‹éœ€è¦
        single="$A",
        pair="$A $B",
        special_tokens=[
            ("<unk>", 0),
            ("<s>", 1),
            ("</s>", 2),
            ("<|im_start|>", 3),
            ("<|im_end|>", 4),
            ("<|pad|>", 5),
            ("<mask>", 6),
            ("<|system|>", 7),
            ("<|user|>", 8),
            ("<|assistant|>", 9),
            ("<|tool|>", 10),
            ("<|function|>", 11),
            ("<|observation|>", 12),
            ("<|unknown|>", 13),
            ("<|safe|>", 14),
            ("<|unsafe|>", 15)
        ]
    )

    # ç‰¹æ®Štoken
    special_tokens = [
        "<unk>", 
        "<s>", 
        "</s>", 
        "<|im_start|>", 
        "<|im_end|>",
        "<|pad|>",  # ä¸“ç”¨pad token
        "<mask>",    # MLMä»»åŠ¡æ”¯æŒ
        "<|system|>",  # ç³»ç»Ÿæç¤º
        "<|user|>",  # ç”¨æˆ·
        "<|assistant|>",  # æ¨¡å‹
        "<|tool|>",  # å·¥å…·
        "<|function|>",  # å‡½æ•°è°ƒç”¨
        "<|observation|>",  # å·¥å…·/å‡½æ•°è¾“å‡º
        "<|unknown|>",  # æœªçŸ¥token
        "<|safe|>",  # å®‰å…¨token
        "<|unsafe|>",  # éå®‰å…¨token
    ]
    
    # è®¡ç®—æ¯ä¸ªé˜¶æ®µçš„å‚æ•°ï¼ŒåŸºäºæœ€åä¸€é˜¶æ®µçš„å®Œå…¨æŠ½æ ·æ¨¡å¼
    final_phase_sampling = phase_sampling_rates[-1]  # æœ€åä¸€é˜¶æ®µçš„é‡‡æ ·ç‡

    # å®šä¹‰è®­ç»ƒé˜¶æ®µå‚æ•°
    training_phases = []
    for i, phase_sampling in enumerate(phase_sampling_rates):
        # è®¡ç®—å½“å‰é˜¶æ®µç›¸å¯¹äºæœ€åä¸€é˜¶æ®µçš„æ¯”ä¾‹
        sampling_ratio = phase_sampling / final_phase_sampling
        
        # æ ¹æ®æŠ½æ ·æ¯”ä¾‹è°ƒæ•´å‚æ•°
        phase_vocab_size = max(
            int(vocab_size * (0.2 + 0.8 * (i / (len(phase_sampling_rates) - 1)))),  # ä»20%åˆ°100%çº¿æ€§å¢é•¿
            min(8000, vocab_size) if i == 0 else min(20000, vocab_size) if i == 1 else vocab_size
        )
        
        phase_min_frequency = min_frequency * (len(phase_sampling_rates) - i)
        
        phase_limit_alphabet = min(
            1000,
            max(800, int(800 + 200 * (i / (len(phase_sampling_rates) - 1))))  # ä»800åˆ°1000çº¿æ€§å¢é•¿
        )
        
        # phase_batch_size = max(
        #     1000,
        #     min(batch_size, int(batch_size * (0.2 + 0.8 * (i / (len(phase_sampling_rates) - 1)))))  # ä»20%åˆ°100%çº¿æ€§å¢é•¿
        # )
        phase_batch_size = batch_size
        
        training_phases.append({
            "name": f"Phase {i+1}",
            "vocab_size": phase_vocab_size,
            "min_frequency": phase_min_frequency,
            "sampling_rate": phase_sampling,
            "batch_size": phase_batch_size,
            "limit_alphabet": phase_limit_alphabet,
            "sampling_ratio": sampling_ratio
        })
    
    # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨å®ä¾‹ï¼Œå°†åœ¨æ‰€æœ‰é˜¶æ®µä¸­é‡ç”¨
    db_manager = SQLiteDatabaseManager(db_path)
    db_manager.connect()

    # åˆ†é˜¶æ®µè®­ç»ƒ
    try:
        for i, phase in enumerate(training_phases):
            print(f"\n=== {phase['name']}/{len(training_phases)}: Sampling Rate {phase['sampling_rate']} ===")
            print(f"Vocab Size: {phase['vocab_size']}, Min Frequency: {phase['min_frequency']}")
            print(f"Batch Size: {phase['batch_size']}, Limit Alphabet: {phase['limit_alphabet']}")

            # é…ç½®å½“å‰é˜¶æ®µçš„è®­ç»ƒå™¨
            trainer = trainers.BpeTrainer(
                vocab_size=phase["vocab_size"],
                special_tokens=special_tokens,
                min_frequency=phase["min_frequency"],
                show_progress=True,
                initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
                continuing_subword_prefix="",
                end_of_word_suffix="",
                limit_alphabet=phase["limit_alphabet"]
            )

            # åˆ›å»ºå½“å‰é˜¶æ®µçš„æ•°æ®è¿­ä»£å™¨
            def phase_data_iterator():
                for batch in batch_iterator_sqlite(
                    db_manager,
                    table_name=table_name,
                    text_field=text_fields, 
                    batch_size=phase["batch_size"], 
                    sampling_rate=phase["sampling_rate"]
                ):
                    yield batch
                    gc.collect()

            # ä½¿ç”¨å†…å­˜æ„ŸçŸ¥è¿­ä»£å™¨åŒ…è£…
            memory_aware_iterator = MemoryAwareIterator(phase_data_iterator())

            # è‡ªåŠ¨è®¡ç®—è¿­ä»£å™¨é•¿åº¦
            length = count_samples_sqlite(
                db_manager,
                table_name=table_name, 
                text_field=text_fields, 
                sampling_rate=phase["sampling_rate"]
            )
            print(f"Found {length} samples for this phase")

            # è®­ç»ƒå½“å‰é˜¶æ®µ
            tokenizer.train_from_iterator(
                memory_aware_iterator, 
                trainer=trainer, 
                length=length
            )

            # ï¼ˆå¯é€‰ï¼‰åœ¨æ¯ä¸ªé˜¶æ®µåä¿å­˜ tokenizer çŠ¶æ€ä»¥å¤‡åç»­ä½¿ç”¨æˆ–æ£€æŸ¥
            tokenizer.save(f"./stellarbytetokenizer/phase_pretrain/tokenizer_phase_{i+1}.json")

            # æ‰“å°å½“å‰é˜¶æ®µçš„ç»Ÿè®¡ä¿¡æ¯
            current_vocab_size = len(tokenizer.get_vocab())
            print(f"{phase['name']} completed. Current vocab size: {current_vocab_size}")
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        db_manager.disconnect()

    # éªŒè¯ç‰¹æ®Štokenæ˜ å°„
    special_token_map = {
        "<unk>": 0,
        "<s>": 1,
        "</s>": 2,
        "<|im_start|>": 3,
        "<|im_end|>": 4,
        "<|pad|>": 5,
        "<mask>": 6,
    }
    
    for token, expected_id in special_token_map.items():
        actual_id = tokenizer.token_to_id(token)
        if actual_id != expected_id:
            print(f"Warning: Special token '{token}' has ID {actual_id} (expected {expected_id})")

    # ä¿å­˜tokenizeræ–‡ä»¶
    tokenizer.save(os.path.join(save_dir, "tokenizer.json"))
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_tokenizer_config(save_dir, model_max_length)
    print(f"Tokenizer saved to {save_dir}")
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    vocab = tokenizer.get_vocab()
    token_lengths = [len(token) for token in vocab.keys() if isinstance(token, str)]
    
    print("\n=== Tokenizer Statistics ===")
    print(f"Vocab size: {len(vocab)}")
    print(f"Average token length: {sum(token_lengths)/len(token_lengths):.2f} chars")
    print(f"Longest token: {max(token_lengths)} chars")
    print(f"Special tokens: {special_tokens}")

def eval_tokenizer(tokenizer_path: str, test_samples: int = 1000) -> Dict[str, Any]:
    """
    å…¨é¢è¯„ä¼°tokenizeråŠŸèƒ½ï¼ŒåŒ…æ‹¬æ€§èƒ½ã€è´¨é‡ã€å¤šè¯­è¨€æ”¯æŒå’Œç‰¹æ®Šåœºæ™¯å¤„ç†
    
    å‚æ•°:
        tokenizer_path: tokenizerä¿å­˜è·¯å¾„
        test_samples: ç”¨äºæ€§èƒ½æµ‹è¯•çš„æ ·æœ¬æ•°é‡
    
    è¿”å›:
        åŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    results = {
        "basic_info": {},
        "performance_metrics": {},
        "quality_metrics": {},
        "special_cases": {},
        "multilingual_support": {},
        "error_handling": {}
    }
    
    # åŠ è½½tokenizer
    try:
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            use_fast=True,
            padding_side="right",
            truncation_side="right"
        )
        load_time = time.time() - start_time
        results["basic_info"]["load_time_seconds"] = load_time
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return results

    # 1. åŸºæœ¬ä¿¡æ¯è¯„ä¼°
    print("\n=== TokenizeråŸºæœ¬ä¿¡æ¯ ===")
    vocab_size = len(tokenizer)
    results["basic_info"]["vocab_size"] = vocab_size
    results["basic_info"]["special_tokens"] = tokenizer.all_special_tokens
    results["basic_info"]["special_token_ids"] = tokenizer.all_special_ids
    results["basic_info"]["model_max_length"] = tokenizer.model_max_length
    
    print(f"Vocab size: {vocab_size}")
    print(f"Special tokens: {tokenizer.all_special_tokens}")
    print(f"Special token IDs: {tokenizer.all_special_ids}")
    print(f"Model max length: {tokenizer.model_max_length}")
    print(f"Load time: {load_time:.3f} seconds")

    # 2. æ€§èƒ½æŒ‡æ ‡è¯„ä¼°
    print("\n=== æ€§èƒ½æŒ‡æ ‡è¯„ä¼° ===")
    performance_metrics = evaluate_tokenizer_performance(tokenizer, test_samples)
    results["performance_metrics"] = performance_metrics
    
    for metric, value in performance_metrics.items():
        print(f"{metric}: {value}")

    # 3. è´¨é‡æŒ‡æ ‡è¯„ä¼°
    print("\n=== è´¨é‡æŒ‡æ ‡è¯„ä¼° ===")
    quality_metrics = evaluate_tokenizer_quality(tokenizer)
    results["quality_metrics"] = quality_metrics
    
    for metric, value in quality_metrics.items():
        if isinstance(value, float):
            print(f"{metric}: {value:.4f}")
        else:
            print(f"{metric}: {value}")

    # 4. ç‰¹æ®Šåœºæ™¯æµ‹è¯•
    print("\n=== ç‰¹æ®Šåœºæ™¯æµ‹è¯• ===")
    special_cases = evaluate_special_cases(tokenizer)
    results["special_cases"] = special_cases
    
    for case, result in special_cases.items():
        print(f"{case}: {result}")

    # 5. å¤šè¯­è¨€æ”¯æŒè¯„ä¼°
    print("\n=== å¤šè¯­è¨€æ”¯æŒè¯„ä¼° ===")
    multilingual_results = evaluate_multilingual_support(tokenizer)
    results["multilingual_support"] = multilingual_results
    
    for lang, metrics in multilingual_results.items():
        print(f"{lang}: {metrics}")

    # 6. é”™è¯¯å¤„ç†è¯„ä¼°
    print("\n=== é”™è¯¯å¤„ç†è¯„ä¼° ===")
    error_handling = evaluate_error_handling(tokenizer)
    results["error_handling"] = error_handling
    
    for test, result in error_handling.items():
        print(f"{test}: {result}")

    # 7. èŠå¤©æ¨¡æ¿æµ‹è¯•
    print("\n=== èŠå¤©æ¨¡æ¿æµ‹è¯• ===")
    chat_template_results = evaluate_chat_template(tokenizer)
    results["chat_template"] = chat_template_results
    
    for test, result in chat_template_results.items():
        if test == "generated_prompt":
            print(f"Generated prompt:\n{result}")
        else:
            print(f"{test}: {result}")

    # 8. è¯æ±‡è¡¨åˆ†æ
    print("\n=== è¯æ±‡è¡¨åˆ†æ ===")
    vocab_analysis = analyze_vocabulary(tokenizer)
    results["vocab_analysis"] = vocab_analysis
    
    for metric, value in vocab_analysis.items():
        if isinstance(value, float):
            print(f"{metric}: {value:.4f}")
        else:
            print(f"{metric}: {value}")

    # 9. ä¸å¸¸ç”¨tokenizerå¯¹æ¯”
    print("\n=== ä¸å¸¸ç”¨tokenizerå¯¹æ¯” ===")
    comparison_results = compare_with_standard_tokenizers(tokenizer)
    results["comparison"] = comparison_results
    
    for tokenizer_name, metrics in comparison_results.items():
        print(f"{tokenizer_name}: {metrics}")

    return results

def evaluate_tokenizer_performance(tokenizer, test_samples: int = 1000) -> Dict[str, Any]:
    """è¯„ä¼°tokenizeræ€§èƒ½æŒ‡æ ‡"""
    results = {}
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_texts = [
        "è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•tokenizerçš„æ€§èƒ½ã€‚",
        "This is an English text for testing tokenizer performance.",
        "1234567890 ç‰¹æ®Šå­—ç¬¦ !@#$%^&*()",
        "https://example.com/path?query=param&value=test",
        "Emojiæµ‹è¯•: ğŸ˜ŠğŸ‘ğŸ”¥ğŸ‰",
        "ä»£ç ç¤ºä¾‹: def hello_world(): print('Hello, World!')",
        "é•¿æ–‡æœ¬æµ‹è¯•: " + "è‡ªç„¶è¯­è¨€å¤„ç† " * 50
    ]
    
    # ç¼–ç é€Ÿåº¦æµ‹è¯•
    start_time = time.time()
    for i in range(test_samples):
        text = random.choice(test_texts)
        tokenizer(text, truncation=True, max_length=512)
    encode_time = time.time() - start_time
    results["encode_speed_tokens_per_second"] = test_samples * 100 / encode_time  # ä¼°ç®—
    
    # è§£ç é€Ÿåº¦æµ‹è¯•
    encoded_texts = [tokenizer(text, truncation=True, max_length=512) for text in test_texts]
    start_time = time.time()
    for i in range(test_samples):
        encoded = random.choice(encoded_texts)
        tokenizer.decode(encoded["input_ids"])
    decode_time = time.time() - start_time
    results["decode_speed_tokens_per_second"] = test_samples * 100 / decode_time  # ä¼°ç®—
    
    # å†…å­˜ä½¿ç”¨æµ‹è¯•
    process = psutil.Process()
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # æ‰§è¡Œä¸€äº›æ“ä½œåå†æ¬¡æµ‹é‡
    for i in range(100):
        text = random.choice(test_texts)
        tokenizer(text, truncation=True, max_length=512)
    
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    results["memory_usage_mb"] = memory_after - memory_before
    
    return results

def evaluate_tokenizer_quality(tokenizer) -> Dict[str, Any]:
    """è¯„ä¼°tokenizerè´¨é‡æŒ‡æ ‡"""
    results = {}
    
    # æµ‹è¯•æ–‡æœ¬
    test_cases = [
        "The quick brown fox jumps over the lazy dog.",
        "æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’æƒ°çš„ç‹—ã€‚",
        "1234567890",
        "Hello, world! ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "https://example.com/path?query=test",
        "Emoji: ğŸ˜ŠğŸ‘ğŸ”¥ğŸ‰",
        "Code: def function(): return True"
    ]
    
    # ç¼–ç -è§£ç ä¸€è‡´æ€§æµ‹è¯•
    perfect_matches = 0
    total_cases = len(test_cases)
    
    for text in test_cases:
        encoded = tokenizer(text)
        decoded = tokenizer.decode(encoded["input_ids"])
        if text == decoded:
            perfect_matches += 1
    
    results["perfect_decode_ratio"] = perfect_matches / total_cases
    
    # å‹ç¼©ç‡æµ‹è¯•
    total_chars = 0
    total_tokens = 0
    
    for text in test_cases:
        encoded = tokenizer(text)
        total_chars += len(text)
        total_tokens += len(encoded["input_ids"])
    
    results["compression_ratio"] = total_chars / total_tokens if total_tokens > 0 else 0
    results["average_tokens_per_sample"] = total_tokens / len(test_cases)
    
    # ç‰¹æ®Štokenä¿ç•™æµ‹è¯•
    special_text = "<|im_start|>user\nHello<|im_end|>"
    encoded = tokenizer(special_text)
    decoded = tokenizer.decode(encoded["input_ids"])
    results["special_tokens_preserved"] = special_text == decoded
    
    return results

def evaluate_special_cases(tokenizer) -> Dict[str, Any]:
    """è¯„ä¼°ç‰¹æ®Šåœºæ™¯å¤„ç†èƒ½åŠ›"""
    results = {}
    
    special_cases = [
        ("ç©ºå­—ç¬¦ä¸²", ""),
        ("ç©ºæ ¼", " "),
        ("å¤šä¸ªç©ºæ ¼", "   "),
        ("æ¢è¡Œç¬¦", "\n\n"),
        ("åˆ¶è¡¨ç¬¦", "Hello\tworld"),
        ("æ··åˆç©ºç™½ç¬¦", "  Hello  \t  World  \n"),
        ("ä»…ç‰¹æ®Šå­—ç¬¦", "!@#$%^&*()"),
        ("æ•°å­—", "1234567890"),
        ("é•¿æ•°å­—", "1234567890" * 10),
        ("URL", "https://example.com/path?query=param&value=test"),
        ("ç”µå­é‚®ä»¶", "user@example.com"),
        ("æ–‡ä»¶è·¯å¾„", "/home/user/document.txt"),
        ("JSON", '{"key": "value", "array": [1, 2, 3]}'),
        ("HTML", "<div class='container'><p>Hello</p></div>"),
        ("SQL", "SELECT * FROM users WHERE id = 1"),
        ("ä»£ç ", "def factorial(n): return 1 if n == 0 else n * factorial(n-1)"),
        ("è¡¨æƒ…ç¬¦å·", "ğŸ˜ŠğŸ‘ğŸ”¥ğŸ‰â¤ï¸ğŸ˜‚"),
        ("å¤åˆè¡¨æƒ…", "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦"),  # å®¶åº­è¡¨æƒ…
        ("é›¶å®½è¿æ¥ç¬¦", "caf\u00e9"),  # å¸¦é‡éŸ³ç¬¦å·
        ("å¼‚ä½“å­—é€‰æ‹©å™¨", "ï¸"),  # VS-15
        ("å¼‚ä½“å­—é€‰æ‹©å™¨", "ï¸"),  # VS-16
        ("ä»å·¦å‘å³æ ‡è®°", "\u200e"),  # LRM
        ("ä»å³å‘å·¦æ ‡è®°", "\u200f"),  # RLM
        ("ZWNJ", "à¤¨à¤¿à¤°à¥à¤­à¤°"),  # å°åœ°è¯­ä¸­çš„ZWNJ
        ("ZWJ", "ğŸ‘¨â€ğŸ’»"),  # ç”·äººå’ŒæŠ€æœ¯å‘˜
    ]
    
    for name, text in special_cases:
        try:
            encoded = tokenizer(text)
            decoded = tokenizer.decode(encoded["input_ids"])
            results[name] = text == decoded
        except Exception as e:
            results[name] = f"Error: {str(e)}"
    
    return results

def evaluate_multilingual_support(tokenizer) -> Dict[str, Any]:
    """è¯„ä¼°å¤šè¯­è¨€æ”¯æŒèƒ½åŠ›"""
    results = {}
    
    languages = [
        ("è‹±è¯­", "The quick brown fox jumps over the lazy dog."),
        ("ä¸­æ–‡", "æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’æƒ°çš„ç‹—ã€‚"),
        ("æ—¥è¯­", "é€Ÿã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒã®ã‚ã¾ãªçŠ¬ã‚’é£›ã³è¶Šãˆã¾ã™"),
        ("éŸ©è¯­", "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤"),
        ("é˜¿æ‹‰ä¼¯è¯­", "Ø§Ù„Ø«Ø¹Ù„Ø¨ Ø§Ù„Ø¨Ù†ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ ÙŠÙ‚ÙØ² ÙÙˆÙ‚ Ø§Ù„ÙƒÙ„Ø¨ Ø§Ù„ÙƒØ³ÙˆÙ„"),
        ("ä¿„è¯­", "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ ĞºĞ¾Ñ€Ğ¸Ñ‡Ğ½ĞµĞ²Ğ°Ñ Ğ»Ğ¸ÑĞ° Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ñ‹Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ½Ğ¸Ğ²ÑƒÑ ÑĞ¾Ğ±Ğ°ĞºÑƒ"),
        ("æ³•è¯­", "Le rapide renard brun saute par-dessus le chien paresseux"),
        ("å¾·è¯­", "Der schnelle braune Fuchs springt Ã¼ber den faulen Hund"),
        ("è¥¿ç­ç‰™è¯­", "El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso"),
        ("å°åœ°è¯­", "à¤¤à¥‡à¤œ à¤­à¥‚à¤°à¥€ à¤²à¥‹à¤®à¤¡à¤¼à¥€ à¤†à¤²à¤¸à¥€ à¤•à¥à¤¤à¥à¤¤à¥‡ à¤•à¥‡ à¤Šà¤ªà¤° à¤•à¥‚à¤¦à¤¤à¥€ à¤¹à¥ˆ"),
        ("è‘¡è„ç‰™è¯­", "A rÃ¡pida raposa marrom salta sobre o cÃ£o preguiÃ§oso"),
        ("å­ŸåŠ æ‹‰è¯­", "à¦¦à§à¦°à§à¦¤ à¦¬à¦¾à¦¦à¦¾à¦®à§€ à¦¶à¦¿à¦¯à¦¼à¦¾à¦² à¦…à¦²à¦¸ à¦•à§à¦•à§à¦° à¦‰à¦ªà¦° jumps"),
        ("æ„å¤§åˆ©è¯­", "La volpe marrone veloce salta sopra il cane pigro"),
        ("åœŸè€³å…¶è¯­", "HÄ±zlÄ± kahverengi tilki tembel kÃ¶peÄŸin Ã¼zerinden atlar"),
    ]
    
    for lang, text in languages:
        try:
            encoded = tokenizer(text)
            tokens = tokenizer.tokenize(text)
            decoded = tokenizer.decode(encoded["input_ids"])
            
            results[lang] = {
                "original_length": len(text),
                "token_count": len(encoded["input_ids"]),
                "compression_ratio": len(text) / len(encoded["input_ids"]) if len(encoded["input_ids"]) > 0 else 0,
                "perfect_decode": text == decoded,
                "unique_tokens": len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0  # ä»¤ç‰Œå¤šæ ·æ€§
            }
        except Exception as e:
            results[lang] = f"Error: {str(e)}"
    
    return results

def evaluate_error_handling(tokenizer) -> Dict[str, Any]:
    """è¯„ä¼°é”™è¯¯å¤„ç†èƒ½åŠ›"""
    results = {}
    
    # æµ‹è¯•å„ç§è¾¹ç•Œå’Œé”™è¯¯æƒ…å†µ
    error_cases = [
        ("ç©ºè¾“å…¥", ""),
        ("è¶…å¤§è¾“å…¥", "A" * 100000),  # è¶…é•¿æ–‡æœ¬
        ("Noneè¾“å…¥", None),
        ("æ•°å­—è¾“å…¥", 123),
        ("åˆ—è¡¨è¾“å…¥", ["text1", "text2"]),
        ("éæ³•Unicode", b'\xFF\xFE'.decode('utf-8', errors='replace')),  # éæ³•UTF-8
        ("æ§åˆ¶å­—ç¬¦", "Hello\u0000World"),  # ç©ºå­—ç¬¦
        ("æ— æ•ˆUTF-8åºåˆ—", b'\xff\xfe'.decode('utf-8', errors='ignore')),  # BOMæ ‡è®°
    ]
    
    for name, input_data in error_cases:
        try:
            if input_data is None:
                encoded = tokenizer.encode(input_data)
            else:
                encoded = tokenizer.encode(input_data)
            results[name] = "Success"
        except Exception as e:
            results[name] = f"Error: {type(e).__name__}"
    
    return results

def evaluate_chat_template(tokenizer) -> Dict[str, Any]:
    """è¯„ä¼°èŠå¤©æ¨¡æ¿åŠŸèƒ½"""
    results = {}
    
    # æµ‹è¯•æ¶ˆæ¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä½ è‡ªå·±ã€‚"},
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä¸“é—¨è®¾è®¡æ¥å›ç­”ä½ çš„é—®é¢˜å’Œæä¾›å¸®åŠ©ã€‚"},
        {"role": "user", "content": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ"}
    ]
    
    try:
        # ç”Ÿæˆæç¤º
        prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        results["generated_prompt"] = prompt
        
        # æµ‹è¯•ç¼–ç è§£ç 
        encoded = tokenizer(
            prompt, 
            truncation=True, 
            max_length=512,
            return_offsets_mapping=True
        )
        decoded = tokenizer.decode(encoded["input_ids"], skip_special_tokens=False)
        
        results["decode_matches_original"] = prompt == decoded
        results["special_tokens_detected"] = any(
            token_id in encoded["input_ids"] 
            for token_id in tokenizer.all_special_ids
        )
        
    except Exception as e:
        results["error"] = f"Chat template error: {str(e)}"
    
    return results

def analyze_vocabulary(tokenizer) -> Dict[str, Any]:
    """åˆ†æè¯æ±‡è¡¨ç‰¹å¾"""
    results = {}
    
    vocab = tokenizer.get_vocab()
    tokens = list(vocab.keys())
    
    # åŸºæœ¬ç»Ÿè®¡
    token_lengths = [len(token) for token in tokens if isinstance(token, str)]
    results["vocab_size"] = len(vocab)
    results["average_token_length"] = sum(token_lengths) / len(token_lengths) if token_lengths else 0
    results["max_token_length"] = max(token_lengths) if token_lengths else 0
    results["min_token_length"] = min(token_lengths) if token_lengths else 0
    
    # ç‰¹æ®Štokenæ¯”ä¾‹
    special_tokens = [token for token in tokens if token in tokenizer.all_special_tokens]
    results["special_token_ratio"] = len(special_tokens) / len(tokens) if tokens else 0
    
    # å­—ç¬¦ç±»å‹åˆ†å¸ƒ
    char_categories = {}
    for token in tokens:
        for char in token:
            category = unicodedata.category(char)
            char_categories[category] = char_categories.get(category, 0) + 1
    
    # æ ‡å‡†åŒ–æ¯”ä¾‹
    total_chars = sum(char_categories.values())
    for category in char_categories:
        char_categories[category] = char_categories[category] / total_chars
    
    results["char_category_distribution"] = char_categories
    
    # å¸¸è§å‰ç¼€å’Œåç¼€
    prefixes = Counter([token[:3] for token in tokens if len(token) >= 3])
    suffixes = Counter([token[-3:] for token in tokens if len(token) >= 3])
    
    results["common_prefixes"] = dict(prefixes.most_common(10))
    results["common_suffixes"] = dict(suffixes.most_common(10))
    
    return results

def compare_with_standard_tokenizers(tokenizer) -> Dict[str, Any]:
    """ä¸æ ‡å‡†tokenizerè¿›è¡Œå¯¹æ¯”"""
    results = {}
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = "The quick brown fox jumps over the lazy dog. æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’æƒ°çš„ç‹—ã€‚"
    
    # å¯¹æ¯”çš„tokenizeråˆ—è¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    standard_tokenizers = {}
    
    try:
        from transformers import GPT2Tokenizer
        standard_tokenizers["gpt2"] = GPT2Tokenizer.from_pretrained("gpt2")
    except:
        pass
        
    try:
        from transformers import BertTokenizer
        standard_tokenizers["bert"] = BertTokenizer.from_pretrained("bert-base-uncased")
    except:
        pass
    
    # å¯¹æ¯”æŒ‡æ ‡
    for name, std_tokenizer in standard_tokenizers.items():
        # ç¼–ç æµ‹è¯•æ–‡æœ¬
        test_encoded = tokenizer(test_text)
        std_encoded = std_tokenizer(test_text)
        
        # è®¡ç®—å¯¹æ¯”æŒ‡æ ‡
        results[name] = {
            "test_token_count": len(test_encoded["input_ids"]),
            "std_token_count": len(std_encoded["input_ids"]),
            "ratio": len(test_encoded["input_ids"]) / len(std_encoded["input_ids"]) if len(std_encoded["input_ids"]) > 0 else 0,
            "common_tokens": len(set(test_encoded["input_ids"]) & set(std_encoded["input_ids"])) / len(set(test_encoded["input_ids"])) if len(set(test_encoded["input_ids"])) > 0 else 0
        }
    
    return results

def main():
    # é…ç½®è·¯å¾„ - æ”¯æŒå¤šæ–‡ä»¶è·¯å¾„
    db_path = "./datasets/tokenizers/pretrain_tokeniser.db"
    data_path = "./datasets/train/model_pretrain.jsonl"

    save_dir = "./stellarbytetokenizer"
    eval_tokenizer_info_dir = './model_info'

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å¯¼å…¥æ•°æ®
    if not os.path.exists(db_path):
        db_manager = SQLiteDatabaseManager(db_path)
        db_manager.connect()
        db_manager.create_table('tokenizer_pretrain_data', 'text')
        db_manager.insert_data(
            data_file_path=data_path,
            table_name='tokenizer_pretrain_data',
            columns='text'
        )
        db_manager.optimize_database()
        db_manager.disconnect()

    # è®­ç»ƒtokenizer
    train_tokenizer(
        db_path=db_path,
        save_dir=save_dir,
        table_name='tokenizer_pretrain_data',
        text_fields='text',
        vocab_size=52000,        # è¯æ±‡è¡¨å¤§å°[2^n] [2^15ï¼ˆ32768ï¼‰æˆ– 2^16ï¼ˆ65536ï¼‰]
        min_frequency=2,         # æœ€ä½è¯é¢‘
        model_max_length=1000000000000000019884624838656,   # æœ€å¤§æ¨¡å‹é•¿åº¦
        batch_size=500,          # æ‰¹å¤„ç†å¤§å°
        phase_sampling_rates=[0.3, 0.7, 1.0],    # ä¸åŒé˜¶æ®µæ•°æ®é‡‡æ ·å æ¯”
    )

    # è¯„ä¼°tokenizer
    eval_results = eval_tokenizer(save_dir, test_samples=1000)
    
    # ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
    with open(os.path.join(eval_tokenizer_info_dir, "tokenizer_evaluation_results.json"), "w", encoding="utf-8") as f:
        json.dump(eval_results, f, ensure_ascii=False, indent=4)
    
    print(f"\nè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {os.path.join(eval_tokenizer_info_dir, 'evaluation_results.json')}")

if __name__ == '__main__':
    main()