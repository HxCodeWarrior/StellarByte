<div align="center">

# âœ¨ StellarByte âœ¨

<p>æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯ï¼Œç…§è§å¤ä»ŠåŒæœ›çš„å¤œç©ºã€‚</p>

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange?style=flat-square&logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow?style=flat-square)](https://huggingface.co/)
[![Blog](https://img.shields.io/badge/Blog-ByteWyrm=flat-square)](https://blog.devnest.top/)

</div>

## ğŸ“š ç®€ä»‹

StellarByte æ˜¯ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹å®ç°ï¼Œä¸ HuggingFace ç”Ÿæ€å®Œå…¨å…¼å®¹ã€‚è¯¥é¡¹ç›®èåˆäº†å¤šç§ç°ä»£ Transformer ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æä¾›é«˜æ•ˆã€çµæ´»ä¸”æ˜“äºä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆå¼ AI ä»»åŠ¡ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å®ç°**ï¼šé›†æˆ FlashAttentionã€KV ç¼“å­˜ç­‰ä¼˜åŒ–æŠ€æœ¯
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶å¯ç‹¬ç«‹ä½¿ç”¨æˆ–ç»„åˆ
- ğŸ”„ **XPos æ—‹è½¬ä½ç½®ç¼–ç **ï¼šæ”¹è¿›çš„ RoPE ä½ç½®ç¼–ç ï¼Œæé«˜é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›
- ğŸ› ï¸ **ä¸°å¯Œçš„ä¼˜åŒ–æŠ€æœ¯**ï¼š
  - âš™ï¸ DeepNorm å½’ä¸€åŒ–ç­–ç•¥
  - ğŸ” LayerScale åˆå§‹åŒ–æŠ€æœ¯
  - ğŸ”€ DropPath æ­£åˆ™åŒ–
  - âš¡ å¹¶è¡Œæ®‹å·®è¿æ¥
- ğŸ“Š **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šå†…ç½® LoRA ä½ç§©é€‚åº”å®ç°
- ğŸ¤— **HuggingFace å…¼å®¹**ï¼šæ— ç¼é›†æˆ Transformers ç”Ÿæ€ç³»ç»Ÿ
- ğŸ“ **æ¸…æ™°çš„ä»£ç ç»“æ„**ï¼šæ˜“äºç†è§£å’Œæ‰©å±•

## ğŸ”§ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/StellarByte.git
cd StellarByte

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼ˆæš‚æœªå®ç°ï¼‰
pip install -e .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
import torch
from stellarbyte import ByteModel, ByteConfig

# åˆ›å»ºé…ç½®
config = ByteConfig(
    vocab_size=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072
)

# åˆå§‹åŒ–æ¨¡å‹
model = ByteModel(config)

# å‡†å¤‡è¾“å…¥
inputs = torch.randint(0, 32000, (1, 512))

# å‰å‘ä¼ æ’­
outputs = model(inputs)
```

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### ä» HuggingFace åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from stellarbyte import ByteModel
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = ByteModel.from_pretrained("path/to/model")
tokenizer = AutoTokenizer.from_pretrained("path/to/tokenizer")

# ç¼–ç æ–‡æœ¬
inputs = tokenizer("æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯", return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(inputs.input_ids, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### ä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

```python
from stellarbyte import ByteModel, LoRAConfig
from stellarbyte.lora import apply_lora_to_model

# åŠ è½½åŸºç¡€æ¨¡å‹
model = ByteModel.from_pretrained("path/to/model")

# é…ç½® LoRA
lora_config = LoRAConfig(
    r=8,
    target_modules=["q_proj", "v_proj"],
    lora_alpha=16,
    lora_dropout=0.05
)

# åº”ç”¨ LoRA åˆ°æ¨¡å‹
model = apply_lora_to_model(model, lora_config)

# ç°åœ¨åªæœ‰ LoRA å‚æ•°ä¼šè¢«æ›´æ–°
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
StellarByte/
â”œâ”€â”€ config/             # é…ç½®ç±»
â”œâ”€â”€ datasets/           # æ•°æ®é›†å¤„ç†
â”œâ”€â”€ model/              # æ¨¡å‹ç»„ä»¶
â”‚   â”œâ”€â”€ Attention.py    # å¤šå¤´è‡ªæ³¨æ„åŠ›å®ç°
â”‚   â”œâ”€â”€ DecoderLayer.py # Transformer è§£ç å™¨å±‚
â”‚   â”œâ”€â”€ LoRA.py         # ä½ç§©é€‚åº”å®ç°
â”‚   â”œâ”€â”€ MLP.py          # å¤šå±‚æ„ŸçŸ¥æœºå®ç°
â”‚   â”œâ”€â”€ MoE.py          # ä¸“å®¶æ··åˆå®ç°ï¼ˆè®¡åˆ’ä¸­ï¼‰
â”‚   â”œâ”€â”€ Position_Embedding.py # ä½ç½®ç¼–ç å®ç°
â”‚   â””â”€â”€ RMSNorm.py      # RMS å½’ä¸€åŒ–å®ç°
â”œâ”€â”€ tokenizer/          # åˆ†è¯å™¨
â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â””â”€â”€ test/               # æµ‹è¯•ä»£ç 
```

## ğŸ”œ å¼€å‘è®¡åˆ’

### 2025.7.13
#### Done:
1. å®ç°BaseModelConfigç±»ï¼Œåç»­çš„è¶…å‚æ•°å°†é€æ¸è¿­ä»£
2. å®ç°RMSNormå±‚å½’ä¸€åŒ–ç±»
3. Transformerç»å…¸çš„MultiHeadAttentionç±»

#### TODOï¼š
1. Attentionåº”ç”¨KVç¼“å­˜ï¼Œæ·»åŠ é‡åŒ–æœºåˆ¶
2. æ„å»ºåŸºç¡€MLPå±‚
3. æ„å»ºåŸºç¡€DecoderLayerå±‚

### 2025.7.14
#### Done:
1. å®ç°Attentionåº”ç”¨ç¼“å­˜æœºåˆ¶
2. å®ç°Attentioné‡åŒ–æœºåˆ¶
3. å®ç°åŸºç¡€MLPå±‚
4. å®ç°åŸºç¡€ByteDecoderLayerå±‚
5. å®ç°LoRA
6. å®ç°DropPath
7. å®ç°KVCacheæœºåˆ¶
8. é‡å†™äº†Attentionä¸­ä¸KVCacheç›¸å…³çš„éƒ¨åˆ†
9. å®ç°æ¨¡å‹è®­ç»ƒä¸­çš„å·¥å…·ç»„ä»¶
- æ—¥å¿—è®°å½•ç»„ä»¶
- ç‚«é…·è¿›åº¦æ¡åŠ è½½ç»„ä»¶
- æ¨¡å‹æƒé‡ç®¡ç†ç»„ä»¶
- æ¨¡å‹ä¿¡æ¯åˆ†æç»„ä»¶
10. å®ç°æ¨¡å‹è®­ç»ƒæ•°æ®é›†åŠ è½½å™¨åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®é›†åŠ è½½å™¨å’ŒSTFè®­ç»ƒæ•°æ®é›†åŠ è½½å™¨
10. åŸºæœ¬æ„å»ºæ¨¡å‹é¢„è®­ç»ƒæµç¨‹

#### TODO:
1. æ„é€ Memoryç±»å¹¶è¿›è¡Œåº”ç”¨
2. åº”ç”¨LoRAç±»
3. æ„é€ å•æ­¥æ¨ç†æ¥å£ def forward_step(self, x_t, past_k, past_v) -> (out, new_k, new_v)
4. Xposä½ç½®ç¼–ç ä¼˜åŒ–
- ç»“æ„æ€§æ­£åˆ™
- åŠ¨æ€thetaè°ƒæ•´
5. Attention
- num_rep æœªè¢«ä½¿ç”¨ 
- å®ç°â€¯çº¿ç¨‹å¹¶è¡Œ/Allâ€‘Reduce
- è¿›ä¸€æ­¥èåˆFlashAttention-2
- æ¥å…¥RetNet
- çº¿æ€§å±‚é‡åŒ–quantize() ä½¿ç”¨äº† torch.quantization.quantize_dynamic()ï¼Œä½†è¿™ä»…é™äºçº¿æ€§å±‚ + æ¨ç†ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥æ”¯æŒGPTQ/AWQ/SmoothQuant
6. KVCache
å°† KVCache.append() æ”¹ä¸ºæ”¯æŒï¼š
- æ»‘çª—ï¼ˆsliding windowï¼‰æˆªæ–­
- å†™å…¥ä½ç½®å¹¶å‘é”å®šï¼ˆif multi-threadï¼‰
- Layer-wise tokenä½ç½®è‡ªåŠ¨åç§»è®¡ç®—

### 2025.7.15
#### Done:
1. æ„å»ºå¹¶ä¼˜åŒ–æ¨¡å‹è®­ç»ƒç»„ä»¶ï¼š
- æ£€æŸ¥ç‚¹ç®¡ç†ç»„ä»¶
- æ„å¤–ä¸­æ–­ä¿æŠ¤ç»„ä»¶
2. åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­åº”ç”¨æ£€æŸ¥ç‚¹ç®¡ç†ä»¥åŠæ„å¤–ä¸­æ–­ä¿æŠ¤ç»„ä»¶

#### TODOï¼š
1. 


## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ–°åŠŸèƒ½å»ºè®®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºæ‚¨çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. æ‰“å¼€ä¸€ä¸ª Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸŒŸ è‡´è°¢

- æ„Ÿè°¢æ‰€æœ‰ä¸º Transformer æ¶æ„å‘å±•åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…
- æ„Ÿè°¢ HuggingFace å›¢é˜Ÿæä¾›çš„å‡ºè‰²å·¥å…·å’Œç”Ÿæ€ç³»ç»Ÿ
- æ„Ÿè°¢æ‰€æœ‰é¡¹ç›®è´¡çŒ®è€…

---

<div align="center">
  <sub>æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯ï¼Œç…§è§å¤ä»ŠåŒæœ›çš„å¤œç©ºã€‚</sub>
</div>