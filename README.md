<div align="center">

# âœ¨ StellarByte âœ¨

<p>æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯ï¼Œç…§è§å¤ä»ŠåŒæœ›çš„å¤œç©ºã€‚</p>

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange?style=flat-square&logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow?style=flat-square)](https://huggingface.co/)
[![Blog](https://img.shields.io/badge/Blog-ByteWyrm-pink?style=flat-square)](https://blog.devnest.top/)

</div>

## ğŸ“š ç®€ä»‹

StellarByte æ˜¯ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹å®ç°ï¼Œä¸ HuggingFace ç”Ÿæ€å®Œå…¨å…¼å®¹ã€‚è¯¥é¡¹ç›®èåˆäº†å¤šç§ç°ä»£ Transformer ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æä¾›é«˜æ•ˆã€çµæ´»ä¸”æ˜“äºä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆå¼ AI ä»»åŠ¡ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å®ç°**ï¼šé›†æˆ FlashAttentionã€KV ç¼“å­˜ç­‰ä¼˜åŒ–æŠ€æœ¯
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶å¯ç‹¬ç«‹ä½¿ç”¨æˆ–ç»„åˆ
- ğŸ”„ **XPos æ—‹è½¬ä½ç½®ç¼–ç **ï¼šæ”¹è¿›çš„ RoPE ä½ç½®ç¼–ç ï¼Œæé«˜é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›
- ğŸ› ï¸ **ä¸°å¯Œçš„ä¼˜åŒ–æŠ€æœ¯**ï¼š
  - âš™ï¸ DeepNorm å½’ä¸€åŒ–ç­–ç•¥
  - ğŸ” LayerScale åˆå§‹åŒ–æŠ€æœ¯
  - ğŸ”€ DropPath æ­£åˆ™åŒ–
  - âš¡ å¹¶è¡Œæ®‹å·®è¿æ¥
- ğŸ“Š **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šå†…ç½® LoRA ä½ç§©é€‚åº”å®ç°
- ğŸ¤— **HuggingFace å…¼å®¹**ï¼šæ— ç¼é›†æˆ Transformers ç”Ÿæ€ç³»ç»Ÿ
- ğŸ“ **æ¸…æ™°çš„ä»£ç ç»“æ„**ï¼šæ˜“äºç†è§£å’Œæ‰©å±•

## ğŸ”§ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/StellarByte.git
cd StellarByte

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼ˆæš‚æœªå®ç°ï¼‰
pip install -e .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
import torch
from stellarbyte import ByteModel, ByteConfig

# åˆ›å»ºé…ç½®
config = ByteConfig(
    vocab_size=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072
)

# åˆå§‹åŒ–æ¨¡å‹
model = ByteModel(config)

# å‡†å¤‡è¾“å…¥
inputs = torch.randint(0, 32000, (1, 512))

# å‰å‘ä¼ æ’­
outputs = model(inputs)
```

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### ä» HuggingFace åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from stellarbyte import ByteModel
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = ByteModel.from_pretrained("path/to/model")
tokenizer = AutoTokenizer.from_pretrained("path/to/tokenizer")

# ç¼–ç æ–‡æœ¬
inputs = tokenizer("æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯", return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(inputs.input_ids, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### ä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

```python
from stellarbyte import ByteModel, LoRAConfig
from stellarbyte.lora import apply_lora_to_model

# åŠ è½½åŸºç¡€æ¨¡å‹
model = ByteModel.from_pretrained("path/to/model")

# é…ç½® LoRA
lora_config = LoRAConfig(
    r=8,
    target_modules=["q_proj", "v_proj"],
    lora_alpha=16,
    lora_dropout=0.05
)

# åº”ç”¨ LoRA åˆ°æ¨¡å‹
model = apply_lora_to_model(model, lora_config)

# ç°åœ¨åªæœ‰ LoRA å‚æ•°ä¼šè¢«æ›´æ–°
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
StellarByte/
|   .gitignore
|   datasets.py
|   LICENSE
|   model_pretrain.py
|   model_stf_train.py
|   README.md
|   requirements.txt
|
+---checkpoints
+---configs
|       pretrain_config.yaml
|
+---datasets
|   |   data_preprocessor.py
|   |   pretrain_hq.jsonl
|   |
|   \---test
|           train.jsonl
|           val.jsonl
|
+---logs
+---model
|   |   Attention.py
|   |   config.py
|   |   DecoderLayer.py
|   |   MLP.py
|   |   Model.py
|   |   MoE.py
|   |   Position_Embedding.py
|   |   RMSNorm.py
|   |   __init__.py
|   |
|   +---utils
|          DropPath.py
|          KVCache.py
|          LoRA.py
|          Memory.py
|          __init__.py
|
+---model_info
+---scripts
+---test
|   |   test_Attention.py
|   |   test_datasets.py
|   |   test_DeocoderLayer.py
|   |   test_KVCache.py
|   |   test_LoRA.py
|   |   test_MLP.py
|   |   test_Position_Embedding.py
|   |   test_RMSNorm.py
|   |
|   +---test_results
|
+---tokenizer
|       special_tokens_map.json
|       tokenizer.json
|       tokenizer_config.json
|
+---utils
        checkpoint.py
        config_params.py
        logger.py
        model_info.py
        progressbar.py
```

## ğŸ”œ å¼€å‘è®¡åˆ’

<details> 
  <summary>2025.7.13</summary>

### Done:
1. å®ç°BaseModelConfigç±»ï¼Œåç»­çš„è¶…å‚æ•°å°†é€æ¸è¿­ä»£
2. å®ç°RMSNormå±‚å½’ä¸€åŒ–ç±»
3. Transformerç»å…¸çš„MultiHeadAttentionç±»

### TODOï¼š
1. Attentionåº”ç”¨KVç¼“å­˜ï¼Œæ·»åŠ é‡åŒ–æœºåˆ¶
2. æ„å»ºåŸºç¡€MLPå±‚
3. æ„å»ºåŸºç¡€DecoderLayerå±‚

</details>

---

<details> 
  <summary>2025.7.14</summary>

### Done:
1. å®ç°Attentionåº”ç”¨ç¼“å­˜æœºåˆ¶
2. å®ç°Attentioné‡åŒ–æœºåˆ¶
3. å®ç°åŸºç¡€MLPå±‚
4. å®ç°åŸºç¡€ByteDecoderLayerå±‚
5. å®ç°LoRA
6. å®ç°DropPath
7. å®ç°KVCacheæœºåˆ¶
8. é‡å†™äº†Attentionä¸­ä¸KVCacheç›¸å…³çš„éƒ¨åˆ†
9. å®ç°æ¨¡å‹è®­ç»ƒä¸­çš„å·¥å…·ç»„ä»¶
- æ—¥å¿—è®°å½•ç»„ä»¶
- ç‚«é…·è¿›åº¦æ¡åŠ è½½ç»„ä»¶
- æ¨¡å‹æƒé‡ç®¡ç†ç»„ä»¶
- æ¨¡å‹ä¿¡æ¯åˆ†æç»„ä»¶
10. å®ç°æ¨¡å‹è®­ç»ƒæ•°æ®é›†åŠ è½½å™¨åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®é›†åŠ è½½å™¨å’ŒSTFè®­ç»ƒæ•°æ®é›†åŠ è½½å™¨
10. åŸºæœ¬æ„å»ºæ¨¡å‹é¢„è®­ç»ƒæµç¨‹

### TODO:
1. æ„é€ Memoryç±»å¹¶è¿›è¡Œåº”ç”¨
2. åº”ç”¨LoRAç±»
3. æ„é€ å•æ­¥æ¨ç†æ¥å£ def forward_step(self, x_t, past_k, past_v) -> (out, new_k, new_v)
4. Xposä½ç½®ç¼–ç ä¼˜åŒ–
- ç»“æ„æ€§æ­£åˆ™
- åŠ¨æ€thetaè°ƒæ•´
5. Attention
- num_rep æœªè¢«ä½¿ç”¨ 
- å®ç°â€¯çº¿ç¨‹å¹¶è¡Œ/Allâ€‘Reduce
- è¿›ä¸€æ­¥èåˆFlashAttention-2
- çº¿æ€§å±‚é‡åŒ–quantize() ä½¿ç”¨äº† torch.quantization.quantize_dynamic()ï¼Œä½†è¿™ä»…é™äºçº¿æ€§å±‚ + æ¨ç†ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥æ”¯æŒGPTQ/AWQ/SmoothQuant
6. KVCache
å°† KVCache.append() æ”¹ä¸ºæ”¯æŒï¼š
- æ»‘çª—ï¼ˆsliding windowï¼‰æˆªæ–­
- å†™å…¥ä½ç½®å¹¶å‘é”å®šï¼ˆif multi-threadï¼‰
- Layer-wise tokenä½ç½®è‡ªåŠ¨åç§»è®¡ç®—

</details>

---

<details>
  <summary>2025.7.15</summary>

### Done:
1. æ„å»ºå¹¶ä¼˜åŒ–æ¨¡å‹è®­ç»ƒç»„ä»¶ï¼š
- æ£€æŸ¥ç‚¹ç®¡ç†ç»„ä»¶
- æ„å¤–ä¸­æ–­ä¿æŠ¤ç»„ä»¶
2. å®ç°LLMçš„è®°å¿†ç®¡ç†æœºåˆ¶
3. å®ç°å•æ­¥æ¨ç†æ¥å£ä»¥åŠç›¸å…³ç»„ä»¶
4. åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­åº”ç”¨æ£€æŸ¥ç‚¹ç®¡ç†ä»¥åŠæ„å¤–ä¸­æ–­ä¿æŠ¤ç»„ä»¶
5. å…³äºLoRA
- ä¼˜åŒ–LoRAï¼šæ”¯æŒçƒ­æ’æ‹”ã€æ­£åˆ™ä¼˜åŒ–ã€å‡å°‘å†…å­˜å ç”¨æå‡é€Ÿåº¦
- æµ‹è¯•LoRA
6. ä¼˜åŒ–ä½ç½®ç¼–ç 
- ç§»é™¤æ—‹è½¬ä½ç½®ç¼–ç çš„æœ¬åœ°ç¼“å­˜ï¼Œæ”¹ç”¨å…¨å±€ç¼“å­˜ç±»RotaryCache
- æ·»åŠ å¯å­¦ä¹ çš„ç¼©æ”¾å› å­å‚æ•°åˆ°XPosRotaryEmbedding
7. ç»Ÿä¸€äº†Attentionéƒ¨åˆ†çš„paramå‚æ•°dtypeå’Œæœ¬å±‚è®¡ç®—æ•°æ®dtype
- ç»Ÿä¸€è®¡ç®—ç²¾åº¦ä¸ºfloat32ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
8. KVCacheæ·»åŠ æ»‘åŠ¨çª—å£æˆªæ–­å¤„ç†

### TODOï¼š
1. LoRAè¿›ä¸€æ­¥ä¼˜åŒ–
- éçº¿æ€§LoRA
- æ”¯æŒConv2d/Transformer.Conv1dæ³¨å…¥
- é€‚é…é‡åŒ–æ¨¡å—
- Tunerå†»ç»“å±‚é€‰æ‹©ç­–ç•¥
2. Attention:
- å®ç°çº¿ç¨‹å¹¶è¡Œ/Allâ€‘Reduce
- è¿›ä¸€æ­¥èåˆFlashAttention-2
3. æµ‹è¯•
- æµ‹è¯•æ•°æ®é›†åŠ è½½å™¨
- æµ‹è¯•LoRA
- æµ‹è¯•Attention
- æµ‹è¯•Memory

### DEBUG
1. å·¥å…·è„šæœ¬åˆ†ææ¨¡å‹ä¿¡æ¯æŠ¥é”™

</details>

---

<details>
  <summary>2025.7.16</summary>

### Done:
1. å®ç°åˆ†å¸ƒå¼å¤šå¡è®­ç»ƒ
2. å®ç°å¼ é‡/æ¨¡å‹å¹¶è¡Œ
3. æ•´åˆæ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œå¹¶æ„é€ å‚æ•°è¯»å–å™¨
4. ä¼˜åŒ–æ˜¾å­˜å ç”¨ã€æå‡ååé€Ÿåº¦
- æ–°å¢å¯æ§æ¢¯åº¦æ£€æŸ¥ç‚¹
- æŒ‡å®šstepåæ¸…ç†æ— ç”¨ç°å­˜
- æ–°å¢FlashAttentionå¯æ§å¼€å…³
5. Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–
- è¿›ä¸€æ­¥èåˆFlashAttention-2
- æ–°å¢æ¨¡å‹/å¼ é‡å¹¶è¡Œå¤„ç†
6. æ¨¡å‹åˆ†æè„šæœ¬ model_info.py ä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- analyze_performance(),æ¯æ¬¡åˆ‡æ¢ batch_size å‰ï¼ŒæŠŠ KVCache æ¸…é›¶å¹¶æŠŠ batch_size è®¾å› Noneï¼Œè®©ä¸‹ä¸€è½® forward è‡ªåŠ¨é‡æ–°åˆ†é…ç¼“å­˜ã€‚
7. æµ‹è¯•è„šæœ¬é€šè¿‡
- Attentionæµ‹è¯•
- datasetsæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•
- LoRAæµ‹è¯•
8. ä¼˜åŒ–æ•°æ®é›†åŠ è½½å™¨
- æ©ç ä»01è½¬æ¢ä¸ºboolç±»å‹
- ä¼˜åŒ–æˆªæ–­å¤„ç†
9. tokenizerä¿®å¤ï¼šå¤„ç†tokenizerçš„padå¡«å……ä¸eos_tokenä¸€æ ·å¯¼è‡´å¡«å……æ··ä¹±ï¼Œåˆ†åˆ«ä½¿ç”¨ç‰¹æ®Šæ ‡è¯†
10. LoRAä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- æƒé‡åˆå¹¶çš„çº¿ç¨‹é”ä½œç”¨åŸŸè¿‡å¤§
- ç¡®ä¿ LoRA å¢é‡è®¡ç®— æ—¶æ•°æ®ç±»å‹ç»Ÿä¸€ä¸ºLoRAç»Ÿä¸€å‚æ•°ç±»å‹self.cfg.dtype or torch.float32
- è§£å†³ LoRAæ³¨å…¥é£é™©ï¼Œæ³¨å…¥å‰åˆ¤æ–­æ¨¡å—æ˜¯å¦å·²ç»æ˜¯ LoRALinearï¼Œè·³è¿‡æ³¨å…¥
- è§£å†³ LoRALinear å†…éƒ¨æƒé‡å¸ƒå±€ä¸ fan_in_fan_out å…³è”ä¸è¶³ é—®é¢˜ï¼Œåœ¨ forward é˜¶æ®µå¢é‡è®¡ç®—æ—¶æ ¹æ® fan_in_fan_out è½¬ç½® LoRA å‚æ•°
- è§£å†³ å¤šé€‚é…å™¨çƒ­åˆ‡æ¢çš„ activate() æœªè§£é™¤æ—§ LoRA æƒé‡å ç”¨æ˜¾å­˜ é—®é¢˜ï¼Œä¿å­˜åŸå§‹å±‚å¼•ç”¨ï¼Œdeactivate æ—¶æ¢å¤åŸå§‹å±‚ï¼Œå½»åº•å¸è½½æ—§ LoRAå±‚
- ä½¿ç”¨ å®‰å…¨ torch.load(), å½“å‰é»˜è®¤ weights_only=Falseï¼Œä½†å®˜æ–¹å·²å®£å¸ƒæœªæ¥ä¼šæ”¹ä¸ºâ€¯Trueï¼Œå› æ­¤æ„å»ºè‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä½¿ç”¨è¯¥å‚æ•°å¯¼å…¥å‡½æ•°
11. KVCacheä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- å½“ T_new >= self.max_T æ¡ä»¶æˆç«‹æ—¶ï¼Œoverflow è¢«èµ‹å€¼äº†ï¼Œä½† current_len æ²¡æœ‰è¢«èµ‹å€¼ã€‚appendå‡½æ•°ä¸­ï¼Œç»™ current_len èµ‹ä¸€ä¸ªåˆå§‹å€¼ï¼Œä¸”æ— è®ºå“ªæ¡åˆ†æ”¯éƒ½ä¿è¯ current_len å·²å®šä¹‰ã€‚
13. ä½¿ç”¨æ¨¡å‹åˆ†æè„šæœ¬å¯¹æ¨¡å‹è¿›è¡Œåˆæ­¥åˆ†æ


### TODO:
1. å®ç°DeepSeed
2. å®ç°åŠ¨æ€å‰ªæ
- AttentionåŠ¨æ€å‰ªæ
- KVCacheåŠ¨æ€å‰ªæ
3. æ•°æ®é›†åŠ è½½å™¨é’ˆå¯¹å¤§æ•°æ®é›†è¿›è¡Œstreamingä¼˜åŒ–
4. æ¨¡å‹åˆ†æè„šæœ¬
- ç»˜å›¾ä¸­æ–‡ä¸æ˜¾ç¤º
- æ¨¡å‹å±‚çº§ç»“æ„åˆ†æä¸é€å½»
- æ·»åŠ æ¨¡å‹å±‚çº§ç»“æ„ç»˜å›¾å¯è§†åŒ–

</details>

---

<details>
  <summary>2025.7.17</summary>

### DONE:
1. å¢å¼ºæ¨¡å‹åˆ†ææŠ¥å‘ŠåŠŸèƒ½å¹¶æ·»åŠ å¯è§†åŒ–å›¾è¡¨
- æ·»åŠ ä¸­æ–‡æ”¯æŒï¼Œè§£å†³å›¾è¡¨ä¸­æ–‡ä¹±ç é—®é¢˜
- æ–°å¢æ¨¡å‹ç»“æ„å›¾ã€ç¨€ç–åº¦çƒ­åŠ›å›¾å’Œé›·è¾¾å›¾ç­‰å¯è§†åŒ–åŠŸèƒ½
- æ”¹è¿›æ¨¡å‹æ¶æ„æè¿°æ ¼å¼ï¼Œå¢åŠ æ›´å¤šç»†èŠ‚
- ä¼˜åŒ–å‚æ•°åˆ†å¸ƒé¥¼å›¾æ ·å¼ï¼Œçªå‡ºæ˜¾ç¤ºæœ€å¤§å æ¯”éƒ¨åˆ†
2. é‡æ„æ¨¡å‹è®­ç»ƒç»„ä»¶â€”â€”è¿›åº¦æ¡ç»„ä»¶
- å°†åŸæœ‰çš„ RichProgressBar ç±»é‡æ„ä¸ºæ›´çµæ´»çš„ ProgressBarManager ç±»
- æ”¯æŒè®­ç»ƒå’ŒéªŒè¯é˜¶æ®µçš„å¤šä»»åŠ¡ç®¡ç†
- æ–°å¢éªŒè¯é˜¶æ®µæŒ‡æ ‡æ±‡æ€»è¡¨æ ¼æ˜¾ç¤ºåŠŸèƒ½
- æ”¹è¿›è¿›åº¦æ¡æ ·å¼å’Œäº¤äº’é€»è¾‘
3. é‡æ„æ¨¡å‹è®­ç»ƒç»„ä»¶,é‡æ„è®­ç»ƒå¾ªç¯å’ŒéªŒè¯é€»è¾‘ï¼Œæ”¹è¿›è¿›åº¦ç®¡ç†
- å°† RichProgressBar æ›¿æ¢ä¸ºæ›´é€šç”¨çš„ ProgressBarManager
- åœ¨è®­ç»ƒå’ŒéªŒè¯å¾ªç¯ä¸­æ·»åŠ è¿›åº¦æ¡æ”¯æŒ
- æ”¹è¿›æŒ‡æ ‡è®¡ç®—å’Œæ—¥å¿—è®°å½•ï¼Œæ·»åŠ å‡†ç¡®ç‡ç»Ÿè®¡
- ä¼˜åŒ–è®¾å¤‡ç®¡ç†å’Œåˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–é€»è¾‘
- è°ƒæ•´ AMP ä¸Šä¸‹æ–‡ç®¡ç†ä»¥æ”¯æŒä¸åŒè®¾å¤‡ç±»å‹
4. ä¼˜åŒ–RMSNormå±‚ï¼Œç§»é™¤ä¸å¿…è¦çš„bufferæ³¨å†Œå¹¶ç®€åŒ–epså¤„ç†
- ä¸å†å°†epsæ³¨å†Œä¸ºbufferï¼Œç›´æ¥ä½œä¸ºtensorå±æ€§ä½¿ç”¨ï¼Œç®€åŒ–ä»£ç ç»“æ„
5. Attentionå¤šå¤´å­æ³¨æ„åŠ›å±‚å…³äºåˆ†å¸ƒå¼è®­ç»ƒè§£å†³éšè—bug
- æ·»åŠ åˆ†å¸ƒå¼åˆå§‹åŒ–æ–¹æ³•å¹¶æ”¹è¿›å¹¶è¡Œé€šä¿¡é€»è¾‘
- æ·»åŠ  init_distributed_mode æ–¹æ³•ç”¨äºæ›´çµæ´»çš„åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–ï¼Œæ”¯æŒä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è¯»å–é…ç½®
- é‡æ„æ¨¡å‹å¹¶è¡Œé€šä¿¡ç»„çš„åˆå§‹åŒ–é€»è¾‘ï¼Œå¢åŠ å¯¹æœªè®¾ç½®åˆ†å¸ƒå¼å˜é‡çš„é”™è¯¯æ£€æŸ¥
6. datasetsæ•°æ®é›†åŠ è½½å™¨ä¿®å¤æ ‡ç­¾å¼ é‡ä¸­æ©ç æœªæ­£ç¡®åº”ç”¨çš„é—®é¢˜
- åœ¨PretrainDatasetä¸­ï¼Œå½“æ©ç ä¸ºFalseæ—¶ï¼Œå¯¹åº”çš„æ ‡ç­¾å¼ é‡å€¼åº”è®¾ä¸º-100ä»¥é¿å…å½±å“æŸå¤±è®¡ç®—ã€‚æ­¤ä¿®æ”¹ç¡®ä¿äº†æŸå¤±å‡½æ•°ä»…è®¡ç®—æœ‰æ•ˆæ ‡è®°çš„æŸå¤±


### TODO:
1. å…³é”®ç±»æ·»åŠ è°ƒè¯•å‡½æ•°
2. ç»Ÿä¸€æ³¨é‡Šé£æ ¼å¹¶å®Œå–„æ³¨é‡Š
3. ä¿®å¤æ¨¡å‹è®­ç»ƒè„šæœ¬ä¸­çš„é—®é¢˜ï¼š
- æ— æ³•æ˜¾ç¤ºä¸€ä¸ªepochä¸­çš„stepè¿›åº¦
- è‡ªåŠ¨æ„å»ºæˆ–è€…è·å–è™šæ‹Ÿç¯å¢ƒä¿¡æ¯
4. è°ƒè¯•æ¨¡å‹é¢„è®­ç»ƒè„šæœ¬

</details>

---

<details>
  <summary>2025.7.18</summary>

### DONE:

1. è®­ç»ƒè„šæœ¬ä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- (1) æ€§èƒ½ä¸è®­ç»ƒæ•ˆç‡çš„é—®é¢˜
  - ä¿®å¤å­¦ä¹ ç‡è°ƒåº¦å™¨è®¡ç®—æ–¹å¼ä¸åˆç†ï¼Œget_lr ä¸­å°† total_iters = len(train_loader) * epochs * (restarts + 1)ï¼Œä½† step å®é™…æ˜¯ per-epoch å†…éƒ¨ stepã€‚è®­ç»ƒä¸­åº”ä½¿ç”¨å…¨å±€ stepï¼ˆglobal_step = epoch * steps_per_epoch + stepï¼‰ï¼Œå¦åˆ™è°ƒåº¦æ›²çº¿ä¸å¹³æ»‘ã€‚
  - ä¿®å¤æ²¡æœ‰æ¢¯åº¦ç´¯è®¡ä¸‹çš„æ­£ç¡®total_stepæ”¯æŒï¼Œåœ¨ get_lr() å’Œ total_iters ä¸­æœªè€ƒè™‘ accumulation_stepsï¼Œå¯¼è‡´è®­ç»ƒå®é™…æ›´æ–°æ­¥æ•°ä¸é¢„æœŸä¸ç¬¦ï¼Œè°ƒåº¦å¤±è¡¡ã€‚
  - å¯ç”¨cudnn.allow.tf32ï¼Œåˆå§‹è®­ç»ƒè„šæœ¬è®¾å®šäº† torch.backends.cuda.matmul.allow_tf32 = Trueï¼Œä½†æ²¡æœ‰è®¾ç½® torch.backends.cudnn.allow_tf32 = Trueã€‚è¿™ä¼šé”™å¤±ä¸€åŠä»¥ä¸Šçš„ TF32 ç®—å­åŠ é€Ÿæœºä¼šã€‚
  - torch.cuda.empty_cache() è°ƒç”¨é¢‘ç¹
- (2) åˆ†å¸ƒå¼è®­ç»ƒçš„é—®é¢˜ï¼Œæ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒ
  - ä¿®å¤ DDPæ—¥å¿—æœªåŠ  rank è¿‡æ»¤ çš„é—®é¢˜ï¼Œè™½ç„¶å¤§å¤šæ•°æ—¥å¿—éƒ½ç”¨ is_main_process() åšäº†åˆ¤æ–­ï¼Œä½†æŸäº›å¼‚å¸¸æ•è·å¦‚ except Exception æˆ– init_distributed() å†…ä»ä¼šå…¨ rank æ‰“å°ï¼Œå»ºè®®ç»Ÿä¸€å°è£…æ—¥å¿—å™¨ã€‚
  - ä¿®å¤ DDPæ¢å¤ä¸å®Œæ•´çš„é—®é¢˜ï¼Œåœ¨æ¢å¤æ¨¡å‹ checkpoint æ—¶ï¼Œstart_step æ²¡æœ‰ç»§ç»­ä½œä¸º global_step ä¼ å…¥ train_epoch()ï¼Œå¯¼è‡´æ–­ç‚¹æ¢å¤è®­ç»ƒæ—¶çš„ LRè°ƒåº¦ã€æ—¥å¿—æ­¥æ•°ã€SwanLab step ä¸å‡†ç¡®ã€‚
- (3) æ˜¾å­˜ç®¡ç†å’Œç¨³å®šæ€§é—®é¢˜
  - ä½¿ç”¨ model.zero_grad(set_to_none=True)ï¼Œæ˜¾å¼ç”¨ set_to_none=True æ›¿æ¢ zero_grad()ï¼Œå¯é‡Šæ”¾æ›´æ—©çš„ grad æ˜¾å­˜ï¼Œæå‡æ˜¾å­˜æ•ˆç‡ã€‚
  - ä¿®å¤ Gradient Checkpoint æœªæŒ‰å±‚ç²’åº¦é…ç½® çš„é—®é¢˜ï¼Œå¯ç”¨äº† model.gradient_checkpointing_enable()ï¼Œä½†è‹¥æ¨¡å‹ç»“æ„è¾ƒæ·±ï¼Œåº”é…åˆé€å±‚æ˜¾å¼è®¾ç½® checkpointing=True çš„ç­–ç•¥ï¼Œæ‰æœ‰å®é™…æ•ˆæœã€‚
- (4) é²æ£’æ€§ä¸å¼‚å¸¸æ¢å¤é—®é¢˜
  - è§£å†³ å¼‚å¸¸æ¢å¤æœªè®°å½• global_step é—®é¢˜ï¼Œæ£€æŸ¥ç‚¹ä¸­åªæœ‰ epoch, stepï¼Œæœªè®°å½• global_stepï¼Œå¯¼è‡´è°ƒåº¦å™¨ä¸æ—¥å¿—é‡å¯åä¸ä¸€è‡´ã€‚
  - è§£å†³ è®­ç»ƒå¼ˆåœºæ•è·æœªç»†åŒ– é—®é¢˜ï¼Œexcept Exception as e: ä¸­æ²¡æœ‰ä½¿ç”¨ traceback.print_exc()ï¼Œæ’æŸ¥é—®é¢˜å›°éš¾ã€‚
- (5)æ·»åŠ  Tokenizer.embedding_sync()ï¼Œæ£€æŸ¥ tokenizer ä¸ embedding å¤§å°æ˜¯å¦åŒæ­¥
- (6)å¢åŠ æ ‡ç­¾å¹³æ»‘æŸå¤±è®¡ç®—å‡½æ•°
- (7)æ·»åŠ CUDAå›¾ä¼˜åŒ–æ ‡è®°
2. RSMNormä¿®å¤ï¼š
- å°†epsä»tensoræ”¹ä¸ºfloatç±»å‹é¿å…é‡å¤è½¬æ¢ï¼Œå‡å°‘å†…å­˜
- å°†epsè½¬æ¢ä¸ºä¸rmsç›¸åŒçš„è®¾å¤‡ä»¥é¿å…è·¨è®¾å¤‡æ“ä½œ
- ç§»é™¤å†—ä½™çš„inv_rmsç±»å‹è½¬æ¢
- ç”¨ torch._dynamo.disable() è£…é¥°å™¨å…³é—­ RMSNorm çš„ forward ç¼–è¯‘ï¼Œé¿å… CUDA Graph å†…å­˜å¤ç”¨å†²çªã€‚
3. Attentionä¿®å¤ï¼š
- æ·»åŠ è‡ªåŠ¨è°ƒæ•´additive_maské•¿åº¦çš„åŠŸèƒ½
- æ–°å¢_adjust_additive_maskæ–¹æ³•ç”¨äºè‡ªåŠ¨å°†additive_maské•¿åº¦ä¸é”®å€¼åºåˆ—å¯¹é½ï¼Œè§£å†³KVç¼“å­˜é•¿åº¦ä¸åŒ¹é…é—®é¢˜
4. Modelä¿®å¤ï¼š
- ä¿®å¤æ³¨æ„åŠ›æ©ç å’Œè®¾å¤‡ç±»å‹ä¸ä¸€è‡´çš„é—®é¢˜å¹¶æ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä¿®å¤äº†æ³¨æ„åŠ›æ©ç ä¸hidden_statesè®¾å¤‡ç±»å‹ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå°†æ©ç è½¬æ¢ä¸ºç›¸åŒè®¾å¤‡å’Œç±»å‹ã€‚
- åŒæ—¶æ·»åŠ äº†æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½ä»¥åœ¨è®­ç»ƒæ—¶èŠ‚çœæ˜¾å­˜ï¼Œä½¿ç”¨éé‡å…¥æ–¹å¼æé«˜ç¨³å®šæ€§ã€‚
5. Loggeræ—¥å¿—è®°å½•å™¨å®Œå–„ï¼šä¸ºæ—¥å¿—æ„å»ºå‡½æ•°æ·»åŠ æ§åˆ¶å°æ—¥å¿—çº§åˆ«å‚æ•°
- æ·»åŠ  console_level å‚æ•°ä»¥å…è®¸è‡ªå®šä¹‰æ§åˆ¶å°è¾“å‡ºçš„æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¿æŒä¸º INFO çº§åˆ«
6. æ›´æ–°é¢„è®­ç»ƒé…ç½®å‚æ•°å’Œæ³¨é‡Šæ ¼å¼
- å°† eval_max_steps æ”¹ä¸º eval_interval ä»¥æ›´å‡†ç¡®æè¿°åŠŸèƒ½
- æ›´æ–°ç‰¹æ®Šæ ‡è®°æ ¼å¼ä¸º <|SBOS|> å’Œ <|SEOS|>
- è°ƒæ•´ vocab_size å’Œå¹¶è¡Œé…ç½®å‚æ•°
- ä¸ºæ—¥å¿—é…ç½®æ·»åŠ æ³¨é‡Šè¯´æ˜

### TODO:
1. ä¼˜åŒ–è®­ç»ƒè„šæœ¬ï¼š
- æ·»åŠ  æ—©åœæ£€æŸ¥
- æ·»åŠ  EMAï¼Œå¹³æ»‘æ”¶æ•›è¿‡ç¨‹ï¼Œæå‡ç²¾åº¦
- æ„å»º metrics.pyï¼Œç»Ÿä¸€ç®¡ç†è®­ç»ƒæŒ‡æ ‡
- ç²¾åº¦ä¸è®­ç»ƒæ”¶æ•›çš„é—®é¢˜
  - ä½¿ç”¨ Label Smoothingï¼Œé¿å…è¿‡æ‹Ÿåˆå’Œæå‡æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®æ”¯æŒå‚æ•°é…ç½®ã€‚
  - å¼•å…¥ Prompt Maskã€Position Shiftç­‰è®­ç»ƒæŠ€å·§
  - PPLè®¡ç®—å¯èƒ½ä¸å‡†ç¡®ï¼Œå½“å‰ evaluate() ä¸­ ppl = exp(avg_loss)ï¼Œä½† avg_loss æ˜¯å¹³å‡ token lossï¼Œè‹¥ loss mask æœªæ­£ç¡®å¤„ç†ï¼Œå¯èƒ½é€ æˆè¿‡å¤§åå·®ã€‚
- ä½¿ç”¨FSDPï¼Œå½“å‰æœ€å¤§æ”¯æŒ DP + DDPï¼Œæœªä½¿ç”¨ torch.distributed.fsdpï¼Œå¯¹äºå‡ åäº¿å‚æ•°ä»¥ä¸Šçš„å¤§æ¨¡å‹åœ¨å¤šèŠ‚ç‚¹ä¸‹ä¸å¤Ÿé«˜æ•ˆã€‚
2. å®Œå–„æ„å¤–ç»ˆæ­¢å¤„ç†
- ä¿®å¤æ„å¤–ç»ˆæ­¢åçˆ†å‡ºå¤§é‡é”™è¯¯çš„é—®é¢˜ï¼Œå½“è®­ç»ƒæ„å¤–ç»ˆæ­¢æ—¶ï¼Œä¼šè§¦å‘å¼‚å¸¸æ•è·ï¼Œä½†å¼‚å¸¸æ•è·åæœªæ¸…ç†ç¯å¢ƒï¼Œå¯¼è‡´å¤§é‡é”™è¯¯æ—¥å¿—è¾“å‡ºã€‚
```
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2025-07-18 13:12:28] [WARNING] [ByteLogger] ğŸ’€ æ”¶åˆ° SIGINTï¼Œå†™å…¥å®Œæ•´æ£€æŸ¥ç‚¹ â€¦
[2025-07-18 13:12:28] [ERROR] [ByteLogger] è®­ç»ƒå¼‚å¸¸: DataLoader worker (pid 2124) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
[2025-07-18 13:12:28] [INFO] [ByteLogger] ğŸ’€ å¼‚å¸¸é€€å‡ºï¼Œæ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹â€¦
Traceback (most recent call last):
  File "/workspace/model_pretrain.py", line 685, in <module>
    train(args, logger)
  File "/workspace/model_pretrain.py", line 636, in train
    raise e
  File "/workspace/model_pretrain.py", line 597, in train
    train_epoch(
  File "/workspace/model_pretrain.py", line 397, in train_epoch
    scaler.scale(loss).backward()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2124) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
```
3. è§£å†³ æœªæ£€æµ‹æ¢¯åº¦å¼‚å¸¸ï¼ˆNaNï¼‰é—®é¢˜ï¼Œè‹¥ loss = NaNã€grad = infï¼Œåº”ç«‹å³ä¸­æ­¢è®­ç»ƒä¿å­˜ checkpointï¼Œé¿å…æµªè´¹èµ„æºã€‚

### DEBUG
1. æ‰¾å‡ºtorch.utils.checkpointé—®é¢˜æ ¹æºå¹¶è¿›è¡Œä¿®å¤
```
/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ–°åŠŸèƒ½å»ºè®®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºæ‚¨çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. æ‰“å¼€ä¸€ä¸ª Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸŒŸ è‡´è°¢

- æ„Ÿè°¢æ‰€æœ‰ä¸º Transformer æ¶æ„å‘å±•åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…
- æ„Ÿè°¢ HuggingFace å›¢é˜Ÿæä¾›çš„å‡ºè‰²å·¥å…·å’Œç”Ÿæ€ç³»ç»Ÿ
- æ„Ÿè°¢æ‰€æœ‰é¡¹ç›®è´¡çŒ®è€…

---

<div align="center">
  <sub>æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯ï¼Œç…§è§å¤ä»ŠåŒæœ›çš„å¤œç©ºã€‚</sub>
</div>