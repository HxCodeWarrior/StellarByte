<div align="center">

# âœ¨ StellarByte âœ¨

<p>æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯ï¼Œç…§è§å¤ä»ŠåŒæœ›çš„å¤œç©ºã€‚</p>

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange?style=flat-square&logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow?style=flat-square)](https://huggingface.co/)
[![Blog](https://img.shields.io/badge/Blog-ByteWyrm-pink?style=flat-square)](https://blog.devnest.top/)

</div>

## ğŸ“š ç®€ä»‹

StellarByte æ˜¯ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹å®ç°ï¼Œä¸ HuggingFace ç”Ÿæ€å®Œå…¨å…¼å®¹ã€‚è¯¥é¡¹ç›®èåˆäº†å¤šç§ç°ä»£ Transformer ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æä¾›é«˜æ•ˆã€çµæ´»ä¸”æ˜“äºä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆå¼ AI ä»»åŠ¡ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å®ç°**ï¼šé›†æˆ FlashAttentionã€KV ç¼“å­˜ç­‰ä¼˜åŒ–æŠ€æœ¯
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶å¯ç‹¬ç«‹ä½¿ç”¨æˆ–ç»„åˆ
- ğŸ”„ **XPos æ—‹è½¬ä½ç½®ç¼–ç **ï¼šæ”¹è¿›çš„ RoPE ä½ç½®ç¼–ç ï¼Œæé«˜é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›
- ğŸ› ï¸ **ä¸°å¯Œçš„ä¼˜åŒ–æŠ€æœ¯**ï¼š
  - âš™ï¸ DeepNorm å½’ä¸€åŒ–ç­–ç•¥
  - ğŸ” LayerScale åˆå§‹åŒ–æŠ€æœ¯
  - ğŸ”€ DropPath æ­£åˆ™åŒ–
  - âš¡ å¹¶è¡Œæ®‹å·®è¿æ¥
- ğŸ“Š **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šå†…ç½® LoRA ä½ç§©é€‚åº”å®ç°
- ğŸ¤— **HuggingFace å…¼å®¹**ï¼šæ— ç¼é›†æˆ Transformers ç”Ÿæ€ç³»ç»Ÿ
- ğŸ“ **æ¸…æ™°çš„ä»£ç ç»“æ„**ï¼šæ˜“äºç†è§£å’Œæ‰©å±•

## ğŸ”§ å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.5.1+
- CUDA 11.8+ (GPUåŠ é€Ÿï¼Œå¯é€‰)

### å®‰è£…æ­¥éª¤

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/HxCodeWarrior/StellarByte.git
cd StellarByte

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¦‚æœéœ€è¦å¼€å‘ç¯å¢ƒ
pip install -r requirements.txt[dev]

# å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼ˆæš‚æœªå®ç°ï¼‰
# pip install -e .
```

### ä¾èµ–è¯´æ˜

é¡¹ç›®ä¾èµ–å·²æŒ‰åŠŸèƒ½æ¨¡å—åˆ†ç±»æ•´ç†ï¼š

- **æ ¸å¿ƒä¾èµ–**ï¼šPyTorchã€Transformersã€æ•°æ®å¤„ç†åº“
- **æ¨¡å‹ç»„ä»¶**ï¼šä½ç½®ç¼–ç ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰å®ç°
- **å¯è§†åŒ–ä¸ç›‘æ§**ï¼šå®éªŒè¿½è¸ªã€æŒ‡æ ‡å¯è§†åŒ–
- **æµ‹è¯•ä¸å¼€å‘**ï¼šå•å…ƒæµ‹è¯•ã€ç±»å‹æ£€æŸ¥
- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šå¤šGPU/å¤šèŠ‚ç‚¹è®­ç»ƒæ”¯æŒ
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå†…å­˜ä¼˜åŒ–ã€è®¡ç®—åŠ é€Ÿ


## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
import torch
from stellarbyte import ByteTransformer, ByteConfig

# åˆ›å»ºé…ç½®
config = ByteModelConfig(
    vocab_size=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072
)

# åˆå§‹åŒ–æ¨¡å‹
model = ByteTransformer(config)

# å‡†å¤‡è¾“å…¥
inputs = torch.randint(0, 32000, (1, 512))

# å‰å‘ä¼ æ’­
outputs = model(inputs)
```

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### ä» HuggingFace åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from stellarbyte import ByteTransformer
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = ByteTransformer.from_pretrained("path/to/model")
tokenizer = AutoTokenizer.from_pretrained("path/to/tokenizer")

# ç¼–ç æ–‡æœ¬
inputs = tokenizer("æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯", return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(inputs.input_ids, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### ä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

```python
from stellarbyte import ByteTransformer, LoRAConfig
from stellarbyte.lora import apply_lora_to_model

# åŠ è½½åŸºç¡€æ¨¡å‹
model = ByteTransformer.from_pretrained("path/to/model")

# é…ç½® LoRA
lora_config = LoRAConfig(
    r=8,
    target_modules=["q_proj", "v_proj"],
    lora_alpha=16,
    lora_dropout=0.05
)

# åº”ç”¨ LoRA åˆ°æ¨¡å‹
model = apply_lora_to_model(model, lora_config)

# ç°åœ¨åªæœ‰ LoRA å‚æ•°ä¼šè¢«æ›´æ–°
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
StellarByte/
|   .gitignore
|   datasets.py
|   LICENSE
|   model_pretrain.py
|   model_stf_train.py
|   README.md
|   requirements.txt
|
+---checkpoints
+---configs
|       pretrain_config.yaml
|
+---datasets
|   |   data_preprocessor.py
|   |   pretrain_hq.jsonl
|   |
|   \---test
|           train.jsonl
|           val.jsonl
|
+---logs
+---model
|   |   Attention.py
|   |   config.py
|   |   DecoderLayer.py
|   |   MLP.py
|   |   Model.py
|   |   MoE.py
|   |   Position_Embedding.py
|   |   RMSNorm.py
|   |   __init__.py
|   |
|   +---utils
|          DropPath.py
|          KVCache.py
|          LoRA.py
|          Memory.py
|          __init__.py
|
+---model_info
+---scripts
+---test
|   |   test_Attention.py
|   |   test_datasets.py
|   |   test_DeocoderLayer.py
|   |   test_KVCache.py
|   |   test_LoRA.py
|   |   test_MLP.py
|   |   test_Position_Embedding.py
|   |   test_RMSNorm.py
|   |
|   +---test_results
|
+---tokenizer
|       special_tokens_map.json
|       tokenizer.json
|       tokenizer_config.json
|
+---utils
        checkpoint.py
        config_params.py
        logger.py
        model_info.py
        progressbar.py
```

## ğŸ”œ å¼€å‘è®¡åˆ’

<details> 
<summary>2025.7.13</summary>

### Done:
1. å®ç°BaseModelConfigç±»ï¼Œåç»­çš„è¶…å‚æ•°å°†é€æ¸è¿­ä»£
2. å®ç°RMSNormå±‚å½’ä¸€åŒ–ç±»
3. Transformerç»å…¸çš„MultiHeadAttentionç±»

### TODOï¼š
1. Attentionåº”ç”¨KVç¼“å­˜ï¼Œæ·»åŠ é‡åŒ–æœºåˆ¶
2. æ„å»ºåŸºç¡€MLPå±‚
3. æ„å»ºåŸºç¡€DecoderLayerå±‚

</details>

---

<details> 
<summary>2025.7.14</summary>

### Done:
1. å®ç°Attentionåº”ç”¨ç¼“å­˜æœºåˆ¶
2. å®ç°Attentioné‡åŒ–æœºåˆ¶
3. å®ç°åŸºç¡€MLPå±‚
4. å®ç°åŸºç¡€ByteDecoderLayerå±‚
5. å®ç°LoRA
6. å®ç°DropPath
7. å®ç°KVCacheæœºåˆ¶
8. é‡å†™äº†Attentionä¸­ä¸KVCacheç›¸å…³çš„éƒ¨åˆ†
9. å®ç°æ¨¡å‹è®­ç»ƒä¸­çš„å·¥å…·ç»„ä»¶
- æ—¥å¿—è®°å½•ç»„ä»¶
- ç‚«é…·è¿›åº¦æ¡åŠ è½½ç»„ä»¶
- æ¨¡å‹æƒé‡ç®¡ç†ç»„ä»¶
- æ¨¡å‹ä¿¡æ¯åˆ†æç»„ä»¶
10. å®ç°æ¨¡å‹è®­ç»ƒæ•°æ®é›†åŠ è½½å™¨åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®é›†åŠ è½½å™¨å’ŒSTFè®­ç»ƒæ•°æ®é›†åŠ è½½å™¨
10. åŸºæœ¬æ„å»ºæ¨¡å‹é¢„è®­ç»ƒæµç¨‹

### TODO:
1. æ„é€ Memoryç±»å¹¶è¿›è¡Œåº”ç”¨
2. åº”ç”¨LoRAç±»
3. æ„é€ å•æ­¥æ¨ç†æ¥å£ def forward_step(self, x_t, past_k, past_v) -> (out, new_k, new_v)
4. Xposä½ç½®ç¼–ç ä¼˜åŒ–
- ç»“æ„æ€§æ­£åˆ™
- åŠ¨æ€thetaè°ƒæ•´
5. Attention
- num_rep æœªè¢«ä½¿ç”¨ 
- å®ç°â€¯çº¿ç¨‹å¹¶è¡Œ/Allâ€‘Reduce
- è¿›ä¸€æ­¥èåˆFlashAttention-2
- çº¿æ€§å±‚é‡åŒ–quantize() ä½¿ç”¨äº† torch.quantization.quantize_dynamic()ï¼Œä½†è¿™ä»…é™äºçº¿æ€§å±‚ + æ¨ç†ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥æ”¯æŒGPTQ/AWQ/SmoothQuant
6. KVCache
å°† KVCache.append() æ”¹ä¸ºæ”¯æŒï¼š
- æ»‘çª—ï¼ˆsliding windowï¼‰æˆªæ–­
- å†™å…¥ä½ç½®å¹¶å‘é”å®šï¼ˆif multi-threadï¼‰
- Layer-wise tokenä½ç½®è‡ªåŠ¨åç§»è®¡ç®—

</details>

---

<details>
<summary>2025.7.15</summary>

### Done:
1. æ„å»ºå¹¶ä¼˜åŒ–æ¨¡å‹è®­ç»ƒç»„ä»¶ï¼š
- æ£€æŸ¥ç‚¹ç®¡ç†ç»„ä»¶
- æ„å¤–ä¸­æ–­ä¿æŠ¤ç»„ä»¶
2. å®ç°LLMçš„è®°å¿†ç®¡ç†æœºåˆ¶
3. å®ç°å•æ­¥æ¨ç†æ¥å£ä»¥åŠç›¸å…³ç»„ä»¶
4. åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­åº”ç”¨æ£€æŸ¥ç‚¹ç®¡ç†ä»¥åŠæ„å¤–ä¸­æ–­ä¿æŠ¤ç»„ä»¶
5. å…³äºLoRA
- ä¼˜åŒ–LoRAï¼šæ”¯æŒçƒ­æ’æ‹”ã€æ­£åˆ™ä¼˜åŒ–ã€å‡å°‘å†…å­˜å ç”¨æå‡é€Ÿåº¦
- æµ‹è¯•LoRA
6. ä¼˜åŒ–ä½ç½®ç¼–ç 
- ç§»é™¤æ—‹è½¬ä½ç½®ç¼–ç çš„æœ¬åœ°ç¼“å­˜ï¼Œæ”¹ç”¨å…¨å±€ç¼“å­˜ç±»RotaryCache
- æ·»åŠ å¯å­¦ä¹ çš„ç¼©æ”¾å› å­å‚æ•°åˆ°XPosRotaryEmbedding
7. ç»Ÿä¸€äº†Attentionéƒ¨åˆ†çš„paramå‚æ•°dtypeå’Œæœ¬å±‚è®¡ç®—æ•°æ®dtype
- ç»Ÿä¸€è®¡ç®—ç²¾åº¦ä¸ºfloat32ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
8. KVCacheæ·»åŠ æ»‘åŠ¨çª—å£æˆªæ–­å¤„ç†

### TODOï¼š
1. LoRAè¿›ä¸€æ­¥ä¼˜åŒ–
- éçº¿æ€§LoRA
- æ”¯æŒConv2d/Transformer.Conv1dæ³¨å…¥
- é€‚é…é‡åŒ–æ¨¡å—
- Tunerå†»ç»“å±‚é€‰æ‹©ç­–ç•¥
2. Attention:
- å®ç°çº¿ç¨‹å¹¶è¡Œ/Allâ€‘Reduce
- è¿›ä¸€æ­¥èåˆFlashAttention-2
3. æµ‹è¯•
- æµ‹è¯•æ•°æ®é›†åŠ è½½å™¨
- æµ‹è¯•LoRA
- æµ‹è¯•Attention
- æµ‹è¯•Memory

### DEBUG
1. å·¥å…·è„šæœ¬åˆ†ææ¨¡å‹ä¿¡æ¯æŠ¥é”™

</details>

---

<details>
<summary>2025.7.16</summary>

### Done:
1. å®ç°åˆ†å¸ƒå¼å¤šå¡è®­ç»ƒ
2. å®ç°å¼ é‡/æ¨¡å‹å¹¶è¡Œ
3. æ•´åˆæ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œå¹¶æ„é€ å‚æ•°è¯»å–å™¨
4. ä¼˜åŒ–æ˜¾å­˜å ç”¨ã€æå‡ååé€Ÿåº¦
- æ–°å¢å¯æ§æ¢¯åº¦æ£€æŸ¥ç‚¹
- æŒ‡å®šstepåæ¸…ç†æ— ç”¨ç°å­˜
- æ–°å¢FlashAttentionå¯æ§å¼€å…³
5. Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–
- è¿›ä¸€æ­¥èåˆFlashAttention-2
- æ–°å¢æ¨¡å‹/å¼ é‡å¹¶è¡Œå¤„ç†
6. æ¨¡å‹åˆ†æè„šæœ¬ model_info.py ä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- analyze_performance(),æ¯æ¬¡åˆ‡æ¢ batch_size å‰ï¼ŒæŠŠ KVCache æ¸…é›¶å¹¶æŠŠ batch_size è®¾å› Noneï¼Œè®©ä¸‹ä¸€è½® forward è‡ªåŠ¨é‡æ–°åˆ†é…ç¼“å­˜ã€‚
7. æµ‹è¯•è„šæœ¬é€šè¿‡
- Attentionæµ‹è¯•
- datasetsæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•
- LoRAæµ‹è¯•
8. ä¼˜åŒ–æ•°æ®é›†åŠ è½½å™¨
- æ©ç ä»01è½¬æ¢ä¸ºboolç±»å‹
- ä¼˜åŒ–æˆªæ–­å¤„ç†
9. tokenizerä¿®å¤ï¼šå¤„ç†tokenizerçš„padå¡«å……ä¸eos_tokenä¸€æ ·å¯¼è‡´å¡«å……æ··ä¹±ï¼Œåˆ†åˆ«ä½¿ç”¨ç‰¹æ®Šæ ‡è¯†
10. LoRAä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- æƒé‡åˆå¹¶çš„çº¿ç¨‹é”ä½œç”¨åŸŸè¿‡å¤§
- ç¡®ä¿ LoRA å¢é‡è®¡ç®— æ—¶æ•°æ®ç±»å‹ç»Ÿä¸€ä¸ºLoRAç»Ÿä¸€å‚æ•°ç±»å‹self.cfg.dtype or torch.float32
- è§£å†³ LoRAæ³¨å…¥é£é™©ï¼Œæ³¨å…¥å‰åˆ¤æ–­æ¨¡å—æ˜¯å¦å·²ç»æ˜¯ LoRALinearï¼Œè·³è¿‡æ³¨å…¥
- è§£å†³ LoRALinear å†…éƒ¨æƒé‡å¸ƒå±€ä¸ fan_in_fan_out å…³è”ä¸è¶³ é—®é¢˜ï¼Œåœ¨ forward é˜¶æ®µå¢é‡è®¡ç®—æ—¶æ ¹æ® fan_in_fan_out è½¬ç½® LoRA å‚æ•°
- è§£å†³ å¤šé€‚é…å™¨çƒ­åˆ‡æ¢çš„ activate() æœªè§£é™¤æ—§ LoRA æƒé‡å ç”¨æ˜¾å­˜ é—®é¢˜ï¼Œä¿å­˜åŸå§‹å±‚å¼•ç”¨ï¼Œdeactivate æ—¶æ¢å¤åŸå§‹å±‚ï¼Œå½»åº•å¸è½½æ—§ LoRAå±‚
- ä½¿ç”¨ å®‰å…¨ torch.load(), å½“å‰é»˜è®¤ weights_only=Falseï¼Œä½†å®˜æ–¹å·²å®£å¸ƒæœªæ¥ä¼šæ”¹ä¸ºâ€¯Trueï¼Œå› æ­¤æ„å»ºè‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä½¿ç”¨è¯¥å‚æ•°å¯¼å…¥å‡½æ•°
11. KVCacheä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- å½“ T_new >= self.max_T æ¡ä»¶æˆç«‹æ—¶ï¼Œoverflow è¢«èµ‹å€¼äº†ï¼Œä½† current_len æ²¡æœ‰è¢«èµ‹å€¼ã€‚appendå‡½æ•°ä¸­ï¼Œç»™ current_len èµ‹ä¸€ä¸ªåˆå§‹å€¼ï¼Œä¸”æ— è®ºå“ªæ¡åˆ†æ”¯éƒ½ä¿è¯ current_len å·²å®šä¹‰ã€‚
13. ä½¿ç”¨æ¨¡å‹åˆ†æè„šæœ¬å¯¹æ¨¡å‹è¿›è¡Œåˆæ­¥åˆ†æ


### TODO:
1. å®ç°DeepSeed
2. å®ç°åŠ¨æ€å‰ªæ
- AttentionåŠ¨æ€å‰ªæ
- KVCacheåŠ¨æ€å‰ªæ
3. æ•°æ®é›†åŠ è½½å™¨é’ˆå¯¹å¤§æ•°æ®é›†è¿›è¡Œstreamingä¼˜åŒ–
4. æ¨¡å‹åˆ†æè„šæœ¬
- ç»˜å›¾ä¸­æ–‡ä¸æ˜¾ç¤º
- æ¨¡å‹å±‚çº§ç»“æ„åˆ†æä¸é€å½»
- æ·»åŠ æ¨¡å‹å±‚çº§ç»“æ„ç»˜å›¾å¯è§†åŒ–

</details>

---

<details>
<summary>2025.7.17</summary>

### DONE:
1. å¢å¼ºæ¨¡å‹åˆ†ææŠ¥å‘ŠåŠŸèƒ½å¹¶æ·»åŠ å¯è§†åŒ–å›¾è¡¨
- æ·»åŠ ä¸­æ–‡æ”¯æŒï¼Œè§£å†³å›¾è¡¨ä¸­æ–‡ä¹±ç é—®é¢˜
- æ–°å¢æ¨¡å‹ç»“æ„å›¾ã€ç¨€ç–åº¦çƒ­åŠ›å›¾å’Œé›·è¾¾å›¾ç­‰å¯è§†åŒ–åŠŸèƒ½
- æ”¹è¿›æ¨¡å‹æ¶æ„æè¿°æ ¼å¼ï¼Œå¢åŠ æ›´å¤šç»†èŠ‚
- ä¼˜åŒ–å‚æ•°åˆ†å¸ƒé¥¼å›¾æ ·å¼ï¼Œçªå‡ºæ˜¾ç¤ºæœ€å¤§å æ¯”éƒ¨åˆ†
2. é‡æ„æ¨¡å‹è®­ç»ƒç»„ä»¶â€”â€”è¿›åº¦æ¡ç»„ä»¶
- å°†åŸæœ‰çš„ RichProgressBar ç±»é‡æ„ä¸ºæ›´çµæ´»çš„ ProgressBarManager ç±»
- æ”¯æŒè®­ç»ƒå’ŒéªŒè¯é˜¶æ®µçš„å¤šä»»åŠ¡ç®¡ç†
- æ–°å¢éªŒè¯é˜¶æ®µæŒ‡æ ‡æ±‡æ€»è¡¨æ ¼æ˜¾ç¤ºåŠŸèƒ½
- æ”¹è¿›è¿›åº¦æ¡æ ·å¼å’Œäº¤äº’é€»è¾‘
3. é‡æ„æ¨¡å‹è®­ç»ƒç»„ä»¶,é‡æ„è®­ç»ƒå¾ªç¯å’ŒéªŒè¯é€»è¾‘ï¼Œæ”¹è¿›è¿›åº¦ç®¡ç†
- å°† RichProgressBar æ›¿æ¢ä¸ºæ›´é€šç”¨çš„ ProgressBarManager
- åœ¨è®­ç»ƒå’ŒéªŒè¯å¾ªç¯ä¸­æ·»åŠ è¿›åº¦æ¡æ”¯æŒ
- æ”¹è¿›æŒ‡æ ‡è®¡ç®—å’Œæ—¥å¿—è®°å½•ï¼Œæ·»åŠ å‡†ç¡®ç‡ç»Ÿè®¡
- ä¼˜åŒ–è®¾å¤‡ç®¡ç†å’Œåˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–é€»è¾‘
- è°ƒæ•´ AMP ä¸Šä¸‹æ–‡ç®¡ç†ä»¥æ”¯æŒä¸åŒè®¾å¤‡ç±»å‹
4. ä¼˜åŒ–RMSNormå±‚ï¼Œç§»é™¤ä¸å¿…è¦çš„bufferæ³¨å†Œå¹¶ç®€åŒ–epså¤„ç†
- ä¸å†å°†epsæ³¨å†Œä¸ºbufferï¼Œç›´æ¥ä½œä¸ºtensorå±æ€§ä½¿ç”¨ï¼Œç®€åŒ–ä»£ç ç»“æ„
5. Attentionå¤šå¤´å­æ³¨æ„åŠ›å±‚å…³äºåˆ†å¸ƒå¼è®­ç»ƒè§£å†³éšè—bug
- æ·»åŠ åˆ†å¸ƒå¼åˆå§‹åŒ–æ–¹æ³•å¹¶æ”¹è¿›å¹¶è¡Œé€šä¿¡é€»è¾‘
- æ·»åŠ  init_distributed_mode æ–¹æ³•ç”¨äºæ›´çµæ´»çš„åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–ï¼Œæ”¯æŒä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è¯»å–é…ç½®
- é‡æ„æ¨¡å‹å¹¶è¡Œé€šä¿¡ç»„çš„åˆå§‹åŒ–é€»è¾‘ï¼Œå¢åŠ å¯¹æœªè®¾ç½®åˆ†å¸ƒå¼å˜é‡çš„é”™è¯¯æ£€æŸ¥
6. datasetsæ•°æ®é›†åŠ è½½å™¨ä¿®å¤æ ‡ç­¾å¼ é‡ä¸­æ©ç æœªæ­£ç¡®åº”ç”¨çš„é—®é¢˜
- åœ¨PretrainDatasetä¸­ï¼Œå½“æ©ç ä¸ºFalseæ—¶ï¼Œå¯¹åº”çš„æ ‡ç­¾å¼ é‡å€¼åº”è®¾ä¸º-100ä»¥é¿å…å½±å“æŸå¤±è®¡ç®—ã€‚æ­¤ä¿®æ”¹ç¡®ä¿äº†æŸå¤±å‡½æ•°ä»…è®¡ç®—æœ‰æ•ˆæ ‡è®°çš„æŸå¤±


### TODO:
1. å…³é”®ç±»æ·»åŠ è°ƒè¯•å‡½æ•°
2. ç»Ÿä¸€æ³¨é‡Šé£æ ¼å¹¶å®Œå–„æ³¨é‡Š
3. ä¿®å¤æ¨¡å‹è®­ç»ƒè„šæœ¬ä¸­çš„é—®é¢˜ï¼š
- æ— æ³•æ˜¾ç¤ºä¸€ä¸ªepochä¸­çš„stepè¿›åº¦
- è‡ªåŠ¨æ„å»ºæˆ–è€…è·å–è™šæ‹Ÿç¯å¢ƒä¿¡æ¯
4. è°ƒè¯•æ¨¡å‹é¢„è®­ç»ƒè„šæœ¬

</details>

---

<details>
<summary>2025.7.18</summary>

### DONE:

1. è®­ç»ƒè„šæœ¬ä¿®å¤å¦‚ä¸‹é—®é¢˜ï¼š
- (1) æ€§èƒ½ä¸è®­ç»ƒæ•ˆç‡çš„é—®é¢˜
  - ä¿®å¤å­¦ä¹ ç‡è°ƒåº¦å™¨è®¡ç®—æ–¹å¼ä¸åˆç†ï¼Œget_lr ä¸­å°† total_iters = len(train_loader) * epochs * (restarts + 1)ï¼Œä½† step å®é™…æ˜¯ per-epoch å†…éƒ¨ stepã€‚è®­ç»ƒä¸­åº”ä½¿ç”¨å…¨å±€ stepï¼ˆglobal_step = epoch * steps_per_epoch + stepï¼‰ï¼Œå¦åˆ™è°ƒåº¦æ›²çº¿ä¸å¹³æ»‘ã€‚
  - ä¿®å¤æ²¡æœ‰æ¢¯åº¦ç´¯è®¡ä¸‹çš„æ­£ç¡®total_stepæ”¯æŒï¼Œåœ¨ get_lr() å’Œ total_iters ä¸­æœªè€ƒè™‘ accumulation_stepsï¼Œå¯¼è‡´è®­ç»ƒå®é™…æ›´æ–°æ­¥æ•°ä¸é¢„æœŸä¸ç¬¦ï¼Œè°ƒåº¦å¤±è¡¡ã€‚
  - å¯ç”¨cudnn.allow.tf32ï¼Œåˆå§‹è®­ç»ƒè„šæœ¬è®¾å®šäº† torch.backends.cuda.matmul.allow_tf32 = Trueï¼Œä½†æ²¡æœ‰è®¾ç½® torch.backends.cudnn.allow_tf32 = Trueã€‚è¿™ä¼šé”™å¤±ä¸€åŠä»¥ä¸Šçš„ TF32 ç®—å­åŠ é€Ÿæœºä¼šã€‚
  - torch.cuda.empty_cache() è°ƒç”¨é¢‘ç¹
- (2) åˆ†å¸ƒå¼è®­ç»ƒçš„é—®é¢˜ï¼Œæ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒ
  - ä¿®å¤ DDPæ—¥å¿—æœªåŠ  rank è¿‡æ»¤ çš„é—®é¢˜ï¼Œè™½ç„¶å¤§å¤šæ•°æ—¥å¿—éƒ½ç”¨ is_main_process() åšäº†åˆ¤æ–­ï¼Œä½†æŸäº›å¼‚å¸¸æ•è·å¦‚ except Exception æˆ– init_distributed() å†…ä»ä¼šå…¨ rank æ‰“å°ï¼Œå»ºè®®ç»Ÿä¸€å°è£…æ—¥å¿—å™¨ã€‚
  - ä¿®å¤ DDPæ¢å¤ä¸å®Œæ•´çš„é—®é¢˜ï¼Œåœ¨æ¢å¤æ¨¡å‹ checkpoint æ—¶ï¼Œstart_step æ²¡æœ‰ç»§ç»­ä½œä¸º global_step ä¼ å…¥ train_epoch()ï¼Œå¯¼è‡´æ–­ç‚¹æ¢å¤è®­ç»ƒæ—¶çš„ LRè°ƒåº¦ã€æ—¥å¿—æ­¥æ•°ã€SwanLab step ä¸å‡†ç¡®ã€‚
- (3) æ˜¾å­˜ç®¡ç†å’Œç¨³å®šæ€§é—®é¢˜
  - ä½¿ç”¨ model.zero_grad(set_to_none=True)ï¼Œæ˜¾å¼ç”¨ set_to_none=True æ›¿æ¢ zero_grad()ï¼Œå¯é‡Šæ”¾æ›´æ—©çš„ grad æ˜¾å­˜ï¼Œæå‡æ˜¾å­˜æ•ˆç‡ã€‚
  - ä¿®å¤ Gradient Checkpoint æœªæŒ‰å±‚ç²’åº¦é…ç½® çš„é—®é¢˜ï¼Œå¯ç”¨äº† model.gradient_checkpointing_enable()ï¼Œä½†è‹¥æ¨¡å‹ç»“æ„è¾ƒæ·±ï¼Œåº”é…åˆé€å±‚æ˜¾å¼è®¾ç½® checkpointing=True çš„ç­–ç•¥ï¼Œæ‰æœ‰å®é™…æ•ˆæœã€‚
- (4) é²æ£’æ€§ä¸å¼‚å¸¸æ¢å¤é—®é¢˜
  - è§£å†³ å¼‚å¸¸æ¢å¤æœªè®°å½• global_step é—®é¢˜ï¼Œæ£€æŸ¥ç‚¹ä¸­åªæœ‰ epoch, stepï¼Œæœªè®°å½• global_stepï¼Œå¯¼è‡´è°ƒåº¦å™¨ä¸æ—¥å¿—é‡å¯åä¸ä¸€è‡´ã€‚
  - è§£å†³ è®­ç»ƒå¼ˆåœºæ•è·æœªç»†åŒ– é—®é¢˜ï¼Œexcept Exception as e: ä¸­æ²¡æœ‰ä½¿ç”¨ traceback.print_exc()ï¼Œæ’æŸ¥é—®é¢˜å›°éš¾ã€‚
- (5)æ·»åŠ  Tokenizer.embedding_sync()ï¼Œæ£€æŸ¥ tokenizer ä¸ embedding å¤§å°æ˜¯å¦åŒæ­¥
- (6)å¢åŠ æ ‡ç­¾å¹³æ»‘æŸå¤±è®¡ç®—å‡½æ•°
- (7)æ·»åŠ CUDAå›¾ä¼˜åŒ–æ ‡è®°
2. RSMNormä¿®å¤ï¼š
- å°†epsä»tensoræ”¹ä¸ºfloatç±»å‹é¿å…é‡å¤è½¬æ¢ï¼Œå‡å°‘å†…å­˜
- å°†epsè½¬æ¢ä¸ºä¸rmsç›¸åŒçš„è®¾å¤‡ä»¥é¿å…è·¨è®¾å¤‡æ“ä½œ
- ç§»é™¤å†—ä½™çš„inv_rmsç±»å‹è½¬æ¢
- ç”¨ torch._dynamo.disable() è£…é¥°å™¨å…³é—­ RMSNorm çš„ forward ç¼–è¯‘ï¼Œé¿å… CUDA Graph å†…å­˜å¤ç”¨å†²çªã€‚
3. Attentionä¿®å¤ï¼š
- æ·»åŠ è‡ªåŠ¨è°ƒæ•´additive_maské•¿åº¦çš„åŠŸèƒ½
- æ–°å¢_adjust_additive_maskæ–¹æ³•ç”¨äºè‡ªåŠ¨å°†additive_maské•¿åº¦ä¸é”®å€¼åºåˆ—å¯¹é½ï¼Œè§£å†³KVç¼“å­˜é•¿åº¦ä¸åŒ¹é…é—®é¢˜
4. Modelä¿®å¤ï¼š
- ä¿®å¤æ³¨æ„åŠ›æ©ç å’Œè®¾å¤‡ç±»å‹ä¸ä¸€è‡´çš„é—®é¢˜å¹¶æ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä¿®å¤äº†æ³¨æ„åŠ›æ©ç ä¸hidden_statesè®¾å¤‡ç±»å‹ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå°†æ©ç è½¬æ¢ä¸ºç›¸åŒè®¾å¤‡å’Œç±»å‹ã€‚
- åŒæ—¶æ·»åŠ äº†æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½ä»¥åœ¨è®­ç»ƒæ—¶èŠ‚çœæ˜¾å­˜ï¼Œä½¿ç”¨éé‡å…¥æ–¹å¼æé«˜ç¨³å®šæ€§ã€‚
5. Loggeræ—¥å¿—è®°å½•å™¨å®Œå–„ï¼šä¸ºæ—¥å¿—æ„å»ºå‡½æ•°æ·»åŠ æ§åˆ¶å°æ—¥å¿—çº§åˆ«å‚æ•°
- æ·»åŠ  console_level å‚æ•°ä»¥å…è®¸è‡ªå®šä¹‰æ§åˆ¶å°è¾“å‡ºçš„æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¿æŒä¸º INFO çº§åˆ«
6. æ›´æ–°é¢„è®­ç»ƒé…ç½®å‚æ•°å’Œæ³¨é‡Šæ ¼å¼
- å°† eval_max_steps æ”¹ä¸º eval_interval ä»¥æ›´å‡†ç¡®æè¿°åŠŸèƒ½
- æ›´æ–°ç‰¹æ®Šæ ‡è®°æ ¼å¼ä¸º <|SBOS|> å’Œ <|SEOS|>
- è°ƒæ•´ vocab_size å’Œå¹¶è¡Œé…ç½®å‚æ•°
- ä¸ºæ—¥å¿—é…ç½®æ·»åŠ æ³¨é‡Šè¯´æ˜

### TODO:
1. ä¼˜åŒ–è®­ç»ƒè„šæœ¬ï¼š
- æ·»åŠ  æ—©åœæ£€æŸ¥
- æ·»åŠ  EMAï¼Œå¹³æ»‘æ”¶æ•›è¿‡ç¨‹ï¼Œæå‡ç²¾åº¦
- æ„å»º metrics.pyï¼Œç»Ÿä¸€ç®¡ç†è®­ç»ƒæŒ‡æ ‡
- ç²¾åº¦ä¸è®­ç»ƒæ”¶æ•›çš„é—®é¢˜
  - ä½¿ç”¨ Label Smoothingï¼Œé¿å…è¿‡æ‹Ÿåˆå’Œæå‡æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®æ”¯æŒå‚æ•°é…ç½®ã€‚
  - å¼•å…¥ Prompt Maskã€Position Shiftç­‰è®­ç»ƒæŠ€å·§
  - PPLè®¡ç®—å¯èƒ½ä¸å‡†ç¡®ï¼Œå½“å‰ evaluate() ä¸­ ppl = exp(avg_loss)ï¼Œä½† avg_loss æ˜¯å¹³å‡ token lossï¼Œè‹¥ loss mask æœªæ­£ç¡®å¤„ç†ï¼Œå¯èƒ½é€ æˆè¿‡å¤§åå·®ã€‚
- ä½¿ç”¨FSDPï¼Œå½“å‰æœ€å¤§æ”¯æŒ DP + DDPï¼Œæœªä½¿ç”¨ torch.distributed.fsdpï¼Œå¯¹äºå‡ åäº¿å‚æ•°ä»¥ä¸Šçš„å¤§æ¨¡å‹åœ¨å¤šèŠ‚ç‚¹ä¸‹ä¸å¤Ÿé«˜æ•ˆã€‚
2. å®Œå–„æ„å¤–ç»ˆæ­¢å¤„ç†
- ä¿®å¤æ„å¤–ç»ˆæ­¢åçˆ†å‡ºå¤§é‡é”™è¯¯çš„é—®é¢˜ï¼Œå½“è®­ç»ƒæ„å¤–ç»ˆæ­¢æ—¶ï¼Œä¼šè§¦å‘å¼‚å¸¸æ•è·ï¼Œä½†å¼‚å¸¸æ•è·åæœªæ¸…ç†ç¯å¢ƒï¼Œå¯¼è‡´å¤§é‡é”™è¯¯æ—¥å¿—è¾“å‡ºã€‚
```
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 315, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
RuntimeError: CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2025-07-18 13:12:28] [WARNING] [ByteLogger] ğŸ’€ æ”¶åˆ° SIGINTï¼Œå†™å…¥å®Œæ•´æ£€æŸ¥ç‚¹ â€¦
[2025-07-18 13:12:28] [ERROR] [ByteLogger] è®­ç»ƒå¼‚å¸¸: DataLoader worker (pid 2124) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
[2025-07-18 13:12:28] [INFO] [ByteLogger] ğŸ’€ å¼‚å¸¸é€€å‡ºï¼Œæ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹â€¦
Traceback (most recent call last):
  File "/workspace/model_pretrain.py", line 685, in <module>
    train(args, logger)
  File "/workspace/model_pretrain.py", line 636, in train
    raise e
  File "/workspace/model_pretrain.py", line 597, in train
    train_epoch(
  File "/workspace/model_pretrain.py", line 397, in train_epoch
    scaler.scale(loss).backward()
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 202, in _handler
    self.ckpt_mgr.save_sync(self.model, self.optimizer, self.scaler,
  File "/workspace/utils/checkpoint.py", line 77, in save_sync
    state = self._collect_state(model, optimizer, scaler,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/utils/checkpoint.py", line 107, in _collect_state
    "scaler_state": scaler.state_dict() if scaler else None,
                    ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 622, in state_dict
    "scale": self.get_scale(),
             ^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 550, in get_scale
    else cast(float, scale.item())
                     ^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2124) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
```
3. è§£å†³ æœªæ£€æµ‹æ¢¯åº¦å¼‚å¸¸ï¼ˆNaNï¼‰é—®é¢˜ï¼Œè‹¥ loss = NaNã€grad = infï¼Œåº”ç«‹å³ä¸­æ­¢è®­ç»ƒä¿å­˜ checkpointï¼Œé¿å…æµªè´¹èµ„æºã€‚

### DEBUG
1. æ‰¾å‡ºtorch.utils.checkpointé—®é¢˜æ ¹æºå¹¶è¿›è¡Œä¿®å¤
```
/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
```

</details>

---

<details>
<summary>2025.7.19</summary>

### DONE
1. ç»™æ¯ä¸ªç»„ä»¶æ·»åŠ ä¸“å±æ ‡å¿—
2. å®Œå–„requirements.txt
3. ä¼˜åŒ–è®­ç»ƒè„šæœ¬ï¼Œæ·»åŠ å¼‚å¸¸è‡ªåŠ¨å¤„ç†
4. æµ‹è¯•è®­ç»ƒè„šæœ¬é€šè¿‡
- è§£å†³å­˜åœ¨äºå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ä¸­çš„torch.utils.checkpointé…ç½®use_reentrant=Falseé—®é¢˜
- ä¿®å¤æ„å¤–ç»ˆæ­¢åçˆ†å‡ºå¤§é‡é”™è¯¯çš„é—®é¢˜ï¼Œå½“è®­ç»ƒæ„å¤–ç»ˆæ­¢æ—¶ï¼Œä¼šè§¦å‘å¼‚å¸¸æ•è·ï¼Œä½†å¼‚å¸¸æ•è·åæœªæ¸…ç†ç¯å¢ƒï¼Œå¯¼è‡´å¤§é‡é”™è¯¯æ—¥å¿—è¾“å‡ºã€‚
5. æ–°å¢å¤šé˜¶æ®µè®­ç»ƒçš„tokenizerå®ç°
- æ”¯æŒæ•°å­¦è¡¨è¾¾å¼ã€ä»£ç å—å’ŒXMLç»“æ„çš„ç‰¹æ®Šå¤„ç†
- å¤šé˜¶æ®µæ¸è¿›å¼è¯æ±‡è¡¨è®­ç»ƒ
- å†…å­˜ä¼˜åŒ–çš„æ‰¹å¤„ç†ç”Ÿæˆå™¨
- å®Œæ•´çš„é…ç½®æ–‡ä»¶å’Œç‰¹æ®Štokenæ”¯æŒ
- å†…ç½®è¯„ä¼°åŠŸèƒ½éªŒè¯tokenizeræ•ˆæœ
6. æ·»åŠ ç¯å¢ƒè®¾ç½®è„šæœ¬å’Œå®‰è£…æ–‡æ¡£
- æ·»åŠ  Windows å’Œ Unix çš„ç¯å¢ƒè®¾ç½®è„šæœ¬ï¼Œç”¨äºè‡ªåŠ¨åŒ–å®‰è£…å’Œé…ç½®å¼€å‘ç¯å¢ƒ
- æ·»åŠ  INSTALL.md å’Œ CONTRIBUTING.md æ–‡æ¡£ï¼Œæä¾›è¯¦ç»†çš„å®‰è£…å’Œè´¡çŒ®æŒ‡å—
- æ·»åŠ  setup.py ç”¨äºç®¡ç†é¡¹ç›®ä¾èµ–å’Œå®‰è£…é…ç½®
- ç¯å¢ƒè®¾ç½®è„šæœ¬æ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
  - æ£€æŸ¥ç³»ç»Ÿä¾èµ–
  - åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
  - å®‰è£…é¡¹ç›®ä¾èµ–
  - éªŒè¯å®‰è£…
  - æ”¯æŒå¼€å‘ç¯å¢ƒå’Œ CUDA é€‰é¡¹
7. ä¼˜åŒ–ä½ç½®ç¼–ç 
- æ·»åŠ max_seq_lenå‚æ•°ï¼Œæ”¯æŒé¢„è®¡ç®—å¹¶ç¼“å­˜æœ€å¤§é•¿åº¦çš„ä½ç½®ç¼–ç 
- ä¼˜åŒ–_get_cos_sin_scaleæ–¹æ³•ï¼Œæ”¯æŒé€šè¿‡offsetå‚æ•°è·å–æŒ‡å®šçª—å£çš„ç¼–ç åˆ‡ç‰‡
8. ä¼˜åŒ–Attentionï¼Œæ·»åŠ repeat_kvæ–¹æ³•å®ç°Grouped-Query Attentionç”¨äºé‡å¤Key/Valueå¼ é‡ä»¥åŒ¹é…Queryçš„å¤´æ•°
9. é‡æ„é—¨æ§å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å—å¹¶æ·»åŠ æ®‹å·®è¿æ¥
- åˆå¹¶w1å’Œw3ä¸ºå…±äº«å‚æ•°çš„w13çº¿æ€§å±‚
- ä½¿ç”¨GEGLUé—¨æ§ç»“æ„æ›¿ä»£åŸæœ‰å®ç°
- æ·»åŠ ByteRMSNormå½’ä¸€åŒ–å±‚
- å¼•å…¥æ®‹å·®è¿æ¥æå‡æ¢¯åº¦æµåŠ¨
10. æ„å»ºåŸºç¡€çš„MoEå±‚ï¼Œä½†æ˜¯ç»“æ„é€»è¾‘è¿˜ä¸å®Œå–„æ— æ³•åº”ç”¨ï¼Œéœ€è¦ä¼˜åŒ–
11. æ„å»ºåŸºç¡€çš„MeMoryè®°å¿†æœºåˆ¶ï¼Œæ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
- update(layer_idx, new_hidden)
  - æ›´æ–°æŒ‡å®šå±‚çš„ memoryã€‚
  - æ”¯æŒ detach()ï¼Œé¿å…æ¢¯åº¦å›ä¼ æ±¡æŸ“ï¼›
  - è‹¥åŸæœ‰è®°å¿†ä¸ä¸ºç©ºï¼Œåˆ™æ‹¼æ¥å½“å‰ new_hidden å¹¶æˆªæ–­è‡³ mem_lenï¼›
  - è‡ªåŠ¨è¿›è¡Œè®¾å¤‡åŒ¹é…ï¼ˆå¦‚åˆ‡ GPUï¼‰ã€‚
- update_all(new_hiddens)
  - æ‰¹é‡æ›´æ–°æ‰€æœ‰å±‚çš„è®°å¿†ï¼ˆä¾‹å¦‚æ¯æ¬¡ forward åæ›´æ–°ï¼‰ã€‚
  - è¦æ±‚ä¼ å…¥çš„åˆ—è¡¨é•¿åº¦ç­‰äº n_layersï¼›
  - å†…éƒ¨è°ƒç”¨ update å‡½æ•°ã€‚
- get(layer_idx),è¿”å›æŒ‡å®šå±‚çš„è®°å¿†ï¼Œç”¨äºå½“å‰æ¨ç†æ‹¼æ¥ã€‚
- clear(),æ¸…é™¤æ‰€æœ‰å±‚çš„è®°å¿†ï¼ˆå¯ç”¨äºæ¯æ®µä¸Šä¸‹æ–‡/ä»»åŠ¡ä¹‹é—´æ¸…é›¶ï¼‰ã€‚
- to(device),å°†å½“å‰ç¼“å­˜è¿ç§»è‡³æŒ‡å®šè®¾å¤‡ï¼ˆé€šå¸¸åœ¨æ¨¡å‹è¿ç§»æ—¶åŒæ­¥è¿ç§»ï¼‰ã€‚
- memory_size(),è¿”å›å„å±‚å½“å‰ä¿ç•™çš„è®°å¿†é•¿åº¦ï¼ˆtoken æ•°ï¼‰ï¼Œæœ‰åŠ©äºè°ƒè¯•ã€‚
- __repr__(),æ¸…æ™°å±•ç¤ºå½“å‰å†…å­˜ä½¿ç”¨çŠ¶æ€ï¼Œæ–¹ä¾¿æ—¥å¿—è¾“å‡ºã€‚

### TODO
1. MoEå±‚ä¼˜åŒ–ï¼š
- åˆ†å¸ƒå¼MoEï¼Œæ¥å…¥ DeepSpeed-MoE æˆ– FSDP çš„ MoE å®ç°æ¨¡å—
- è·¯ç”±å™¨é‡å‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨ GShard-style noisy-topk æˆ– Gumbel Softmax æé«˜æ¢ç´¢èƒ½åŠ›
- é«˜æ•ˆä¸“å®¶å…±äº«ï¼Œå¤šä¸ª MoE å±‚å¤ç”¨å…±äº«ä¸“å®¶æ± ï¼ˆå¦‚ M6ã€GLaMï¼‰ï¼Œå¯ç”¨ä¸“å®¶æ± ç»Ÿä¸€è°ƒåº¦
- æé«˜ token åˆ†æ´¾å’Œæ‰§è¡Œæ•ˆç‡	,æ›¿ä»£é€ä¸“å®¶æ”¶é›†æ–¹å¼ï¼Œè½¬å‘åŸºäºç¨€ç–è¡¨ç¤ºçš„å¹¶è¡Œå¤„ç†
- å‡å°‘å†…å­˜æµªè´¹,ç”¨ç¨€ç–å¼ é‡æˆ–ç¨€ç–è·¯ç”±ç»“æ„æ›¿ä»£ç¨ å¯† dispatch_mask
- å¢å¼ºé²æ£’æ€§,å¯¹ token æº¢å‡ºéƒ¨åˆ†å¼•å…¥â€œæ®‹å·®è·¯å¾„â€æˆ–å†è·¯ç”±æœºåˆ¶
- æ”¹è¿›è´Ÿè½½å‡è¡¡ Loss,å¼•å…¥ softmax entropyã€expected load KL loss ç­‰æ›´åˆç†çš„çº¦æŸ
- å¢å¼ºæ¨¡å—æ¸…æ™°åº¦å’Œå¯ç»´æŠ¤æ€§,æ‹†è§£åŠŸèƒ½å‡½æ•°ã€æ¸…æ™°æ³¨é‡Šã€å‘½åæ ‡å‡†åŒ–
- åŠ¨æ€ä¸“å®¶æ¿€æ´»æ•° k,è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ kï¼Œæ›´æ™ºèƒ½åŒ–è°ƒåº¦
- æ··åˆå®¹é‡è°ƒåº¦ç­–ç•¥,é«˜ä¼˜å…ˆ token åˆ†é…æ›´å¤šå®¹é‡
- æ¨¡å—åŒ–è§£è€¦,åˆ†ç¦» Gate, Router, Expert æ›´ä¾¿äºæ›¿æ¢
2. å°è¯•åµŒå…¥MoEå±‚ä¼˜åŒ–æ¨¡å‹
3. MeMoryæœºåˆ¶ä¼˜åŒ–ï¼š
- æ”¯æŒæ»‘åŠ¨çª—å£æ›´æ–°ï¼Œè®¾å®šçª—å£æ»‘åŠ¨çš„æ¯”ä¾‹ï¼Œå®ç°æ›´åŠ å¹³æ»‘çš„è®°å¿†æ›´æ›¿
- æ··åˆKVCacheï¼Œå°†KVç¼“å­˜ä¸hidden memoryç»Ÿä¸€ç®¡ç†ï¼Œä¸ºAttentionæœåŠ¡
- æŒä¹…åŒ–ä¿å­˜ï¼Œæä¾›save()ã€load()ï¼Œæ”¯æŒä¸­æ–­æ¢å¤

</details>

---

<details>
<summary>2025.7.20</summary>

### DONE
1. æ„å»ºMoERouterè·¯ç”±ç³»ç»Ÿ
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—¨æ§æœºåˆ¶ï¼ˆContext-Aware Gatingï¼‰ -> å¯æ”¯æŒé•¿åºåˆ—ã€å¤šè¯­è¨€ã€å¤§æ¨¡å‹è°ƒåº¦åœºæ™¯ï¼Œå¯¹ç¨€ç–è¡¨ç¤ºå»ºæ¨¡èƒ½åŠ›å¼ºã€‚
  - å¼•å…¥ä½ç½®ç¼–ç  + å†å²çŠ¶æ€ + å½“å‰è¯­ä¹‰ç‰¹å¾å½¢æˆè”åˆä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ˆcontext_featï¼‰ã€‚
  - é—¨æ§ç½‘ç»œ (gate_mlp) è¾“å…¥ä¸º [x, context_feat]ï¼Œå¯é€‚åº”å½“å‰tokenä¸ä¸Šä¸‹æ–‡çš„å¾®å¦™å˜åŒ–ã€‚
  - RMSNorm ä¸ dropout æå‡æ¨¡å‹ç¨³å®šæ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚
- æ¸©åº¦åŠ¨æ€è°ƒæ•´ä¸ä¸“å®¶ä¼˜å…ˆçº§ -> å®ç°å†·çƒ­ä¸“å®¶åŠ¨æ€æ¿€æ´»ã€æ”¶æ•›æ›´å¿«ã€è·¯ç”±æ›´ç¨³å®šã€token åˆ†å¸ƒæ›´å‡è¡¡ã€‚
  - ä½¿ç”¨ self.temperature æ ¹æ®ä¸“å®¶è´Ÿè½½å·®å¼‚åŠ¨æ€è°ƒæ•´ softmax æ¸©åº¦ï¼Œæ§åˆ¶ä¸“å®¶é€‰æ‹©çš„åˆ†å¸ƒç†µã€‚
  - ä¸“å®¶ä¼˜å…ˆçº§é€šè¿‡ self.expert_priority å’Œ self.expert_cold_priority åŠ¨æ€æ›´æ–°ï¼Œå¯¹ä¸æ´»è·ƒä¸“å®¶æä¾›å†·å¯åŠ¨æ”¯æŒã€‚
- åŠ¨æ€å®¹é‡è°ƒæ§æœºåˆ¶ -> è§£å†³ token åˆ†å¸ƒæä¸å‡è¡¡æ—¶çš„å®¹é‡ç“¶é¢ˆé—®é¢˜ï¼Œæ˜¯å¤§è§„æ¨¡ MoE æ¨¡å‹éƒ¨ç½²çš„å…³é”®ç»„ä»¶ã€‚
  - ä½¿ç”¨ä¸“å®¶åˆ©ç”¨ç‡ (expert_utilization) è°ƒæ•´æ¯è½® capacityï¼Œé˜²æ­¢éƒ¨åˆ†ä¸“å®¶é•¿æœŸé¥±å’Œæˆ–é—²ç½®ã€‚
  - å¯è®¾ç½®æœ€å¤§/æœ€å°å®¹é‡ä¸Šä¸‹é™ï¼Œå…¼é¡¾å¼¹æ€§ä¸ç¨³å®šæ€§ã€‚
- é«˜æ•ˆçš„å‘é‡åŒ–ä¸“å®¶åˆ†å‘è°ƒåº¦ -> å®Œå…¨å‘é‡åŒ–å®ç°ï¼Œæ€§èƒ½ä¼˜äº for-loop è°ƒåº¦ï¼›é€‚ç”¨äº TP/SP å¹¶è¡Œç¯å¢ƒä¸‹çš„è°ƒåº¦è®¡åˆ’ç”Ÿæˆã€‚
  - _vectorized_dispatch åŸºäº torch_scatter + top-k åˆ†å‘æƒé‡æ„å»ºä¸“å®¶-æ ·æœ¬æ˜ å°„è¡¨ã€‚
  - token ä¸»å¯¼åˆ†å‘ï¼ˆç”± gate è¾“å‡ºæ§åˆ¶ï¼‰ï¼Œä¸“å®¶ä¸»å¯¼ç­›é€‰ï¼ˆåŸºäºåˆ†é…ä¼˜å…ˆçº§ä¸å®¹é‡é™åˆ¶ï¼‰ã€‚
  - åˆ†é…é€»è¾‘æ˜ç¡®ï¼šå¯ç”¨ä¸“å®¶ä¼˜å…ˆ + æƒé‡è¶Šå¤§ä¼˜å…ˆã€‚
- æº¢å‡ºå¤„ç†æœºåˆ¶ï¼ˆOverflow Handlingï¼‰ -> é¿å… token ä¸¢å¤±ï¼Œä¿éšœæ¨¡å‹é²æ£’æ€§ï¼Œæé«˜æ¨¡å‹è®­ç»ƒç¨³å®šæ€§ã€‚
  - æº¢å‡ºtoken é‡‡ç”¨ å¤‡é€‰ä¸“å®¶æœºåˆ¶ï¼ˆé top-k ä¸­åˆ†æ•°æœ€é«˜çš„ï¼‰ã€å†·å¯åŠ¨ä¸“å®¶è·¯ç”±ã€å†å²ä¸“å®¶ç²˜æ€§ fallback å’Œæœ€ç»ˆ éšæœº fallbackã€‚
  - fallback å€¾å‘äºé€‰æ‹©è´Ÿè½½è½» + é•¿æœŸæœªæ´»è·ƒ + å­¦ä¹ æƒé‡é«˜çš„ä¸“å®¶ã€‚
- å¤šç›®æ ‡è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆAux Lossï¼‰ -> ä¿è¯è·¯ç”±ç¨³å®šä¸ä¸“å®¶å‡åŒ€è´Ÿè½½ï¼Œæå‡è®­ç»ƒæ•ˆç‡å’Œä¸“å®¶æ³›åŒ–èƒ½åŠ›ã€‚
  - é€šè¿‡ KLã€MSE å’Œä¸“å®¶ load variance å®ç°è·¯ç”±ç†µçº¦æŸï¼Œæå‡åˆ†å¸ƒå‡åŒ€æ€§ã€‚
  - å¼•å…¥ token entropy æƒé‡æœºåˆ¶ï¼Œé«˜ä¸ç¡®å®šæ€§ token è¢«æ›´ç²¾ç»†å¤„ç†ã€‚
- ä¸“å®¶çŠ¶æ€æ›´æ–°ä¸è‡ªé€‚åº”å­¦ä¹  -> å®ç°ä¸“å®¶ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œæ”¯æŒåœ¨çº¿ä¸“å®¶å‰”é™¤/æ›¿æ¢ã€å†·çƒ­ä¸“å®¶åˆ‡æ¢ç­‰æœºåˆ¶ã€‚
  - ç»´æŠ¤ä¸“å®¶åˆ†é…çŠ¶æ€ï¼ˆexpert_load, utilization, priorityï¼‰å¹¶é€šè¿‡ EMA æ›´æ–°ã€‚
  - ç»Ÿè®¡ä¿¡æ¯æ”¯æŒåŠ¨æ€è°ƒåº¦å†³ç­–ä¸è®­ç»ƒæŒ‡æ ‡ç›‘æ§ã€‚
2. ä¸€å¥MoERouteré‡æ„MoEå±‚
- ä¸“å®¶å¹¶è¡Œï¼ˆExpert Parallelismï¼‰æ”¯æŒï¼šè®¾è®¡ä¸­æ˜ç¡®åŒºåˆ†äº†num_expertsï¼ˆå…¨å±€ä¸“å®¶æ•°ï¼‰å’Œnum_local_expertsï¼ˆå½“å‰è®¾å¤‡ä¸Šçš„ä¸“å®¶æ•°ï¼‰ï¼Œå¹¶ä¸”é€šè¿‡diståˆ†å¸ƒå¼é€šä¿¡ç®¡ç†ä¸“å®¶å¹¶è¡Œã€‚
- åŠ¨æ€å®¹é‡ç®¡ç†ï¼šé€šè¿‡router_configä¸­max_capacityå‚æ•°ä»¥åŠç¼“å†²åŒºexpert_inputså’Œexpert_outputsçš„åŠ¨æ€åˆ†é…ï¼Œå¯¹ä¸“å®¶è¾“å…¥å®¹é‡çš„æ§åˆ¶ï¼Œé¿å…äº†é™æ€å›ºå®šå®¹é‡å¯¼è‡´çš„å†…å­˜æµªè´¹ã€‚
- å®¹é”™è·¯ç”±æœºåˆ¶ï¼šä½¿ç”¨fallback_expertæ¥å¤„ç†â€œæº¢å‡ºtokenâ€ï¼Œé˜²æ­¢å› ä¸“å®¶å®¹é‡é™åˆ¶å¯¼è‡´tokenä¸¢å¤±ï¼Œå¢å¼ºé²æ£’æ€§ã€‚åŒæ—¶æä¾›dropoutæœºåˆ¶ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚
- é›¶æµªè´¹å†…å­˜ç®¡ç†ï¼šé¢„æ³¨å†Œç¼“å†²åŒºé¿å…åŠ¨æ€å†…å­˜åˆ†é…ï¼Œå‡å°‘æ˜¾å­˜ç¢ç‰‡å’Œé¢‘ç¹åˆ†é…å¼€é”€ï¼Œåˆ©äºé«˜æ•ˆè®­ç»ƒã€‚
- æ„å»ºä¸°å¯Œæ€§èƒ½ç›‘æ§æŒ‡æ ‡ï¼šåŒ…æ‹¬ä¸“å®¶åˆ©ç”¨ç‡ã€è´Ÿè½½ä¸å‡è¡¡åº¦ã€æº¢å‡ºç‡ç­‰ï¼Œæ–¹ä¾¿å®æ—¶ç›‘æ§MoEå±‚è¿è¡ŒçŠ¶å†µã€‚
- åŠ¨æ€ä¸“å®¶è´Ÿè½½å‡è¡¡ï¼ˆsplit/mergeï¼‰ç­–ç•¥ï¼šè®¾è®¡äº†æ ¹æ®åˆ©ç”¨ç‡åŠ¨æ€åˆ†è£‚è¿‡è½½ä¸“å®¶ã€åˆå¹¶ä½è½½ä¸“å®¶çš„æœºåˆ¶ï¼Œæœ‰åˆ©äºè®­ç»ƒæœŸé—´ä¸“å®¶èµ„æºè‡ªé€‚åº”è°ƒæ•´ï¼Œæå‡æ¨¡å‹æ•ˆç‡ã€‚
- è®¾è®¡åˆç†ä¸“å®¶æ¨¡å—ï¼šä¸“å®¶å†…éƒ¨ä½¿ç”¨å¸¦é—¨æ§çš„GLUç»“æ„ï¼ˆæ¿€æ´»+é—¨æ§ä¹˜ç§¯ï¼‰åŠå½’ä¸€åŒ–ï¼Œç¬¦åˆå½“å‰MoEä¸“å®¶çš„ä¸»æµè®¾è®¡ï¼Œè®¡ç®—æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›å…¼é¡¾ã€‚
3. Memoryæœºåˆ¶ä¼˜åŒ–ï¼š
- åˆ†å±‚è®°å¿†æ§åˆ¶ï¼šåº•å±‚å¯ä¿ç•™æ›´é•¿å†å²ï¼Œé«˜å±‚å¯å‡å°‘è®¡ç®—
- æ™ºèƒ½batchå¤„ç†ï¼šæ”¯æŒbatchå°ºå¯¸å˜åŒ–æ—¶çš„è‡ªåŠ¨å¹¿æ’­/è£å‰ª
- è®°å¿†èåˆï¼šæ–°æ—§è®°å¿†åŠ æƒèåˆä¿ç•™å…³é”®ä¿¡æ¯
- ç­–ç•¥é…ç½®ï¼šæä¾›strict/select/repeatä¸‰ç§å°ºå¯¸é€‚é…ç­–ç•¥
4. æ„å»ºç›¸å…³æµ‹è¯•ä»£ç å¹¶ä¿®å¤bug
- æ„å»ºMemoryæµ‹è¯•ä»£ç å¹¶æµ‹è¯•é€šè¿‡ï¼Œä¿®å¤äº†ç›¸å…³BUG
- æ„å»ºMoERouteræµ‹è¯•ä»£ç å¹¶æµ‹è¯•é€šè¿‡ï¼Œä¿®å¤äº†ç›¸å…³BUG

### TODO
1. MoEå±‚ä¼˜åŒ–ï¼š
- æ„å»ºMoEå±‚é¢„çƒ­æœºåˆ¶
2. è®­ç»ƒè„šæœ¬ä¸­åŠ å…¥
- update_cold_priority()ï¼Œç”¨äºæ›´æ–°å†·é—¨ä¸“å®¶ä¼˜å…ˆçº§
```python
@torch.no_grad()
def update_cold_priority(self):
    # åˆ©ç”¨å½“å‰ä¸“å®¶åˆ©ç”¨ç‡ï¼Œä½åˆ©ç”¨ç‡ä¸“å®¶å†·å¯åŠ¨ä¼˜å…ˆçº§æå‡
    utilization = self.expert_utilization.clamp(0, 1)
    cold_priority = 1.0 + (1.0 - utilization)  # ä½åˆ©ç”¨ç‡åŠ æˆèŒƒå›´[1,2]
    self.expert_cold_priority.copy_(cold_priority)
```
3. æ„å»ºtest_MoERouter.pyæµ‹è¯•ä»£ç ï¼Œå¹¶ä¿®å¤ç›¸å…³BUG
4. æ„å»ºtest_MoE.pyæµ‹è¯•ä»£ç ï¼Œå¹¶ä¿®å¤ç›¸å…³BUG
5. å°è¯•åµŒå…¥MoEå±‚ä¼˜åŒ–æ¨¡å‹
6. MoERouterè¿˜å­˜åœ¨å¦‚ä¸‹é—®é¢˜å¾…è§£å†³ï¼š
- é—®é¢˜ 1ï¼šä¸“å®¶å†·å¯åŠ¨æƒé‡çš„åˆå§‹åŒ–è¿‡äºç»Ÿä¸€ï¼Œself.expert_cold_priority é»˜è®¤å…¨ä¸º 1ï¼Œç¼ºä¹åŸºäºå†å²ç»Ÿè®¡çš„åˆå§‹åŒ–ç­–ç•¥ã€‚
  - å»ºè®®ï¼šå¯å¼•å…¥å†·å¯åŠ¨å†å²æ—¶é—´æˆ³æˆ–å†·å´æ—¶é—´çª—å£ï¼ŒåŠ¨æ€æ›´æ–° [1.0, 2.0] åˆ†å¸ƒã€‚
- é—®é¢˜ 2ï¼šfallback è¿‡ç¨‹å­˜åœ¨å†²çªé£é™©ï¼Œåœ¨ _handle_overflow_fallback ä¸­ï¼Œå¤šä¸ª token å¯èƒ½äº‰ç”¨åŒä¸€ä¸“å®¶ï¼Œå°¤å…¶åœ¨ batch å¤§æ—¶æœªå®Œå…¨å¹¶å‘å®‰å…¨ã€‚
  - å»ºè®®ï¼šå¯å¼•å…¥ dispatch_bitmap æˆ–åˆ©ç”¨ index_put_ + åŸå­è®¡æ•°æ–¹æ¡ˆæ›´å®‰å…¨æ›´æ–°ã€‚
- é—®é¢˜ 4ï¼šä¸“å®¶åˆ†é…ç­–ç•¥ç¼ºä¹åˆ†å¸ƒå¼æ„ŸçŸ¥ï¼Œå½“å‰æ‰€æœ‰ä¸“å®¶è°ƒåº¦é€»è¾‘åŸºäºå•èŠ‚ç‚¹ä¿¡æ¯ï¼Œä¸è€ƒè™‘è·¨ GPU / TP experts çš„åˆ†å¸ƒã€‚
  - å»ºè®®ï¼šå¼•å…¥è·¨è®¾å¤‡ expert_rankï¼Œæ„å»º local_vs_global_expert_maskï¼Œå®ç°è·¨èŠ‚ç‚¹è´Ÿè½½å‡è¡¡ã€‚
- é—®é¢˜ 5ï¼šè°ƒåº¦ä¿¡æ¯æœªæ˜¾å¼æ”¯æŒå¤šç²’åº¦ token åˆ†é…ï¼Œå½“å‰æ‰€æœ‰åˆ†é…åŸºäº flat tokenï¼Œå¦‚æœè¾“å…¥æœ‰ padding/attention maskï¼Œåˆ™å¯èƒ½é”™è¯¯åˆ†é…ã€‚
  - å»ºè®®ï¼šå¼•å…¥ token_mask æœºåˆ¶ï¼Œç²¾ç¡®æ§åˆ¶æœ‰æ•ˆ tokenã€‚
- é—®é¢˜ 6ï¼šè´Ÿè½½ç»Ÿè®¡çŠ¶æ€æœªæŒä¹…åŒ–/å­˜ç›˜ï¼Œexpert_priority, utilization ç­‰çŠ¶æ€å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å˜åŒ–å¤§ï¼Œä½†æœªæŒä¹…åŒ–æˆ–å¤ç”¨ã€‚
  - å»ºè®®ï¼šåŠ æŒä¹…åŒ–æ¥å£ï¼ˆå¦‚save_state_dict / load_state_dictï¼‰ï¼Œæ”¯æŒçƒ­é‡å¯ã€‚

</details>

---

<detaills>
<summary>2025.7.21</summary>

### DONE
1. å°†è®¸å¯è¯ä»MITæ›´æ”¹ä¸ºCC BY-NC 4.0,æ›´æ–°è®¸å¯è¯æ–‡ä»¶ä»¥ä½¿ç”¨CC BY-NC 4.0åè®®ï¼Œæ·»åŠ äº†éå•†ä¸šä½¿ç”¨é™åˆ¶å’Œç½²åè¦æ±‚
2. ä¼˜åŒ–MoERouterè·¯ç”±ç³»ç»Ÿï¼Œ å¢å¼ºè·¯ç”±å™¨çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œå®¹é”™æœºåˆ¶
- æ–°å¢max_positionså‚æ•°æ§åˆ¶ä½ç½®ç¼–ç èŒƒå›´
- é‡æ„é—¨æ§ç½‘ç»œè¾“å…¥ç»´åº¦ï¼ŒåŠ å…¥ä½ç½®å’Œå†å²ä¿¡æ¯
- æ”¹è¿›æº¢å‡ºå¤„ç†é€»è¾‘ï¼Œå¢åŠ åŠ¨æ€å¤‡é€‰ä¸“å®¶é€‰æ‹©å’Œç²˜æ€§å›é€€æœºåˆ¶
- ä¼˜åŒ–ä¸“å®¶è´Ÿè½½ç»Ÿè®¡ç²¾åº¦ï¼Œä½¿ç”¨float32ç±»å‹
- ä¿®å¤æ½œåœ¨çš„ç©ºæº¢å‡ºå¤„ç†è¾¹ç•Œæƒ…å†µ
3. é‡æ„MoELayerï¼Œä½¿ç”¨ByteMoERouteræ„é€ åŸºç¡€ByteMoELayerï¼Œæš‚ä¸æ”¯æŒDDP

### TODO
1. ByteMoELayerå‡çº§ä¼˜åŒ–ï¼ŒæŒ‰ç…§DeepSeed-MoEé£æ ¼æ„é€ åˆ†å¸ƒå¼ByteMoELayerï¼Œä»å•ä¸€çš„localæ˜¯å…ˆåˆ°å¤šå¡åˆ†å¸ƒå¼
2. ä½¿ç”¨pytestæ„å»ºByteMoELayerï¼Œå¹¶ä¿®å¤ç›¸å…³BUG

</details>

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ–°åŠŸèƒ½å»ºè®®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºæ‚¨çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. æ‰“å¼€ä¸€ä¸ª Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸŒŸ è‡´è°¢

- æ„Ÿè°¢æ‰€æœ‰ä¸º Transformer æ¶æ„å‘å±•åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…
- æ„Ÿè°¢ HuggingFace å›¢é˜Ÿæä¾›çš„å‡ºè‰²å·¥å…·å’Œç”Ÿæ€ç³»ç»Ÿ
- æ„Ÿè°¢æ‰€æœ‰é¡¹ç›®è´¡çŒ®è€…

---

<div align="center">
  <sub>æŠŠæ¯ä¸ªå­—èŠ‚éƒ½ç‚¹äº®æˆä¸€ç›ç¯ï¼Œç…§è§å¤ä»ŠåŒæœ›çš„å¤œç©ºã€‚</sub>
</div>