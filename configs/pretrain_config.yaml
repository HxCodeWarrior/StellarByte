seed: 42
device: cuda
tokenizer_path: "./tokenizer"
train_data_path: "./data/train.jsonl"
val_data_path: "./data/val.jsonl"
out_dir: "./checkpoints"
vocab_size: 32000
model_dim: 768
num_layers: 12
num_attention_heads: 12
num_kv_heads: 6
dim_multiplier: 4
max_seq_len: 2048
drop_path_prob: 0.0
hidden_dropout_prob: 0.1
attention_dropout_prob: 0.1
residual_dropout_prob: 0.1
layer_norm_eps: 1e-5
initializer_range: 0.02
layerscale_init: 1e-5
tie_word_embeddings: true
xpos_rope_theta: 10000.0
xpos_scale_base: 512.0
use_flash_attention: true
use_causal: true
use_cache: false
key_cache_dtype: float16
value_cache_dtype: float16
model_parallel_size: 1
tensor_parallel_size: 1
tensor_parallel_rank: 0
batch_size: 32
epochs: 5
accumulation_steps: 8
grad_clip: 1.0
learning_rate: 5e-5
min_lr: 5e-6
weight_decay: 0.01
warmup_steps_ratio: 0.05
warmup_start_lr: 5e-7
lr_decay_rate: 0.8
lr_decay_steps_ratio: 0.3
num_restarts: 0
dtype: bfloat16
amp: false
log_interval: 100
save_interval: 1000
eval_interval: 2000
max_checkpoints: 5
eval_batch_size: 64
eval_max_steps: 100
num_workers: 4
