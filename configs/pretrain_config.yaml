# =======================================================================
#  文件: pretrain_config.yaml
#  说明: Byte‑Transformer 预训练/继续训练的统一配置
#        ├─ 一级键 = 功能模块
#        └─ 二级键 = 具体超参
#  ⚠️注意：YAML 规范要求键和值之间必须有一个空格
#         可按需增删；键名若改动，记得调整代码中的读取逻辑
# =======================================================================

experiment:               # ▶ 实验基础信息
  seed: 42                # 全局随机种子，保证可复现
  use_swanlab: false      # 是否启用 SwanLab 实验可视化
  swanlab:                # SwanLab 相关设置
    project: StellarByte-LLM
    experiment_name: Pretrain-StellarByte-LLM
    api_key: ""           # ← 请填入你的 API KEY

training:                 # ▶ 训练流程
  batch_size: 32
  epochs: 5
  accumulation_steps: 8   # 梯度累积
  grad_clip: 1.0
  grad_checkpoint:  True
  use_torch_compile: True
  compile_mode: max-autotune
  empty_cache_interval: 100

evaluation:               # ▶ 验证
  eval_batch_size: 64
  eval_interval:  100

distributed:              # ▶ 分布式&设备
  enable_ddp: false       # true→使用 torchrun 启动 DDP
  local_rank: -1          # torchrun 会自动注入
  device: cpu            # "cuda" / "cpu"

dataset:                  # ▶ 数据与 DataLoader
  tokenizer_path: ./tokenizer
  train_data_path: ./datasets/test/train.jsonl
  val_data_path:   ./datasets/test/val.jsonl     # 若无验证集可留空
  test_data_path:  ./datasets/test/test.jsonl    # 若无验证集可留空
  num_workers: 4

dataset_loader:            # ▶ 数据集加载器配置（对应 PretrainDataset 和 SFTDataset）
  max_length: 512          # 最大序列长度（Token级）
  fields:                  # 需要拼接的字段列表，适用于 PretrainDataset，如：input、output、thinking
    - text
  template: ""             # 拼接模板，支持格式化字段；无则为 None 或空字符串，示例："问:{input} 答:{output}"
  add_bos: true            # 是否添加BOS token（如 GPT 风格）
  sft_special_tokens:      # SFTDataset 特殊标记配置（默认无需修改，留空即可）
    assistant_start_str: "<|SBOS|>assistant\n"
    im_end_str: "<|SEOS|>"
    
model:                    # ▶ 网络结构
  vocab_size: 50007
  dim: 768                # 隐藏维度
  n_layers: 12
  n_heads: 16 # dim 必须能被 n_heads 整除，否则报错
  n_kv_heads: 8
  hidden_dim: null        # null→按 4×dim 推断
  dim_multiplier: 4
  max_seq_len: 2048
  dropout:                # 各类 Dropout
    drop_path_prob:         0.0
    hidden_dropout_prob:    0.1
    attention_dropout_prob: 0.1
    residual_dropout_prob:  0.1
  norm:
    layer_norm_eps:    1.0e-5
    initializer_range: 0.02
    layerscale_init:   1.0e-5
  embeddings:
    tie_word_embeddings: false
  xpos:                   # XPos‑RoPE 位置编码
    rope_theta: 10000.0
    scale_base: 512.0
  attention:
    use_flash_attention: false
    use_causal: true
  kv_cache:               # 推理加速
    use_cache: true
    key_dtype: float16    # float16 / bfloat16 / float32
    value_dtype: float16
  parallel:               # 张量/模型并行
    model_parallel_size:  1
    tensor_parallel_size: 1
    tensor_parallel_rank: 1

optimizer:                # ▶ AdamW
  learning_rate: 5.0e-5
  min_lr:        5.0e-6
  weight_decay:  0.01

lr_scheduler:             # ▶ 余弦 + 线性预热 + 周期重启
  warmup_steps_ratio:   0.05
  warmup_start_lr:      5.0e-7
  lr_decay_rate:        0.8
  lr_decay_steps_ratio: 0.3
  num_restarts:         0

precision:                # ▶ 混合精度
  dtype: bfloat16         # float32 / float16 / bfloat16
  amp: false              # 额外启用 torch.cuda.amp（与 dtype 互斥）

logging:                  # ▶ 日志与检查点
  logs_dir:        ./logs        # 日志文件路径
  checkpoints_dir: ./checkpoints # 检查点保存路径
  log_interval:    100           # 日志打印频率（步数）
  save_interval:   1000          # 模型检查点保存频率（步数）
  keep_latest:     5             # 保存最新的 n 个检查点
  keep_epoch:      10            # 保留最近 N 个完整 Epoch 检查点
  keep_best:       3             # 保存验证损失最优模型检查点。
