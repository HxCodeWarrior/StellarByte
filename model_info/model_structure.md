## 🔥 StellarByte-LLM 1.0模型架构

### 模型整体架构 **ByteTransformer**
```
┌─────────────────────────────────────────────────────────────────────────┐
│                           ByteTransformer                               │
│                                                                         │
│  ┌───────────────┐                                                      │
│  │ Embed Tokens  │                                                      │
│  └───────┬───────┘                                                      │
│          ▼                                                              │
│  ┌───────────────┐                                                      │
│  │Embed Dropout  │                                                      │
│  └───────┬───────┘                                                      │
│          ▼                                                              │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                     Decoder Layers Stack                        │    │
│  │  ┌─────────────────────────────────────────────────────────┐    │    │
│  │  │                   ByteDecoderLayer × N                  │    │    │
│  │  │  ┌─────────────┐    ┌─────────────┐                     │    │    │
│  │  │  │   RMSNorm   │    │   RMSNorm   │                     │    │    │
│  │  │  └──────┬──────┘    └──────┬──────┘                     │    │    │
│  │  │         ▼                  ▼                            │    │    │
│  │  │  ┌─────────────┐    ┌─────────────┐                     │    │    │
│  │  │  │ Multi-Head  │    │     MLP     │                     │    │    │
│  │  │  │ Attention   │    │ (Gated MLP) │                     │    │    │
│  │  │  └──────┬──────┘    └──────┬──────┘                     │    │    │
│  │  │         │                  │                            │    │    │
│  │  │         ▼                  ▼                            │    │    │
│  │  │  ┌─────────────┐    ┌─────────────┐                     │    │    │
│  │  │  │ LayerScale  │    │ LayerScale  │                     │    │    │
│  │  │  └──────┬──────┘    └──────┬──────┘                     │    │    │
│  │  │         │                  │                            │    │    │
│  │  │         ▼                  ▼                            │    │    │
│  │  │  ┌─────────────┐    ┌─────────────┐                     │    │    │
│  │  │  │  DropPath   │    │  DropPath   │                     │    │    │
│  │  │  └──────┬──────┘    └──────┬──────┘                     │    │    │
│  │  │         │                  │                            │    │    │
│  │  │         └──────────┬───────┘                            │    │    │
│  │  │                    ▼                                    │    │    │
│  │  │             Residual Connection                         │    │    │
│  │  └─────────────────────────────────────────────────────────┘    │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│          ▼                                                              │
│  ┌───────────────┐                                                      │
│  │    RMSNorm    │                                                      │
│  └───────┬───────┘                                                      │
│          ▼                                                              │
│  ┌───────────────┐                                                      │
│  │    LM Head    │                                                      │
│  └───────────────┘                                                      │
└─────────────────────────────────────────────────────────────────────────┘
```

### 解码层 **ByteDecoderLayer**
```
┌──────────────────────────────────────────────────────────────────────────┐
│                        ByteDecoderLayer                                  │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ┌─────────┐     ┌───────────────────────────┐                          │
│   │         │     │                           │                          │
│   │  Input  │────►│        RMSNorm_attn       │─┐                        │
│   │   [x]   │     │                           │ │                        │
│   │         │     └───────────────────────────┘ │                        │
│   └─────────┘                                   ▼                        │
│        │         ┌────────────────────────────────────────────┐          │
│        │         │           MultiHeadSelfAttention           │          │
│        │         │                                            │          │
│        │         │  ┌─────────┐  ┌─────────┐  ┌─────────┐     │          │
│        │         │  │   W_q   │  │   W_k   │  │   W_v   │     │          │
│        │         │  └─────────┘  └─────────┘  └─────────┘     │          │
│        │         │       │            │            │          │          │
│        │         │       ▼            ▼            ▼          │          │
│        │         │  ┌─────────────────────────────────────┐   │          │
│        │         │  │        XPosRotaryEmbedding          │   │          │
│        │         │  └─────────────────────────────────────┘   │          │
│        │         │       │            │            │          │          │
│        │         │       ▼            ▼            ▼          │          │
│        │         │  ┌─────────────────────────────────────┐   │          │
│        │         │  │       Scaled Dot-Product Attn       │   │          │
│        │         │  └─────────────────────────────────────┘   │          │
│        │         │                    │                       │          │
│        │         │                    ▼                       │          │
│        │         │  ┌─────────────────────────────────────┐   │          │
│        │         │  │                W_o                  │   │          │
│        │         │  └─────────────────────────────────────┘   │          │
│        │         │                    │                       │          │
│        │         └────────────────────┼───────────────────────┘          │
│        │                              │                                  │
│        │                              ▼                                  │
│        │         ┌─────────────────────────────────────┐                 │
│        │         │           DropPath(p)               │                 │
│        │         └─────────────────────────────────────┘                 │
│        │                              │                                  │
│        │                              ▼                                  │
│        │         ┌─────────────────────────────────────┐                 │
│        │         │           LayerScale_attn           │                 │
│        │         └─────────────────────────────────────┘                 │
│        │                              │                                  │
│        ├──────────────────────────────┼──────────────────────────────────┤
│        │                              ▼                                  │
│        │         ┌─────────────────────────────────────┐                 │
│        └────────►│               Add                   │                 │
│                  └─────────────────────────────────────┘                 │
│                                     │                                    │
│                                     ▼                                    │
│                  ┌─────────────────────────────────────┐                 │
│                  │            RMSNorm_ffn              │                 │
│                  └─────────────────────────────────────┘                 │
│                                     │                                    │
│                                     ▼                                    │
│                  ┌──────────────────────────────────────┐                │
│                  │                MLP                   │                │
│                  │   ┌───────────┐     ┌───────────┐    │                │
│                  │   │    w1     │     │    w3     │    │                │
│                  │   └───────────┘     └───────────┘    │                │
│                  │         │                │           │                │
│                  │         ▼                ▼           │                │
│                  │   ┌───────────┐     ┌───────────┐    │                │
│                  │   │   SiLU    │     │  Linear   │    │                │
│                  │   └───────────┘     └───────────┘    │                │
│                  │         │                │           │                │
│                  │         └────────┬─────────┘         │                │
│                  │                  │                   │                │
│                  │                  ▼                   │                │
│                  │   ┌───────────────────────────┐      │                │
│                  │   │       Element-wise *      │      │                │
│                  │   └───────────────────────────┘      │                │
│                  │                  │                   │                │
│                  │                  ▼                   │                │
│                  │   ┌───────────────────────────┐      │                │
│                  │   │            w2             │      │                │
│                  │   └───────────────────────────┘      │                │
│                  │                  │                   │                │
│                  └──────────────────┼───────────────────┘                │
│                                     │                                    │
│                                     ▼                                    │
│                  ┌─────────────────────────────────────┐                 │
│                  │           DropPath(p)               │                 │
│                  └─────────────────────────────────────┘                 │
│                                     │                                    │
│                                     ▼                                    │
│                  ┌─────────────────────────────────────┐                 │
│                  │           LayerScale_mlp            │                 │
│                  └─────────────────────────────────────┘                 │
│                                     │                                    │
│                                     ▼                                    │
│                  ┌─────────────────────────────────────┐                 │
│                  │               Add                   │───────────────► │
│                  └─────────────────────────────────────┘                 │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 多头自注意力层 **ByteMultiHeadSelfAttention**
```
┌─────────────────────────────────────────────────────────────────────────┐
│                    MultiHeadSelfAttention                               │
│                                                                         │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐                │
│  │      W_q      │  │      W_k      │  │      W_v      │                │
│  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘                │
│          ▼                  ▼                  ▼                        │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐                │
│  │  Reshape to   │  │  Reshape to   │  │  Reshape to   │                │
│  │  Multi-head   │  │  Multi-head   │  │  Multi-head   │                │
│  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘                │
│          │                  │                  │                        │
│          ▼                  ▼                  ▼                        │
│  ┌─────────────────────────────────────────────────────────┐            │
│  │                 XPosRotaryEmbedding                     │            │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐      │            │
│  │  │     Cos     │  │     Sin     │  │    Scale    │      │            │
│  │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘      │            │
│  │         └──────────┬─────┘                │             │            │
│  │                    ▼                      ▼             │            │
│  │             Apply Rotation          Apply Scaling       │            │
│  └─────────────┬─────────────────────┬─────────────────────┘            │
│                │                     │                                  │
│                ▼                     ▼                                  │
│  ┌───────────────────┐      ┌────────────────────┐                      │
│  │ Rotated Q Vectors │      │ Rotated K Vectors  │                      │
│  └─────────┬─────────┘      └──────────┬─────────┘                      │
│            │                           │                                │
│            └───────────┬───────────────┘                                │
│                        ▼                                                │
│  ┌─────────────────────────────────────────────┐                        │
│  │              Attention Scores               │                        │
│  │  (Q·K^T / sqrt(d_k)) + Attention Mask       │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│  ┌─────────────────────────────────────────────┐                        │
│  │                  Softmax                    │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│  ┌─────────────────────────────────────────────┐                        │
│  │             Attention Dropout               │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│  ┌─────────────────────────────────────────────┐                        │
│  │         Weighted Sum (Attention·V)          │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│  ┌─────────────────────────────────────────────┐                        │
│  │       Reshape and Concatenate Heads         │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│  ┌─────────────────────────────────────────────┐                        │
│  │                    W_o                      │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│  ┌─────────────────────────────────────────────┐                        │
│  │              Residual Dropout               │                        │
│  └───────────────────┬─────────────────────────┘                        │
│                      ▼                                                  │
│                   Output                                                │
└─────────────────────────────────────────────────────────────────────────┘
```

### 位置旋转嵌入层 **XPosRotaryEmbedding**
```
┌─────────────────────────────────────────────────────────────────────────┐
│                      XPosRotaryEmbedding                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│   ┌─────────┐     ┌─────────┐                                           │
│   │   xq    │     │   xk    │                                           │
│   └─────────┘     └─────────┘                                           │
│        │              │                                                 │
│        ▼              ▼                                                 │
│   ┌─────────────────────────────────────────────────────────┐           │
│   │                  _get_cos_sin_scale                     │           │
│   │                                                         │           │
│   │   ┌─────────────────────────────────────────────────┐   │           │
│   │   │                 RotaryCache                     │   │           │
│   │   │  (缓存cos/sin/scale张量，避免重复计算)              │   │           │
│   │   └─────────────────────────────────────────────────┘   │           │
│   │                                                         │           │
│   │   ┌─────────┐     ┌─────────┐     ┌─────────┐           │           │
│   │   │   cos   │     │   sin   │     │  scale  │           │           │
│   │   └─────────┘     └─────────┘     └─────────┘           │           │
│   └─────────────────────────────────────────────────────────┘           │
│        │              │              │                                  │
│        ▼              ▼              ▼                                  │
│   ┌─────────┐     ┌─────────┐     ┌─────────┐                           │
│   │ xq*scale│     │ xk/scale│     │_rotate_half│                        │
│   └─────────┘     └─────────┘     └─────────┘                           │
│        │              │              │                                  │
│        │              │              │                                  │
│        ▼              ▼              ▼                                  │
│   ┌─────────────────────────────────────────────────────────┐           │
│   │  xq_out = xq_scaled * cos + _rotate_half(xq_scaled) * sin           │
│   │  xk_out = xk_scaled * cos + _rotate_half(xk_scaled) * sin           │
│   └─────────────────────────────────────────────────────────┘           │
│        │              │                                                 │
│        ▼              ▼                                                 │
│   ┌─────────┐     ┌─────────┐                                           │
│   │ xq_out  │     │ xk_out  │                                           │
│   └─────────┘     └─────────┘                                           │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 层归一化层 **RMSNorm**
```
┌─────────────────────────────────────────────────────────────────────────┐
│                             RMSNorm                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│   ┌─────────┐                                                           │
│   │  Input  │                                                           │
│   │    x    │                                                           │
│   └─────────┘                                                           │
│        │                                                                │
│        ▼                                                                │
│   ┌─────────────────────────────────────────────────────────┐           │
│   │                    _rms_norm                            │           │
│   │                                                         │           │
│   │   ┌─────────────────────────────────────────────────┐   │           │
│   │   │  1. 提升到float32计算平方和均值                     │  │            │
│   │   │  rms = torch.mean(x.to(torch.float32).pow(2),   │   │           │
│   │   │                   dim=-1, keepdim=True)         │   │           │
│   │   └─────────────────────────────────────────────────┘   │           │
│   │                          │                              │           │
│   │                          ▼                              │           │
│   │   ┌─────────────────────────────────────────────────┐   │           │
│   │   │  2. 计算平方根倒数，并确保数值稳定                    │   │           │
│   │   │  inv_rms = torch.rsqrt(rms.clamp_min(eps))      │   │           │
│   │   └─────────────────────────────────────────────────┘   │           │
│   │                          │                              │           │
│   │                          ▼                              │           │
│   │   ┌─────────────────────────────────────────────────┐   │           │
│   │   │  3. 转回原始数据类型                               │   │           │
│   │   │  inv_rms = inv_rms.to(x.dtype)                  │   │           │
│   │   │  weight = weight.to(x.dtype)                    │   │           │
│   │   └─────────────────────────────────────────────────┘   │           │
│   │                          │                              │           │
│   │                          ▼                              │           │
│   │   ┌─────────────────────────────────────────────────┐   │           │
│   │   │  4. 归一化并应用可学习权重                          │   │           │
│   │   │  return (x * inv_rms) * weight                  │   │           │
│   │   └─────────────────────────────────────────────────┘   │           │
│   │                                                         │           │
│   └─────────────────────────────────────────────────────────┘           │
│                          │                                              │
│                          ▼                                              │
│   ┌─────────┐                                                           │
│   │ Output  │                                                           │
│   └─────────┘                                                           │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 多层感知机层 **GatedMLP**
```
┌─────────────────────────────────────────────────────────────────────────┐
│                              MLP                                        │
│                                                                         │
│  ┌───────────────┐                  ┌───────────────┐                   │
│  │      W1       │                  │      W3       │                   │
│  └───────┬───────┘                  └───────┬───────┘                   │
│          ▼                                  ▼                           │
│  ┌───────────────┐                  ┌───────────────┐                   │
│  │  SiLU (Swish) │                  │   (No Activ)  │                   │
│  └───────┬───────┘                  └───────┬───────┘                   │
│          │                                  │                           │
│          └─────────────────┬────────────────┘                           │
│                            ▼                                            │
│  ┌─────────────────────────────────────────────────┐                    │
│  │              Element-wise Multiply              │                    │
│  └───────────────────────┬─────────────────────────┘                    │
│                          ▼                                              │
│  ┌─────────────────────────────────────────────────┐                    │
│  │                      W2                         │                    │
│  └───────────────────────┬─────────────────────────┘                    │
│                          ▼                                              │
│  ┌─────────────────────────────────────────────────┐                    │
│  │                    Dropout                      │                    │
│  └───────────────────────┬─────────────────────────┘                    │
│                          ▼                                              │
│                       Output                                            │
└─────────────────────────────────────────────────────────────────────────┘
```
