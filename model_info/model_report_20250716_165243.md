# ByteTransformer 模型分析报告

生成时间: 2025-07-16 16:52:43

## 1. 模型概览

- **模型类型**: ByteTransformer
- **词汇表大小**: 32,000
- **模型维度**: 768
- **层数**: 12
- **注意力头数**: 16
- **KV头数**: 8
- **隐藏层维度**: 3072
- **最大序列长度**: 2048
- **总参数量**: 155,358,240
- **非嵌入参数量**: 106,206,240

## 2. 参数统计

### 2.1 参数类型分布

| 参数类型   |   参数数量 | 占比   |
|:-----------|-----------:|:-------|
| embedding  |   24576000 | 15.82% |
| attention  |   21233664 | 13.67% |
| mlp        |   84934656 | 54.67% |
| norm       |      19200 | 0.01%  |
| lm_head    |   24576000 | 15.82% |

### 2.2 层参数分布

| 层名称   |   参数数量 | 占比   |
|:---------|-----------:|:-------|
| layer_0  |    8850456 | 5.70%  |
| layer_1  |    8850456 | 5.70%  |
| layer_2  |    8850456 | 5.70%  |
| layer_3  |    8850456 | 5.70%  |
| layer_4  |    8850456 | 5.70%  |
| layer_5  |    8850456 | 5.70%  |
| layer_6  |    8850456 | 5.70%  |
| layer_7  |    8850456 | 5.70%  |
| layer_8  |    8850456 | 5.70%  |
| layer_9  |    8850456 | 5.70%  |

*注: 仅显示前10层，共12层*

## 3. 内存使用

| 内存类型   | 大小       |
|:-----------|:-----------|
| 参数       | 592.64 MB  |
| 缓冲区     | 48.00 MB   |
| 激活值     | 316.00 MB  |
| KV缓存     | 36.00 MB   |
| 梯度       | 592.64 MB  |
| 优化器状态 | 1185.29 MB |
| 总计       | 2.71 GB    |

## 4. 计算复杂度

### 4.1 总计算量

- **总浮点运算次数**: 58,485,374,976 FLOPS
- **总浮点运算次数(G)**: 58.49 GFLOPS
- **总浮点运算次数(T)**: 0.0585 TFLOPS

### 4.2 计算分布

| 计算部分   | FLOPS          | 占比   |
|:-----------|:---------------|:-------|
| 注意力层   | 1,409,286,144  | 2.41%  |
| MLP层      | 2,415,919,104  | 4.13%  |
| LM头       | 12,582,912,000 | 21.51% |

### 4.3 生成计算量

- **每个token的浮点运算次数**: 7,471,104 FLOPS
- **每秒浮点运算次数(估计)**: 74,711,040 FLOPS
- **每秒浮点运算次数(G)(估计)**: 0.07 GFLOPS

## 5. 性能分析

设备: cuda

### 5.1 前向传播性能

#### 批次大小 = 1

|   序列长度 | 延迟      | 吞吐量         |
|-----------:|:----------|:---------------|
|        128 | 32.46 ms  | 3,943 tokens/s |
|        512 | 84.02 ms  | 6,094 tokens/s |
|       2048 | 316.01 ms | 6,481 tokens/s |

#### 批次大小 = 4

|   序列长度 | 延迟       | 吞吐量         |
|-----------:|:-----------|:---------------|
|        128 | 65.50 ms   | 7,817 tokens/s |
|        512 | 299.32 ms  | 6,842 tokens/s |
|       2048 | 1159.01 ms | 7,068 tokens/s |

#### 批次大小 = 16

|   序列长度 | 延迟       | 吞吐量         |
|-----------:|:-----------|:---------------|
|        128 | 213.37 ms  | 9,598 tokens/s |
|        512 | 1215.43 ms | 6,740 tokens/s |
|       2048 | 4878.82 ms | 6,716 tokens/s |

### 5.2 生成性能

|   批次大小 | 生成速度       | 每token时间    |
|-----------:|:---------------|:---------------|
|          1 | 27.83 tokens/s | 35.94 ms/token |
|          2 | 34.09 tokens/s | 29.34 ms/token |
|          4 | 38.57 tokens/s | 25.93 ms/token |

## 6. 结论和建议

### 6.1 参数效率

### 6.2 计算效率

### 6.3 内存优化

## 7. 附录：模型架构

```
ByteTransformer(
  (embed_tokens): Embedding(32000, 768)
  (layers.0): DecoderLayer(
    (attn): MultiHeadSelfAttention(...)
    (mlp): MLP(...)
  )
  (layers.1): DecoderLayer(
    (attn): MultiHeadSelfAttention(...)
    (mlp): MLP(...)
  )
  (layers.2): DecoderLayer(
    (attn): MultiHeadSelfAttention(...)
    (mlp): MLP(...)
  )
  ... 9 more layers ...
  (norm): LayerNorm(768)
  (lm_head): Linear(768, 32000)
)
```

## 8. 可视化图表

### 8.1 参数分布

![参数分布](plots/parameter_distribution.png)

### 8.2 计算分布

![计算分布](plots/compute_distribution.png)

### 8.3 层参数分布

![层参数分布](plots/layer_parameters.png)

